{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Santander Customer Transaction Prediction</center>"},{"metadata":{},"cell_type":"markdown","source":"\n<center><img src=\"https://www.forbrukslånlavrente.com/wp-content/uploads/2015/06/santander-consumer-bank-logo.jpg\" alt=\"drawing\" style=\"width:800px;\"/></center>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n<p>In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.\n\nIn the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#importing all the libraries needed\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\nimport random\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import mean_squared_error\n#--------------------------------------------------\n#SVM\nimport numpy as np\n#import cvxopt\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n%matplotlib inline\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## **Loading packages **"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\")\ndata_test = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/test.csv\")\ndata_submission = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#   <center>Exploratory Data Analysis (EDA)</center> \n##  **Data processing \"Train\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t = data_train\ndf_test = data_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- ** Dataset comprises of 200000 observations and 202 characteristics. **\n>- ** Out of which one is dependent variable and rest 201 are independent variables — physico-chemical characteristics.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Train shape : ',df_t.shape)\nprint ('Test shape : ',df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both train and test data have 200,000 entries and 202, respectivelly 201 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**check missing values in train data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- ** Data has only float, object and integer values. **\n>- ** No variable column has null/missing values. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is notably a large difference between 75th %tile and max values of predictors “var_5”, ”var_190”, ”var_0”."},{"metadata":{},"cell_type":"markdown","source":"#### Few key insights just by looking at dependent variable are as follows:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.target.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- This tells us vote count of each quality score in descending order.\n>- “target” has most values concentrated in the categories '0'.\n"},{"metadata":{},"cell_type":"markdown","source":"I got a good a glimpse of data. But that’s the thing with Data Science the more you get involved the harder it is for you to stop exploring.Let’s now explore data with beautiful graphs. Python has a visualization library ,Seaborn which build on top of matplotlib. It provides very attractive statistical graphs in order to perform both Univariate and Multivariate analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can make few observations here:\n\n>- standard deviation is relatively large for both train and test variable data;\n>- min, max, mean, sdt values for train and test data looks quite close;\n>- mean values are distributed over a large range."},{"metadata":{},"cell_type":"markdown","source":"To use linear regression for modelling,its necessary to remove correlated variables to improve your model.One can find correlations using pandas “.corr()” function and can visualize the correlation matrix using a heatmap in seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.iloc[2:5, 2:10].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Force reduced data for visual graph viewing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.heatmap(df_t.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n** Nothing is clear therefore we take less data for visual viewing **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df_t.iloc[2:5, 2:10].corr(), annot = True, vmin=-1, vmax=1, center= 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- Lighter shades represents positive correlation while dark shades represents negative correlation.\n>- If you set annot=True, you’ll get values by which features are correlated to each other in grid-cells."},{"metadata":{},"cell_type":"markdown","source":"**The number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will show just 5% of the data. On x axis we show train values and on the y axis we show the test values."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(df_t[::20],df_test[::20], features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Let's check the distribution of target value in train dataset. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_t['target'], palette='Set3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** The data is unbalanced with respect with target value. **"},{"metadata":{},"cell_type":"markdown","source":"## Distribution of mean and std\nLet's check the distribution of the mean values per row in the train and test set."},{"metadata":{},"cell_type":"markdown","source":"\n![](https://i.talkingofmoney.com/img/articles-2017/normal-distribution-table-explained-3.png)"},{"metadata":{},"cell_type":"markdown","source":"Let's check the distribution of the mean values per row in the train and test set.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nfeatures = df_t.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(df_t[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(df_test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the distribution of the mean values per columns in the train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(df_t[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(df_test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_t.drop(['ID_code', 'target'], axis=1)\ny = df_t['target']\nX_test = df_test.drop(['ID_code'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  <center>Logistic Regression</center>"},{"metadata":{},"cell_type":"markdown","source":"\n<center><img src=\"https://miro.medium.com/max/2900/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg\" alt=\"drawing\" style=\"width:800px;\"/></center>"},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression Model Fitting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predicting the test set results and calculating the accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\ncm = confusion_matrix\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is telling us that we have 53180+1414 correct predictions and 4700+706 incorrect predictions."},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/888/1*7J08ekAwupLBegeUI8muHA.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\npred = logreg.predict(X_test)\nprint(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test), columns=np.unique(y_test))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logist_pred = logreg.predict_proba(X_test)[:,1]\nlogist_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Root Mean Squared Logarithmic Error (RMSLE) evaluation function"},{"metadata":{},"cell_type":"markdown","source":"![](http://statistica.ru/upload/medialibrary/fca/image001.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ntrain_pred = logreg.predict(X_train)\nprint('RMSLE : {:.4f}'.format(rmsle(y_train, train_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the other hand, RMSE fails to capture any special relation between the Predicted value and the Actual Value and it is completely linear in both direction of the zero error."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_t = df_test.drop(['ID_code'], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_pred_test = logreg.predict_proba(test_t)[:,1]\nresult = df_test[['ID_code']]\nresult['target'] = logreg_pred_test\nresult.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Naive Bayes </center>"},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://miro.medium.com/max/1514/1*46DjCygiiqhgHYlQFS4ULQ.jpeg\" alt=\"drawing\" style=\"width:600px;\"/></center>"},{"metadata":{},"cell_type":"markdown","source":"### ** Formula **"},{"metadata":{},"cell_type":"markdown","source":"<center>![](https://miro.medium.com/max/503/1*6dmvRYysiU5PwWIcHRdKVw.png)</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train, y_train)\nacc = nb.score(X_test,y_test)*100\ny_pred = nb.predict(X_test)\ncm_nb = confusion_matrix\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test), columns=np.unique(y_test))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, nb.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, nb.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_pred_test = nb.predict_proba(test_t)[:,1]\nresult = df_test[['ID_code']]\nresult['target'] = nb_pred_test\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('NB_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Decision Trees and Random Forests </center>"},{"metadata":{},"cell_type":"markdown","source":"\n<center><img src=\"https://miro.medium.com/max/670/1*dTCdSK0QoC7RndDo1eIrDg.png\" alt=\"drawing\" style=\"width:500px;\"/></center>"},{"metadata":{},"cell_type":"markdown","source":"# <center>DECISION TREE MODEL</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier(class_weight='balanced',max_depth=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test), columns=np.unique(y_test))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, tree.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, tree.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_pred_test = tree.predict_proba(test_t)[:,1]\nresult = df_test[['ID_code']]\nresult['target'] =tr_pred_test\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('tree_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Random Forests </center>"},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://miro.medium.com/max/1170/1*58f1CZ8M4il0OZYg2oRN4w.png\" alt=\"drawing\" style=\"width:500px;\"/></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create the model with 100 trees\nmodel = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n# Fit on training data\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions = model.predict(X_test)\n# Calculate the absolute errors\nerrors = abs(predictions - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test_d, model.predict(X_test_d))\nfpr, tpr, thresholds = roc_curve(y_test_d, model.predict_proba(X_test_d)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rd_pred_test = model.predict_proba(test_t)[:,1]\nresult = df_test[['ID_code']]\nresult['target'] =rd_pred_test\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('rd_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center>XGboost</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\ncm = pd.DataFrame(cm, index=np.unique(y_test), columns=np.unique(y_test))\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\n\nfig, ax = plt.subplots(figsize=[5,2])\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)\n\nsns.heatmap(cm, cmap= \"YlGnBu\", annot= annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_pred_test = model.predict_proba(test_t)[:,1]\nresult = df_test[['ID_code']]\nresult['target'] =xg_pred_test\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('xg_baseline.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}