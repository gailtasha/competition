{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>NoteBook - Santander Competition</h1>\n\nL'objectif de ce Notebook est de pouvoir prédire le comportement des clients de la banque espagnole Santander. Dans ce challenge, nous devons identifer quels client réalisera une transaction particulière dans le future.<br></br><br></br>\nDans ce problème, nous avons deux résultats possibles : le client réalise la transaction ou non. Nous sommes donc dans un problème de classification binaire. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectFromModel\n\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On se met dans le répertoire où se trouve nos jeux de données\nif os.path.exists('../input'): \n    os.chdir('../input')\nelse: pass","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_train = 'train.csv'\ndata_test = 'test.csv'\nX = pd.read_csv(data_train, index_col=None)\nY = X['target']\n\nX.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut voir avec la fonction **count** qu'il n'y a pas de missing values dans le jeu de données et il n'y a donc pas d'imputation à réaliser. Nous allons maintenant observer la corrélation entre les différentes variables du jeu de données.\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(100, 100))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On observe à travers cette matrice des corrélations qu'il n'y a pas de liens forts entre les différentes variables du jeu de données. Nous allons maintenant observer la proportion de transactions de chacune des classes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cible = pd.value_counts(X[\"target\"], sort=True).sort_index()\nprint(cible)\n\ncible.plot(kind='bar', figsize=(10,7))\nplt.title(\"Histogramme de transaction\")\nplt.xlabel(\"Type\")\nplt.ylabel(\"Nombre\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut observer sur le graphique ci-dessus, qu'on a deux classes existantes et qu'il y a un déséquilibre entre les 0 et les 1. En effet, environ 90% des observations appartiennent à la classe 0 et 10% à la classe restantes.<br></br>\nCela risque d'être un problème pour la création de modèles, nos modèles risqueront de toujours prédire la classe dominante, ici la classe 0. Pour palier à ce problème, nous allons faire appel à une stratégie d'échantillonage : **L'undersampling** \n\n<h2> Undersampling </h2>\nEn analyse des données, cette méthode consiste au rééquilibrage de la distribution des classes dans le jeu de données. Pour ce faire, elle enlève des observation issues de la classe majoritaire (ici la classe 0). <br></br><br></br>\nEn effet, cette éthode à pour objectif de rendre les algorithmes de Machine Learning que nous implémenterons plus intelligent et créer des modèles de classification plus pertinent. "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_1 = len(X[X['target'] == 1])\nnon_num_1_indices = X[X.target == 0].index\nrandom_indices = np.random.choice(non_num_1_indices,num_1, replace=False)\nnum_1_indices = X[X.target == 1].index\nunder_sample_indices = np.concatenate([num_1_indices,random_indices])\nunder_sample = X.loc[under_sample_indices]\n\ny_under = under_sample['target']\n\n\ncible = pd.value_counts(under_sample[\"target\"], sort=True).sort_index()\nprint(cible)\nunder_sample.drop(['target', 'ID_code'], axis='columns', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Désormais, nous avons autant d'observations dans la classe 1 et 0. Nous allons donc pouvoir commencer à tester quelques algorithmes de Machine Learning. <br></br> <br></br>\nAvant de passer au première algorithme, donnons une courte définition de ce qu'est le **Machine Learning** : Il peut être vu comme étant le champ d'étude qui vise à donner la capacité à une machine d'apprendre sans être explicitement programmé.\n<h2> La régression logistique</h2>\n\nLa régression logit est couramment utilisée pour estimer la probabilité quu'une observation appartienne à une classe particulière. Si la probabilité estimé est supérieure à 50%, alors notre modèle prédit que l'observation appartient à la classe 1, sinon il prédit qu'elle appartient à l'autre classe. On a donc affaire à un classificateur binaire, ce qui est compatible avec notre problème de transaction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# On commence par partitionner notrer jeu de données pour tester l'algorithme.\n#Pour ce faire, on segmente notre jeu de données en deux parties : une partie entraînement et une de test.\n\nX_train, X_test, y_train, y_test = train_test_split(under_sample, y_under, test_size=0.3, random_state=0, stratify=y_under)\n\nclf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\npredictions = clf.predict(X_test)\nclf.predict_proba(X_test)\nclf.score(X_test, y_test)\n\ncnf_matrix=confusion_matrix(y_test,predictions)\nprint(\" Le rappel de ce modèle est :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n\n\nfig= plt.figure(figsize=(6,3))# pour plot le graphe\nprint(\"TP\",cnf_matrix[1,1,]) # nombre de transactions \"1\" prédit comme étant \"1\"\nprint(\"TN\",cnf_matrix[0,0]) # nombre de transactions \"0\" prédit comme étant \"0\"\nprint(\"FP\",cnf_matrix[0,1]) # nombre de transactions \"0\" prédit comme étant \"1\"\nprint(\"FN\",cnf_matrix[1,0]) # nombre de transactions \"1\" prédit comme étant \"0\"\nsns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted_class\")\nplt.ylabel(\"Real class\")\nplt.show()\nprint(\"\\n--------------------------Classification Report------------------------------------\")\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On observe que le modèle possède un rappel de 0.76, on a donc une proportion de positifs prédit parmis tous les positifs de la population. Au niveau de la précision (qui permet de mesurer la proportion de positifs de la population parmis tous les positifs prédits), on obtient 0.76 également. \n<h2>Forêts aléatoires ou Random Forest</h2>\nUne forêt aléatoire est un ensemble de plusieurs arbres de décison construit aléatoirement qui s'exécutent en parallèle, avant de les moyenner. "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=200, max_depth=None, criterion='entropy', random_state=0, n_jobs=-1, min_samples_leaf=15)\nclf.fit(X_train, y_train)\nclf.feature_importances_\npredictions1 = clf.predict(X_test)\nclf.predict_proba(X_test)\nclf.score(X_test, y_test)\n\ncnf_matrix=confusion_matrix(y_test,predictions1)\nprint(\" Le rappel de ce modèle est :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n\n\nfig= plt.figure(figsize=(6,3))# to plot le graphe\nprint(\"TP\",cnf_matrix[1,1,]) # nombre de transactions \"1\" prédit comme étant \"1\"\nprint(\"TN\",cnf_matrix[0,0]) # nombre de transactions \"0\" prédit comme étant \"0\"\nprint(\"FP\",cnf_matrix[0,1]) # nombre de transactions \"0\" prédit comme étant \"1\"\nprint(\"FN\",cnf_matrix[1,0]) # nombre de transactions \"1\" prédit comme étant \"0\"\nsns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted_class\")\nplt.ylabel(\"Real class\")\nplt.show()\nprint(\"\\n--------------------------Classification Report------------------------------------\")\nprint(classification_report(y_test,predictions1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On observe que ce modèle se débrouille mieux que celui de la régression logistique pour classer les différentes observations. En effet, on peut voir un rappel d'environ 0.77 (soit un gain de 0.02 sur le modèle précédent) et une précision de 0.76. \n<h2>Boosting : Light GBM </h2>\nLes méthodes de Boosting correspondent à une méthode de classification (et de régression) non linéaire très performante. Cette méthode agit par itérations successives, la connaissance d'un classifieur, dit faible, est rajouté à un classifieur final, que l'on appelle classifieur fort. <br></br>\nOn utillise plusieurs paramètres qui permettent d'ajuster le modèle, tel que le taux d'apprentissage, le nombre de feuilles ou la profondeur maximale de l'arbre.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc(m, train, test): \n    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n\nparams = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'false',\n    'boosting': 'gbdt',\n    'learning_rate': 0.030020874987649975,\n    'max_depth':8,\n    'num_leaves': 20,\n    'min_data_in_leaf':879,\n    'feature_fraction': 0.28058785906270434,\n    'subsample': 0.9865508992786004\n}\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test)\n\nprint('Start training...')\ngbm = lgb.train(params,\n                train_data,\n                num_boost_round=10000,\n                valid_sets=test_data,\n                early_stopping_rounds=500) # Arret si 500 iterations sans gain de performance\n\nprint('Start predicting...')\npred1s = gbm.predict(X_test, num_iteration=gbm.best_iteration)\nprint(pred1s)\nprint('LGBM scoring :', mean_squared_error(y_test, pred1s))\n\ncnf_matrix=confusion_matrix(y_test,pred1s)\nprint(\" Le rappel de ce modèle est :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n\n\nfig = plt.figure(figsize=(6,3))# to plot the graph\nprint(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\nprint(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\nprint(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\nprint(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\nsns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted_class\")\nplt.ylabel(\"Real class\")\nplt.show()\nprint(\"\\n---------------------------Classification Report------------------------------------\")\nprint(classification_report(y_test,preds1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cette fois-ci, on fait appelle à l'AUC (Area Under The Curve) pour scorer notre modèle. Plus grande est l'AUC, meilleur est le modèle."},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame({'gain': gbm.feature_importance(importance_type='gain'),\n                           'feature': gbm.feature_name()})\nimportance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(10, 40))\nax = sns.barplot(x='gain', y='feature', data=importance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sur le graphique ci-dessus, on peut observer le gain d'information qu'apporte au modèle chacune des variables. Cela nous permettra de choisir les variables qui fond du bruit et que l'on pourrait potentiellement enlever pour améliorer notre modèle par la suite.<br></br>\nPour ce faire, nous allons créer une petite fonction seuil de gain, qui va nous permettre de récupérer la liste de svariable ayant un gain supérieur "},{"metadata":{"trusted":true},"cell_type":"code","source":"i_seuil = 600\nl_new_set = []\nvaleurs = importance[['feature', 'gain']].values\nfor i in  range(valeurs.shape[0]):\n    if valeurs[i][1] > i_seuil:\n        l_new_set.append(valeurs[i][0])\nl_new_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On obtient donc la liste des variables apportant un gain supérieur à 500 au modèle light GBM. Nous allons désormais réaliser un test avec les variables selectionnées.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = under_sample[l_new_set]\n\nX_train, X_test, y_train, y_test = train_test_split(df_new, y_under, test_size=0.25, random_state=0, stratify=y_under)\n\n\ndef auc(m, train, test): \n    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n\nparams = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'false',\n    'boosting': 'gbdt',\n    'learning_rate': 0.03754469445963884,\n    'max_depth':5,\n    'num_leaves': 2,\n    'min_data_in_leaf':453,\n    'feature_fraction': 0.536120208317914,\n    'subsample': 0.2452056321344909\n}\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test)\n\nprint('Start training...')\ngbm = lgb.train(params,\n                train_data,\n                num_boost_round=10000,\n                valid_sets=test_data,\n                early_stopping_rounds=500) # Arret si 500 iterations sans gain de performance\n\nprint('Start predicting...')\npred1s = gbm.predict(X_test, num_iteration=gbm.best_iteration)\nprint(pred1s)\nprint('LGBM scoring :', mean_squared_error(y_test, pred1s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Optimisation des hyperparamètres de Light GBM </h2>\n\nLes hyperparamètres sont des paramètres ajustables que l'on choisit pour entraîner notre modèle qui régit le processus d'entraînement lui-même. L'optimisation de ces hyperparamètres revient à résoudre le problème de choisir un set d'hyperparamètres optimaux pour l'algorithme de Machines Learning que l'on souhaite utiliser (dans notre cas : Light GBM). En plus de pouvoir aider à trouver les hyperparamètres optimaux, il permet aussi un grand gain de temps pour le Data Scientist. Il existe plusieurs approches de recherche du set optimal : Le Grid Search, le Random Search ou bien le Bayesian Optimization. <br></br>\nDans notre cas, nous allons utiliser l'approche du Grid Search, qui est un mode de recherche exhaustif à travers d'intervals spécifié manuellemebt sur l'espace des Hyperparamètres de l'algorithme d'apprentissage. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import skopt\n\nN_ROWS=200000\nTRAIN_PATH = 'train.csv'\nSTATIC_PARAMS = {'boosting': 'gbdt',\n                'objective':'binary',\n                'metric': 'auc',\n                'num_threads': 8,\n                }\nN_CALLS = 2\nNUM_BOOST_ROUND = 10000\nEARLY_STOPPING_ROUNDS = 1000\n\ndef train_evaluate(X, y, params):\n    X_train, X_valid, y_train, y_valid = train_test_split(df_new, y_under, test_size=0.3, random_state=0, stratify=y_under)\n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n\n    model = lgb.train(params, train_data,\n                      num_boost_round=NUM_BOOST_ROUND,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      valid_sets=[valid_data], \n                      valid_names=['valid'])\n    \n    score = model.best_score['valid']['auc']\n    return score\n\n\nspace = [skopt.space.Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),\n         skopt.space.Integer(1, 30, name='max_depth'),\n         skopt.space.Integer(1, 100, name='num_leaves'),\n         skopt.space.Integer(10, 1000, name='min_data_in_leaf'),\n         skopt.space.Real(0.1, 1.0, name='feature_fraction', prior='uniform'),\n         skopt.space.Real(0.1, 1.0, name='subsample', prior='uniform'),\n         ]\n\ndata = pd.read_csv(TRAIN_PATH, nrows=N_ROWS)\n    \nX = data.drop(['ID_code', 'target'], axis=1)\ny = data['target']\n\n@skopt.utils.use_named_args(space)\ndef objective(**params):\n    all_params = {**params, **STATIC_PARAMS}\n    return -1.0 * train_evaluate(X, y, all_params)\n\nresults = skopt.dummy_minimize(objective, space, n_calls=N_CALLS)\n\nprint('Best Validation AUC: {}'.format(-1.0 * results.fun))\nprint('Best Params: {}'.format(results.x))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avec l'aide de cette fonction, on obtient les paramètres pour le score d'AUC le plus grand. la fonction *dummy_minimize* va, à l'aide des fonctions *objective* et *space*, chercher la valeur AUC la plus grande qui correspond au modèle le plus performant. Il indiquera juste en-dessous la valeur des hyperparamètres qui permettent d'obtenir une telle performance.  \n<h2>Soumission des résultats : 0.89204</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(data_test, index_col=None)\ntest_data.drop(['ID_code'], axis='columns', inplace=True)\nfrom xlsxwriter.workbook import Workbook\n\n\npreds = gbm.predict(test_data, num_iteration=gbm.best_iteration)\nprint(preds)\n\nsub = pd.read_csv('sample_submission.csv')\nsub['target'] = preds\n    \nsub.to_csv('sample_submission.csv')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}