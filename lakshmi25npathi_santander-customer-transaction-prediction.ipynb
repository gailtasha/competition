{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Project title :- Santander customer transaction prediction using Python**"},{"metadata":{},"cell_type":"markdown","source":"**Problem statement :-**\n\nIn this challenge, we need to identify which customers will make a specific transaction in\nthe future, irrespective of the amount of money transacted.\n"},{"metadata":{},"cell_type":"markdown","source":"**Contents:**\n\n 1. Exploratory Data Analysis\n           * Loading dataset and libraries\n           * Data cleaning\n           * Typecasting the attributes\n           * Target classes count        \n           * Missing value analysis\n        2. Attributes Distributions and trends\n           * Distribution of train attributes\n           * Distribution of test attributes\n           * Mean distribution of attributes\n           * Standard deviation distribution of attributes\n           * Skewness distribution of attributes\n           * Kurtosis distribution of attributes      \n           * Outliers analysis\n        4. Correlation matrix \n        5. Split the dataset into train and test dataset\n        7. Modelling the training dataset\n           * Logistic Regression Model\n           * SMOTE Model\n           * LightGBM Model\n        8. Cross Validation Prediction\n           * Logistic  Regression CV Prediction\n           * SMOTE CV Prediction\n           * LightGBM CV Prediction\n        9. Model performance on test dataset\n           * Logistic Regression Prediction\n           * SMOTE Prediction\n           * LightGBM Prediction\n        10. Model Evaluation Metrics\n           * Confusion Matrix\n           * ROC_AUC score\n        11. Choosing best model for predicting customer transaction"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score\nfrom sklearn.metrics import roc_auc_score,confusion_matrix,make_scorer,classification_report,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import ClusterCentroids,NearMiss, RandomUnderSampler\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn import tree\nimport graphviz\nfrom pdpbox import pdp, get_dataset, info_plots\nimport scikitplot as skplt\nfrom scikitplot.metrics import plot_confusion_matrix,plot_precision_recall_curve\n\n\nfrom scipy.stats import randint as sp_randint\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))\n\nrandom_state=42\nnp.random.seed(random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing the train dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#importing the train dataset\ntrain_df=pd.read_csv('../input/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shape of the train dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Shape of the train dataset\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Summary of the dataset\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target classes count**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#target classes count\ntarget_class=train_df['target'].value_counts()\nprint('Count of target classes :\\n',target_class)\n#Percentage of target classes count\nper_target_class=train_df['target'].value_counts()/len(train_df)*100\nprint('percentage of count of target classes :\\n',per_target_class)\n\n#Countplot and violin plot for target classes\nfig,ax=plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train_df.target.values,ax=ax[0],palette='husl')\nsns.violinplot(x=train_df.target.values,y=train_df.index.values,ax=ax[1],palette='husl')\nsns.stripplot(x=train_df.target.values,y=train_df.index.values,jitter=True,color='black',linewidth=0.5,size=0.5,alpha=0.5,ax=ax[1],palette='husl')\nax[0].set_xlabel('Target')\nax[1].set_xlabel('Target')\nax[1].set_ylabel('Index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**                   \n* We have a unbalanced data,where 90% of the data is the number of customers those will not make a transaction and 10% of the data is those who will make a transaction.\n* Look at the violin plots seems that there are no relationship between the target with the index of the train dataframe.This is more dominated by the zero targets then for the ones.\n* Look at the jitter plots with violin plots. We can observed that targets looks uniformly distributed over the indexs of the dataframe."},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of train attributes**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ndef plot_train_attribute_distribution(t0,t1,label1,label2,train_attributes):\n    i=0\n    sns.set_style('whitegrid')\n    \n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    \n    for attribute in train_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(t0[attribute],hist=False,label=label1)\n        sns.distplot(t1[attribute],hist=False,label=label2)\n        plt.legend()\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see first 100 test attributes will be displayed in next cell."},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\nt0=train_df[train_df.target.values==0]\nt1=train_df[train_df.target.values==1]\ntrain_attributes=train_df.columns.values[2:102]\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see next 100 test attributes will be displayed in next cell."},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\ntrain_attributes=train_df.columns.values[102:203]\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**\n* We can observed that their is a considerable number of features which are significantly have different distributions for two target variables. For example like var_0,var_1,var_9,var_198 var_180 etc.\n*  We can observed that their is a considerable number of features which are significantly have same distributions for two target variables. For example like var_3,var_7,var_10,var_171,var_185 etc.\n"},{"metadata":{},"cell_type":"markdown","source":"**Importing the test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#importing the test dataset\ntest_df=pd.read_csv('../input/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shape of the test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Shape of the test dataset\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of test attributes**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ndef plot_test_attribute_distribution(test_attributes):\n    i=0\n    sns.set_style('whitegrid')\n    \n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    \n    for attribute in test_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(test_df[attribute],hist=False)\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see first 100 test attributes will be displayed in next cell."},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\ntest_attributes=test_df.columns.values[1:101]\nplot_test_attribute_distribution(test_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see next 100 test attributes will be displayed in next cell."},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\ntest_attributes=test_df.columns.values[101:202]\nplot_test_attribute_distribution(test_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**\n* We can observed that their is a considerable number of features which are significantly have different distributions. \n  For example like var_0,var_1,var_9,var_180 var_198 etc.\n* We can observed that their is a considerable number of features which are significantly have same distributions. \n  For example like var_3,var_7,var_10,var_171,var_185,var_192 etc.\n"},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of mean values per rows and columns in train and test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Distribution of mean values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for mean values per column in train attributes\nsns.distplot(train_df[train_attributes].mean(axis=0),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for mean values per column in test attributes\nsns.distplot(test_df[test_attributes].mean(axis=0),color='green',kde=True,bins=150,label='test')\nplt.title('Distribution of mean values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of mean values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for mean values per row in train attributes\nsns.distplot(train_df[train_attributes].mean(axis=1),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for mean values per row in test attributes\nsns.distplot(test_df[test_attributes].mean(axis=1),color='green',kde=True, bins=150, label='test')\nplt.title('Distribution of mean values per row in train and test dataset')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of standard deviation(std) values per rows and columns in train and test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Distribution of std values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for std values per column in train attributes\nsns.distplot(train_df[train_attributes].std(axis=0),color='red',kde=True,bins=150,label='train')\n#Distribution plot for std values per column in test attributes\nsns.distplot(test_df[test_attributes].std(axis=0),color='blue',kde=True,bins=150,label='test')\nplt.title('Distribution of std values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of std values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for std values per row in train attributes\nsns.distplot(train_df[train_attributes].std(axis=1),color='red',kde=True,bins=150,label='train')\n#Distribution plot for std values per row in test attributes\nsns.distplot(test_df[test_attributes].std(axis=1),color='blue',kde=True, bins=150, label='test')\nplt.title('Distribution of std values per row in train and test dataset')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****"},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of skewness per rows and columns in train and test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Distribution of skew values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for skew values per column in train attributes\nsns.distplot(train_df[train_attributes].skew(axis=0),color='green',kde=True,bins=150,label='train')\n#Distribution plot for skew values per column in test attributes\nsns.distplot(test_df[test_attributes].skew(axis=0),color='blue',kde=True,bins=150,label='test')\nplt.title('Distribution of skewness values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of skew values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for skew values per row in train attributes\nsns.distplot(train_df[train_attributes].skew(axis=1),color='green',kde=True,bins=150,label='train')\n#Distribution plot for skew values per row in test attributes\nsns.distplot(test_df[test_attributes].skew(axis=1),color='blue',kde=True, bins=150, label='test')\nplt.title('Distribution of skewness values per row in train and test dataset')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us look distribution of kurtosis values per rows and columns in train and test dataset**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Distribution of kurtosis values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for kurtosis values per column in train attributes\nsns.distplot(train_df[train_attributes].kurtosis(axis=0),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for kurtosis values per column in test attributes\nsns.distplot(test_df[test_attributes].kurtosis(axis=0),color='red',kde=True,bins=150,label='test')\nplt.title('Distribution of kurtosis values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of kutosis values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for kurtosis values per row in train attributes\nsns.distplot(train_df[train_attributes].kurtosis(axis=1),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for kurtosis values per row in test attributes\nsns.distplot(test_df[test_attributes].kurtosis(axis=1),color='red',kde=True, bins=150, label='test')\nplt.title('Distribution of kurtosis values per row in train and test dataset')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing value analysis**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Finding the missing values in train and test data\ntrain_missing=train_df.isnull().sum().sum()\ntest_missing=test_df.isnull().sum().sum()\nprint('Missing values in train data :',train_missing)\nprint('Missing values in test data :',test_missing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values are present in both train and test data."},{"metadata":{},"cell_type":"markdown","source":"**Correlation between the attributes**"},{"metadata":{},"cell_type":"markdown","source":"We can observed that the correlation between the train attributes is very small."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Correlations in train attributes\ntrain_attributes=train_df.columns.values[2:202]\ntrain_correlations=train_df[train_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index()\ntrain_correlations=train_correlations[train_correlations['level_0']!=train_correlations['level_1']]\nprint(train_correlations.head(10))\nprint(train_correlations.tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observed that the correlation between the test attributes is very small."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Correlations in test attributes\ntest_attributes=test_df.columns.values[1:201]\ntest_correlations=test_df[test_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index()\ntest_correlations=test_correlations[test_correlations['level_0']!=test_correlations['level_1']]\nprint(test_correlations.head(10))\nprint(test_correlations.tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation plot for train and test data**"},{"metadata":{},"cell_type":"markdown","source":"We can observed from correlation distribution plot that the correlation between the train and test attributes is very very small, it means that features are independent each other."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Correlations in train data\ntrain_correlations=train_df[train_attributes].corr()\ntrain_correlations=train_correlations.values.flatten()\ntrain_correlations=train_correlations[train_correlations!=1]\ntest_correlations=test_df[test_attributes].corr()\n#Correlations in test data\ntest_correlations=test_correlations.values.flatten()\ntest_correlations=test_correlations[test_correlations!=1]\n\nplt.figure(figsize=(20,5))\n#Distribution plot for correlations in train data\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\n#Distribution plot for correlations in test data\nsns.distplot(test_correlations, color=\"Blue\", label=\"test\")\nplt.xlabel(\"Correlation values found in train and test\")\nplt.ylabel(\"Density\")\nplt.title(\"Correlation distribution plot for train and test attributes\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering**"},{"metadata":{},"cell_type":"markdown","source":"Let us do some feature engineering by using\n* Permutation importance\n* Partial dependence plots"},{"metadata":{},"cell_type":"markdown","source":"**Permutation importance**"},{"metadata":{},"cell_type":"markdown","source":"Permutation variable importance measure in a random forest for classification and regression."},{"metadata":{"trusted":false},"cell_type":"code","source":"#training data\nX=train_df.drop(columns=['ID_code','target'],axis=1)\ntest=test_df.drop(columns=['ID_code'],axis=1)\ny=train_df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us build simple model to find features which are more important."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Split the training data\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,random_state=42)\n\nprint('Shape of X_train :',X_train.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train.shape)\nprint('Shape of y_valid :',y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random forest classifier**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Random forest classifier\nrf_model=RandomForestClassifier(n_estimators=10,random_state=42)\n#fitting the model\nrf_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us calculate weights and show important features using eli5 library."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom eli5.sklearn import PermutationImportance\nperm_imp=PermutationImportance(rf_model,random_state=42)\n#fitting the model\nperm_imp.fit(X_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see important features,"},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\n#Important features\neli5.show_weights(perm_imp,feature_names=X_valid.columns.tolist(),top=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take aways:\n* Importance of the features decreases as we move down the top of the column.\n* As we can see the features shown in green indicate that they have a positive impact on our prediction\n* As we can see the features shown in white indicate that they have no effect on our prediction\n* As we can see the features shown in red indicate that they have a negative impact on our prediction\n* The most important feature is 'Var_81'"},{"metadata":{},"cell_type":"markdown","source":"**Partial dependence plots**"},{"metadata":{},"cell_type":"markdown","source":"Partial dependence plot gives a graphical depiction of the marginal effect of a variable on the class probability or classification.While feature importance shows what variables most affect predictions, but partial dependence plots show how a feature affects predictions."},{"metadata":{},"cell_type":"markdown","source":"Let us calculate partial dependence plots on random forest"},{"metadata":{},"cell_type":"markdown","source":"**Partial dependence plot**"},{"metadata":{},"cell_type":"markdown","source":"Let us see impact of the main features which are discovered in the previous section by using the pdpbox."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Create the data we will plot 'var_81'\nfeatures=[v for v in X_valid.columns if v not in ['ID_code','target']]\npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_81')\n#plot feature \"var_81\"\npdp.pdp_plot(pdp_data,'var_81')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**\n* The y_axis is interpreted as change in prediction from what it would be predicted at the baseline.\n* The blue shaded area indicates the level of confidence of 'var_81'."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Create the data we will plot \npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_109')\n#plot feature \"var_109\"\npdp.pdp_plot(pdp_data,'var_109')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**\n* The y_axis is interpreted as change in prediction from what it would be predicted at the baseline.\n* The blue shaded area indicates the level of confidence of 'var_109'."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Create the data we will plot \npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_12')\n#plot feature \"var_12\"\npdp.pdp_plot(pdp_data,'var_12')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take aways:**\n* The y_axis is interpreted as change in prediction from what it would be predicted at the baseline.\n* The blue shaded area indicates the level of confidence of 'var_12'."},{"metadata":{},"cell_type":"markdown","source":"**Handling of imbalanced data**\n\nNow we are going to explore 6 different approaches for dealing with imbalanced datasets.\n* Change the performance metric\n* Oversample minority class\n* Undersample majority class\n* Synthetic Minority Oversampling Technique(SMOTE)\n* Change the algorithm"},{"metadata":{},"cell_type":"markdown","source":"Now let us start with simple Logistic regression model."},{"metadata":{},"cell_type":"markdown","source":"**Split the train data using StratefiedKFold cross validator**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Training data\nX=train_df.drop(['ID_code','target'],axis=1)\nY=train_df['target']\n#StratifiedKFold cross validator\ncv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\nfor train_index,valid_index in cv.split(X,Y):\n    X_train, X_valid=X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid=Y.iloc[train_index], Y.iloc[valid_index]\n\nprint('Shape of X_train :',X_train.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train.shape)\nprint('Shape of y_valid :',y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression model**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Logistic regression model\nlr_model=LogisticRegression(random_state=42)\n#fitting the lr model\nlr_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy of model**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Accuracy of the model\nlr_score=lr_model.score(X_train,y_train)\nprint('Accuracy of the lr_model :',lr_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross validation prediction of lr_model**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Cross validation prediction\ncv_predict=cross_val_predict(lr_model,X_valid,y_valid,cv=5)\n#Cross validation score\ncv_score=cross_val_score(lr_model,X_valid,y_valid,cv=5)\nprint('cross_val_score :',np.average(cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of the model is not the best metric to use when evaluating the imbalanced datasets as it may be misleading. So, we are going to change the performance metric."},{"metadata":{},"cell_type":"markdown","source":"**Confusion matrix**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Confusion matrix\ncm=confusion_matrix(y_valid,cv_predict)\n#Plot the confusion matrix\nplot_confusion_matrix(y_valid,cv_predict,normalize=False,figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reciever operating characteristics (ROC)-Area under curve(AUC) score and curve**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#ROC_AUC score\nroc_score=roc_auc_score(y_valid,cv_predict)\nprint('ROC score :',roc_score)\n\n#ROC_AUC curve\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_valid,cv_predict)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we compare the roc_auc_score and model accuracy , model is not performing well on imbalanced data."},{"metadata":{},"cell_type":"markdown","source":"**Classification report**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Classification report\nscores=classification_report(y_valid,cv_predict)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observed that f1 score is high for number of customers those who will not make a transaction then the who will make a transaction. So, we are going to change the algorithm."},{"metadata":{},"cell_type":"markdown","source":"**Model performance on test data**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Predicting the model\nX_test=test_df.drop(['ID_code'],axis=1)\nlr_pred=lr_model.predict(X_test)\nprint(lr_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Oversample minority class:**\n* It can be defined as adding more copies of minority class.\n* It can be a good choice when we don't have a ton of data to work with.\n* Drawback is that we are adding information.This may leads to overfitting and poor performance on test data.\n"},{"metadata":{},"cell_type":"markdown","source":"**Undersample majority class:**\n* It can be defined as removing some observations of the majority class.\n* It can be a good choice when we have a ton of data -think million of rows.\n* Drawback is that we are removing information that may be valuable.This may leads to underfitting and poor performance on test data."},{"metadata":{},"cell_type":"markdown","source":"Both Oversampling and undersampling techniques have some drawbacks. So, we are not going to use this models for this problem and also we will use other best algorithms."},{"metadata":{},"cell_type":"markdown","source":"**Synthetic Minority Oversampling Technique(SMOTE)**"},{"metadata":{},"cell_type":"markdown","source":"SMOTE uses a nearest neighbors algorithm to generate new and synthetic data to used for training the model."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom imblearn.over_sampling import SMOTE\n#Synthetic Minority Oversampling Technique\nsm = SMOTE(random_state=42, ratio=1.0)\n#Generating synthetic data points\nX_smote,y_smote=sm.fit_sample(X_train,y_train)\nX_smote_v,y_smote_v=sm.fit_sample(X_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see how baseline logistic regression model performs on synthetic data points."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Logistic regression model for SMOTE\nsmote=LogisticRegression(random_state=42)\n#fitting the smote model\nsmote.fit(X_smote,y_smote)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy of model**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Accuracy of the model\nsmote_score=smote.score(X_smote,y_smote)\nprint('Accuracy of the smote_model :',smote_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation prediction of smoth_model"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Cross validation prediction\ncv_pred=cross_val_predict(smote,X_smote_v,y_smote_v,cv=5)\n#Cross validation score\ncv_score=cross_val_score(smote,X_smote_v,y_smote_v,cv=5)\nprint('cross_val_score :',np.average(cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion matrix**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Confusion matrix\ncm=confusion_matrix(y_smote_v,cv_pred)\n#Plot the confusion matrix\nplot_confusion_matrix(y_smote_v,cv_pred,normalize=False,figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reciever operating characteristics (ROC)-Area under curve(AUC) score and curve**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#ROC_AUC score\nroc_score=roc_auc_score(y_smote_v,cv_pred)\nprint('ROC score :',roc_score)\n\n#ROC_AUC curve\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_smote_v,cv_pred)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification report**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Classification report\nscores=classification_report(y_smote_v,cv_pred)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model performance on test data**"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#Predicting the model\nX_test=test_df.drop(['ID_code'],axis=1)\nsmote_pred=smote.predict(X_test)\nprint(smote_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observed that smote model is performing well on imbalance data compare to logistic regression."},{"metadata":{},"cell_type":"markdown","source":"**LightGBM:**\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. We are going to use LightGBM model.\n"},{"metadata":{},"cell_type":"markdown","source":"Let us build LightGBM model"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Training the model\n#training data\nlgb_train=lgb.Dataset(X_train,label=y_train)\n#validation data\nlgb_valid=lgb.Dataset(X_valid,label=y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**choosing of  hyperparameters**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Selecting best hyperparameters by tuning of different parameters\nparams={'boosting_type': 'gbdt', \n          'max_depth' : -1, #no limit for max_depth if <0\n          'objective': 'binary',\n          'boost_from_average':False, \n          'nthread': 12,\n          'metric':'auc',\n          'num_leaves': 100,\n          'learning_rate': 0.08,\n          'max_bin': 950,      #default 255\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 1.2, #L1 regularization(>0)\n          'reg_lambda': 1.2,#L2 regularization(>0)\n          'min_split_gain': 0.5, #>0\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'is_unbalance':True,\n          'scale_pos_weight': 1,\n          }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training the lgbm model**"},{"metadata":{"trusted":false},"cell_type":"code","source":"num_rounds=3000\nlgbm= lgb.train(params,lgb_train,num_rounds,valid_sets=[lgb_train,lgb_valid],verbose_eval=100,early_stopping_rounds = 2000)\nlgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**lgbm model performance on test data**"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test=test_df.drop(['ID_code'],axis=1)\n#predict the model\n#probability predictions\nlgbm_predict_prob=lgbm.predict(X_test,random_state=42,num_iteration=lgbm.best_iteration)\n#Convert to binary output 1 or 0\nlgbm_predict=np.where(lgbm_predict_prob>=0.5,1,0)\nprint(lgbm_predict_prob)\nprint(lgbm_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us plot the important features**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#plot the important features\nlgb.plot_importance(lgbm,max_num_features=150,importance_type=\"split\",figsize=(20,50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion :**\n\nWe tried model with logistic regression,smote and lightgbm. But, both smote and lightgbm model is performing well on imbalanced data compared to other models based on scores of roc_auc_score."},{"metadata":{"trusted":false},"cell_type":"code","source":"#final submission\nsub_df=pd.DataFrame({'ID_code':test_df['ID_code'].values})\nsub_df['Target']=lgbm_predict_prob\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}