{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nfrom sklearn.metrics import roc_auc_score, roc_curve\nwarnings.filterwarnings('ignore')\nfrom xgboost import XGBClassifier\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ndf_test=pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the null values\n\ntotal=df_train.isnull().sum().sort_values(ascending = False)\npercent=(df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending = False)\nmissing_data=pd.concat([total,percent],axis =1,keys=['Total','Percent'])\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NO NULL VALUES"},{"metadata":{},"cell_type":"markdown","source":"**the target variable: Distribution of 'target'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing percentage of target variable\n\ndf_train.target.value_counts().plot(kind='pie',autopct='%1.0f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So by sseing pie chart it is clear the data set is imbalance as 90% of target variable has value 0."},{"metadata":{},"cell_type":"markdown","source":"**univariate analysis of target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#skewness and kurtosis\n\nsns.distplot(df_train['target'])\nprint(\"Skewness: %f\" % df_train['target'].skew())\nprint(\"Kurtosis: %f\" % df_train['target'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Filter method\n\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = df_train.corr()\n#sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output variable\ncor_target = abs(cor[\"target\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.03]\nrelevant_features=list(relevant_features.index)\ntype(relevant_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_features=relevant_features[1:]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(relevant_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for correlation between filter_features\n\ncor1=abs(df_train[relevant_features].corr())\ncor1[cor1>0.01].stack()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so all the filter features are highly uncorrelated with each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"a=['ID_code','target']\nrelevant_features.extend(a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1=df_train[relevant_features]\ndf_train1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Box plot of filter_variables1 for checking the distriution\nplt.subplots(figsize=(18,8))\ndf_train1.boxplot(rot=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram for filter_variables1\ndf_train1.hist(figsize=(20,20));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one = df_train1[df_train1['target']==1]\nzero = df_train1[df_train1['target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var=df_train1.columns[:-2]\nfig, ax = plt.subplots(10,9,figsize=(20,20))\nj=0\nfor i in var:\n    j+=1\n    plt.subplot(10,9,j)\n    sns.distplot(one[i], color='r')\n    sns.distplot(zero[i], color='b')\n    plt.title(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df_train1.drop(['ID_code','target'],axis=1)\ny = df_train1['target']\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\n\n#lr=[0.01,0.05,0.1,0.2,0.3]\n#base_learners = [50,100,150,200,250,300,350]\n#Depths = [3,5,7,9,11,13,15]\n#childweight=[80,85,90]\n#lossfunc=[0.1,0.2,0.3,0.4]\n#subsampl=[0.5,1]\n\n#param_grid = {'gamma':lossfunc}\n              #'learning_rate': lr\n              #'n_estimators': base_learners\n              #'max_depth':Depths,\n              #'min_child_weight':childweight\n              #\n              #'subsample': subsampl,\n              \n              \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from xgboost import XGBClassifier\n\n\n#clf = XGBClassifier(objective=\"binary:logistic\",max_delta_step=0,learning_rate=0.2,n_estimators=200,max_depth=3,min_child_weight=80)\n#model = GridSearchCV(clf, param_grid, scoring = 'roc_auc', cv=3, n_jobs = -1,pre_dispatch=2)\n#model.fit(train_X, train_y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#print(\"Best: %f using %s\" % (model.best_score_, model.best_params_))\n#means = model.cv_results_['mean_test_score']\n#stds = model.cv_results_['std_test_score']\n#params = model.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimal value of number of base learners\noptimal_learners = 2000\nprint(\"The optimal number of base learners is : \",optimal_learners)\n# Optimal value of depth\noptimal_depth = 3\nprint(\"\\nThe optimal value of depth is : \",optimal_depth)\n# Optimal value of childweight\noptimal_childweight = 80\nprint(\"\\nThe optimal value of childweight is : \",optimal_childweight)\n# Optimal value of lossfunc\noptimal_lossfunc = 0.2\nprint(\"\\nThe optimal value of lossfunction is : \",optimal_lossfunc)\n# Optimal value of subsampl\noptimal_subsampl = 0.5\nprint(\"\\nThe optimal value of subsample is : \",optimal_subsampl)\n# Optimal value of lr\noptimal_lr = 0.2\nprint(\"\\nThe optimal value of lr is : \",optimal_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom xgboost import XGBClassifier\n\nprint(datetime.now())\n\nclffinal = XGBClassifier(objective = \"binary:logistic\",n_estimators= optimal_learners,max_delta_step=0 ,max_depth=optimal_depth,\n                         min_child_weight=optimal_childweight, gamma=optimal_lossfunc, subsample= optimal_subsampl,\n                         learning_rate= optimal_lr,eval_metric =\"auc\",n_jobs = -1).fit(train_X,train_y)\nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clffinal.predict_proba(val_X)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(\"Training score :\" + str(roc_auc_score(train_y,clffinal.predict(train_X))))\nprint(\"validation score :\" + str(roc_auc_score(val_y,y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=relevant_features[:-2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = df_test.drop(['ID_code'],axis=1)\ntest_X = test_X[b]\nTest_Prediction = clffinal.predict(test_X)\n\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = Test_Prediction\nsub_df.to_csv(\"submission_final.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}