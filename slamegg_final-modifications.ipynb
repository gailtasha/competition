{"cells":[{"metadata":{},"cell_type":"markdown","source":"We started the project by importing general libraries and reading csv files\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ndataset_test = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we split the dataset_train to X_train and y_train and dataset_test to X_test and y_test. This procedure is done in order to split necessary column from train and test dataset (the column name is 'target' and 'ID_code')"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dataset_train.iloc[:, dataset_train.columns != 'target']\ny_train = dataset_train.iloc[:, 1].values\nX_test = dataset_test.iloc[:, dataset_test.columns != 'ID_code'].values\ny_test = dataset_test.iloc[:, 1].values\nX_train = X_train.iloc[:, X_train.columns != 'ID_code'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we implemented xgboost classifier in order to predict probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By making calibration, we've found the best values for each option"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid = {'max_depth': [3], \n#               'gamma': [0,9],\n#               'n_estimators': [1000],\n#               'tree_method': ['gpu_hist'],\n#               'n_gpus': [1],\n#               'colsample_bytree': [0.1,1],\n#               'subsample': [0.82],\n#               'scale_pos_weight': [8.951238929246692]\n             \n#              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with ignore_warnings(category=DeprecationWarning):\n#     xgb_grid = GridSearchCV(xgb, param_grid, cv=10, refit=True, verbose=1, n_jobs=-1)\n#     xgb_grid.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By implementing the following calculations we found the most suitable value for last option named 'scale_pos_weight'"},{"metadata":{"trusted":true},"cell_type":"code","source":"neg = len(y_train)-sum(y_train)\npos = sum(y_train)\nscale_pos_weight  = float(neg/pos)\nscale_pos_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By doing gridsearch, we identified the best values for rest options, and implemented them into main function called 'XGBClassifier'"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier(scale_pos_weight=scale_pos_weight,\n                        objective='binary:logistic',\n                        random_state= 21,\n                        subsample=0.83,\n                        tree_method = 'gpu_hist',\n                        learning_rate = 0.1, ## From initial gridsearch\n                        n_estimators = 1000 ,  ## From initial gridsearch\n                        tree_depth= 3     ## From initial gridsearch\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we fit generated XGB with X_train and y_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It takes some time, but finally fitting is succesfully completed. And after that, using XGB we made prediction using existing X_test array."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xg = XGB.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an current accuracy score for XGB prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction value is 0.74202, but it is not the final value"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train, y_pred_xg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to meet the requirements, we assigned each ID to predicted values, and put them into new dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_xg = pd.concat((dataset_test.ID_code, pd.Series(y_pred_xg).rename('target')), axis = 1)\ndataset_xg.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final step for XGBoost is to export dataset in .csv format in order to make submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_xg.to_csv('xg_boost_gpu_newlast1_submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And after successful export, we implemented the next algorithm called Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As previously mentioned, we also fitted X_train and y_train which we successfully splited at the beginning. And after predicting probability, we assigned their values to newly created array, in order to cut unnecessary columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"GNB.fit(X_train,y_train)\ny_preds_test = GNB.predict_proba(X_test)\n\nprobs_pos_test_gnb_one  = []\nfor pred in y_preds_test:\n    probs_pos_test_gnb_one.append(pred[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_private = 0.85107\nxgboost_private = 0.80526\ngnb_private = 0.88763\n\nlog_reg_public = 0.84947\nxgboost_public = 0.80901\ngnb_public = 0.88848","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we plot the histogram using probability value"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# the histogram of the data\nn, bins, patches = plt.hist(probs_pos_test_gnb_one, 50, density=1, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('Probability')\nplt.ylabel('GNB_values')\nplt.title('GNB')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_gnb = pd.concat((dataset_test.ID_code, pd.Series(probs_pos_test_gnb).rename('target')), axis = 1)\ndataset_gnb.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally exporting dataset in .csv format"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_gnb.to_csv('gnb_submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Turn to Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we fit X_traing and y_train using logisticRegression classifier with class_weight = balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"logist = LogisticRegression(C=0.001, class_weight='balanced')\n\nlogist.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Like GNB, we also predicted probability of X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"logist_pred = logist.predict_proba(X_test)\nlogist_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And assigned them into new array"},{"metadata":{"trusted":true},"cell_type":"code","source":"probs_pos_test_log  = []\nfor pred in logist_pred:\n    probs_pos_test_log.append(pred[1])\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"THen we plot histogram of LogReg"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the histogram of the data\nn, bins, patches = plt.hist(probs_pos_test_log, 50, density=1, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('Probability')\nplt.ylabel('Log_reg_val')\nplt.title('Log')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, exported them in .csv format"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_log = pd.concat((dataset_test.ID_code, pd.Series(probs_pos_test_log).rename('target')), axis = 1)\ndataset_log.target.value_counts()\ndataset_log.to_csv('log_submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After all steps, we got private and public score for each method. The graphs are build according this values."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['log_reg', 'xg_boost', 'gnb']\nvalues = [log_reg_private, xgboost_private, gnb_private]\n\nplt.figure(figsize=(15, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\nvalues\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['log_reg', 'xg_boost', 'gnb']\nvalues = [log_reg_public, xgboost_public, gnb_public]\n\nplt.figure(figsize=(15, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\nvalues\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}