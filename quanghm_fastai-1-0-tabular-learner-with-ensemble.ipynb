{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport random\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport featuretools as ft\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom fastai import *\nfrom fastai.tabular import *\nfrom fastai.basic_data import DataBunch\n# from tqdm import tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n%reload_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Data Load and Exploration"},{"metadata":{"trusted":true,"_uuid":"fe71cb7397b7d00ecc8119c7d6723ebaa720ac7d"},"cell_type":"code","source":"indir = '../input'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a003ecb4d01e1667131015f490107ca829631360","scrolled":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(indir, 'train.csv'))\ndf_summary = df.describe(); df_summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7d535b76a5f44854f69bc57e736a65dec945be6"},"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(indir, 'test.csv')).set_index('ID_code')\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize Data\nWe could let FastAI normalize the data automatically, but we choose to do so manually for more flexibility"},{"metadata":{"trusted":true,"_uuid":"e7775d60b85439ad5e2ed0a39b674089e3ca1cff","scrolled":true},"cell_type":"code","source":"# get the features list\nfeatures = list(test_df.columns)\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dmean, dmin, dmax = df_summary.loc['mean'],df_summary.loc['min'], df_summary.loc['max'] \ndrange = dmax - dmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:, features] = (df[features] - dmin[features])/drange[features]\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.loc[:, features] = (test_df[features] - dmin[features])/drange[features]\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nWe don't really do anything special but polynomial features"},{"metadata":{"trusted":true,"_uuid":"0aa875abee784e13275eac53e14d6d9ba6ee4b77"},"cell_type":"code","source":"def augment_df(df):\n    for feature in features:\n        df[f'sq_{feature}'] = df[feature]**2\n        df[f'repo_{feature}'] = df[feature].apply(lambda x: 0 if x==0 else 1/x)\n        df[f'repo_sq_{feature}'] = df[f'repo_{feature}']**2\n        df[f'cube_{feature}'] = df[feature]**3\n#         df[f'repo_cube_{feature}'] = df[f'repo_{feature}']**3\n#         df[f'p4_{feature}'] = df[feature]**4\n#         df[f'sin_{feature}'] = sin(df[feature])\n#         df[f'exp_{feature}'] = exp(df[feature])\n#         df[f'log_{feature}'] = df[f'sq_{feature}'].apply(lambda x: 0 if x==0 else log(x))\n    \n    df['min'] = df[features].min(axis=1)\n    df['mean'] = df[features].mean(axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['sum'] = df[features].sum(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['var'] = df[features].var(axis=1)\n    df['abs_sum'] = df[features].abs().sum(axis=1)\n    df['abs_mean'] = df[features].abs().mean(axis=1)\n    df['abs_median'] = df[features].abs().median(axis=1)\n    df['abs_std'] = df[features].abs().std(axis=1)\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['sq_kurt'] = df[[f'sq_{feature}' for feature in features]].kurt(axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9bcc84443e66c866807e636b70593f1bd4d57ac"},"cell_type":"code","source":"%%time\naugment_df(df)\naugment_df(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7917673b9cb0272c4abef88052b9da4f750c73cc"},"cell_type":"code","source":"features = list(test_df.columns[:-12])\nstats_features = list(test_df.columns[-12:])\nnum_features = len(features)\nnum_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22611ce399dcfeae0d5086b97b4044832de7a3ed"},"cell_type":"code","source":"### Feature Understanding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8764cf92aeb1c8bf79f9ff9fa2051f16093572b"},"cell_type":"code","source":"def view_dist(df, columns, row=10, col=10):\n    fig, axes = plt.subplots(10,10,figsize=(30,30))\n    axes = axes.flatten()\n    for col, ax in zip(columns, axes):\n        sns.kdeplot(df.loc[df.target==0, col], ax=ax, color='r', label='0')\n        sns.kdeplot(df.loc[df.target==1, col], ax=ax, color='b', label='1')\n        ax.legend()\n        ax.set_title(f'{col}')\n        \n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20f5597a1450968cd9f168d2a9e18347c580ef63"},"cell_type":"code","source":"# view_dist(df, features[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace3a9ed6cc564cfd836b0ebe676ea8832bee155"},"cell_type":"markdown","source":"## Split training data into train and validation sets"},{"metadata":{"trusted":true,"_uuid":"4334a73e6a5ec3238c44cef9cf4e0cb1c7eaddfe"},"cell_type":"code","source":"# seed = 2019\n# train_samples = df.sample(frac=0.95, random_state=seed)\n# valid_samples = df.drop(train_samples.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13d206555afc58c8c7a64130bde1c06f06aa41c6"},"cell_type":"code","source":"random.seed(31415926)\nvalid_idx = random.sample(list(df.index.values), int(len(df)*0.2) )\ntrain_idx = df.drop(valid_idx).index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"421d122e215733a9d1bb3c8eaef8e2035518c91e"},"cell_type":"markdown","source":"Grab a statistic summary of the training set. We may use this later in adding noises to the data during training"},{"metadata":{"trusted":true,"_uuid":"5f6ab8efbc2784a11e06053ea42b87b5852121cd"},"cell_type":"code","source":"# verify that positive sample distribution in validation set is similar to that of the whole data\ndf.iloc[valid_idx].target.sum() / len(valid_idx) , df.target.sum() / len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c6e2916a79a765d576551c32858773b380dc2a1"},"cell_type":"code","source":"class roc(Callback):\n    '''\n    Updated on March 28 2019 to reflect new change in FastAI's Callback\n    ROC_AUC metric callback for fastai. Compute ROC score over each batch and returns the average over batches.\n    TO DO: rolling average\n    '''\n    def on_epoch_begin(self, **kwargs):\n        self.total = 0\n        self.batch_count = 0\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        preds = F.softmax(last_output, dim=1)\n        # roc_auc_score does not work on batches which does not contain both classes.\n        try:\n            roc_score = roc_auc_score(to_np(last_target), to_np(preds[:,1]))\n            self.total += roc_score\n            self.batch_count += 1\n        except:\n            pass\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        return add_metrics(last_metrics, self.total/self.batch_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7d4f07265c6fc6e8917cef11d577dcf81cdeedf"},"cell_type":"markdown","source":"## FastAI Tabular Learner\nWe start off with the default learner from FastAI"},{"metadata":{"trusted":true,"_uuid":"13f768fd1f8bc3f568f6e1c8292552f2a03c9c76"},"cell_type":"code","source":"BATCH_SIZE = 2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"228b1c1dbeaecf19a6fbbf9537dbde99eb997a9c"},"cell_type":"markdown","source":"First we want to find the correct learning rate for this dataset/problem. This only needs to run once.\nThe *optimal* learning rate found is 0.01"},{"metadata":{"trusted":true,"_uuid":"391ad4bb26af7c819b74501fcf94a14ef1a78384"},"cell_type":"code","source":"# data = TabularDataBunch.from_df(path='.', df=df, \n#                                 dep_var='target', \n#                                 valid_idx=valid_samples.index, \n#                                 cat_names=[], \n#                                 cont_names=features, \n#                                 procs=[tabular.transform.Normalize],\n#                                 test_df=test_df)\n\n#learner = tabular_learner(data, layers=[200,100], ps=[0.5,0.2], metrics=[accuracy, roc()])\n\n#learner.lr_find()\n#learner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0fc1d69b99e96840f23d91e5081b7537ce518a"},"cell_type":"markdown","source":"This is the main train and evaluate function. Since we are training multiple learners, we choose to save the model to harddisk and load them later if needed."},{"metadata":{"trusted":true,"_uuid":"0251d89e865b4363ad939cc6b8d37d06815a07d9"},"cell_type":"code","source":"def train_and_eval_tabular_learner(train_df,\n                                   train_features, \n                                   valid_idx,\n                                   add_noise=False,\n                                   lr=0.02, epochs=1, layers=[200, 50], ps=[0.5, 0.2], name='learner'):\n    \n    data = TabularDataBunch.from_df(path='.', df=train_df, \n                                    dep_var='target', \n                                    valid_idx=valid_idx, \n                                    cat_names=[], \n                                    cont_names=train_features, \n                                    bs=BATCH_SIZE,\n                                    procs=[],\n                                    test_df=test_df)\n    learner = tabular_learner(data, layers=layers, ps=ps, metrics=[roc()])\n\n    learner.fit_one_cycle(epochs, lr)\n\n    # learner.save(name,with_opt=False)\n        \n    # run prediction on validation set\n    valid_predicts, _ = learner.get_preds(ds_type=DatasetType.Valid)\n    valid_probs = np.array(valid_predicts[:,1])\n    valid_targets = train_df.loc[valid_idx].target.values\n    valid_score = roc_auc_score(valid_targets, valid_probs)\n    \n    # run prediction on test    \n    test_predicts, _ = learner.get_preds(ds_type=DatasetType.Test)\n    test_probs = to_np(test_predicts[:, 1])\n\n    return valid_score, valid_probs, test_probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b609d8da569d9d93aea092ac65c8633fcde24fd2","scrolled":false},"cell_type":"code","source":"%%time\nsub_features = []\nvalid_scores = []\nvalid_predictions = []\npredictions = []\nnum_learner = 1000\nnum_epochs = 5\nsaved_model_prefix = 'learner'\n\nfor i in range(num_learner):\n    print('training model {:}'.format(i))\n    sub_features.append(random.sample(list(features), int(num_features*0.5)) + stats_features)\n    name = f'{saved_model_prefix}_{i}'\n\n    score, valid_probs, test_probs = train_and_eval_tabular_learner(df,\n                                                                    sub_features[-1], \n                                                                    valid_idx, \n                                                                    epochs=num_epochs, \n                                                                    lr=0.01, \n                                                                    name=name)\n    \n    valid_scores.append(score)\n    valid_predictions.append(valid_probs)\n    predictions.append(test_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d7bc2bfd4f597a3aed603feab7d35f49cb13848"},"cell_type":"code","source":"print(valid_scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ccdf700717f76505e7dd6c255025105076a3a4"},"cell_type":"markdown","source":"## Visualize ROC on the Validation Set"},{"metadata":{"trusted":true,"_uuid":"938806c79f2f01a26fb8e598a68e3168fa49f429"},"cell_type":"code","source":"# roc_auc_score on validation set\naverage_valid_predicts = sum(valid_predictions)/len(valid_predictions)\nvalid_auc_score = roc_auc_score(df.iloc[valid_idx].target, average_valid_predicts); valid_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faea258c3b49f8003871150f5f84bc9e06d8fd67","scrolled":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_true=df.iloc[valid_idx].target,y_score=average_valid_predicts)\nplt.figure(figsize=(9,9))\nplt.plot(fpr, tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb9988d9619465d90c98ee7b78e6eded2198a9f"},"cell_type":"markdown","source":"## Test and Submit"},{"metadata":{"trusted":true,"_uuid":"93d29d419845aeb1452e0b4d2ac6160bc28da610"},"cell_type":"code","source":"# this is if we want to average only on the models that score more than average\n# predicts = np.zeros(predictions[0].shape)\n# counts = 0\n# for i in range(num_epochs):\n#     if valid_scores[i] > average_valid_score:\n#         predicts += predictions[i]\n#         counts += 1\n        \n# print(\"number of models: {:}\".format(counts))\n\n# predicts = sum(predictions)/counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5500678a1a9dad68c77da84144ce8783e9557d77"},"cell_type":"code","source":"test_df['target'] = sum(predictions)/len(valid_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acdca2039e34fe40e3ab2cf524ca93b1440e9e47"},"cell_type":"code","source":"# add timestamp to submission\nfrom datetime import datetime\nnow = datetime.now()\nmodel_time = now.strftime(\"%Y%m%d-%H%M\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca38f6cb59f04f91bd5a82d00ce7b1b35ef810c3"},"cell_type":"code","source":"test_df[['target']].to_csv(f'submission_fastai_ensemble_{model_time}_{valid_auc_score}.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17b177e942e64469f455fe074c64b03f28478f7a"},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(f'submission_fastai_ensemble_{model_time}_{valid_auc_score}.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}