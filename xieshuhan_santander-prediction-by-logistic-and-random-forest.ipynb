{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import the package that will be needed for the project.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary data analysis including info,description,head name, and column name.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As a data quality specialist, always check the data quality issues, e.g. whether there will be missing values, null values, and etc. And if there are a few ways to figure it out, for example, fill in the blank with 0, mean value, or expand the data set to minimize the influence caused by missing values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_missing_value(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are no missing values in the train data set.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_missing_value(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are no missing values in the test data set either, which makes the data cleansing step easier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_missing_value(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The visualization part of the data analysis, the only visualization for the train/test data set I can think of is I am trying to find out whether the data is correlated. Below is the finding, unfortunately, they are not correlated AT ALL.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = train.drop([\"target\"], axis=1).corr()\ntrain_corr = train_corr.values.flatten()\ntrain_corr = train_corr[train_corr != 1]\n\ntest_corr = test.corr()\ntest_corr = test_corr.values.flatten()\ntest_corr = test_corr[test_corr != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_corr, color=\"Red\", label=\"train\")\nsns.distplot(test_corr, color=\"Yellow\", label=\"test\")\nplt.xlabel(\"Correlation values found in train\")\nplt.ylabel(\"Density\")\nplt.title(\"Correlation Relationship\"); \nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now is the machine learning part, since the out put is 0/1(yes/no), it will be not a regression model; moreover, when I got some more complicated case, I always try different model to check which one is more accurate; for this case, I choose logistic regression and random forest to predict**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['target']\nX = train.iloc[:, 2:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will separate the train data to train/dev data set to do the machine learning, set the test size, and random state, with a summary of the data set.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3, random_state=101)\nprint('X_train shape: ', X_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('X_dev shape: ', X_dev.shape)\nprint('y_dev shape: ', y_dev.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import the logistic regression model from sklearn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C = 0.001, class_weight = 'balanced')\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make prediction, and see the prediction data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_lr = lr.predict_proba(X_dev)[:,1]\nprediction_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**See how the model fits the data by classification_report and confusion_matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprediction_lr_var = [0 if i < 0.5 else 1 for i in prediction_lr]\nprint(confusion_matrix(y_dev, prediction_lr_var))\nprint('\\n')\nprint(classification_report(y_dev, prediction_lr_var))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As the model fit the data well, I will use the model to test the test data set, and come up with the prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_var = test.drop(columns = ['ID_code'])\nlr_pred_test = lr.predict_proba(test_var)[:,1]\n\nsubmit = test[['ID_code']]\nsubmit['target'] = lr_pred_test\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When I got a project, I always try with mutiple machine learning models, here is the ensembling model which is the random forest part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, random_state = 101, verbose = 1, class_weight = 'balanced', max_features = 0.5, min_samples_leaf = 100)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_rfc = rfc.predict(X_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprediction_rfc_var = [0 if i < 0.5 else 1 for i in prediction_rfc]\nprint(confusion_matrix(y_dev, prediction_rfc_var))\nprint('\\n')\nprint(classification_report(y_dev, prediction_rfc_var))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make prediction by using the random forest model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_pred_test = rfc.predict_proba(test_var)[:,1]\n\nsubmit = test[['ID_code']]\nsubmit['target'] = rfc_pred_test\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('lr_Santnader.csv', index = False)\nsubmit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}