{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CPU vs GPU with GBM - 400 features + augmentation\n---\n\nThe aim of this notebook is to compare two GBM models trained using CPU and with the [Kaggle's free GPU](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu). \n\nI have already compared the [200 models (2 features each)](https://github.com/FedericoRaimondi/me/blob/master/Santander_Customer_Transaction_Prediction/PredictiveAnalysis_ModelTuning/PredictiveAnalysis_ModelTuning_GPU.ipynb) version and CPU performed better for LGBM because of the not large enough training datasets.\n\nAfter [this discussion](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89004#521415) with [Chris Deotte](https://www.kaggle.com/cdeotte), I decided to compare also a 400 features + augmentation model to see if GPU wins against CPU.\n\n**EDIT**: Chris correctly suggested to try also other kind of GBM (not only LGBM as I did in the previous version of the kernel) because, for instance with CatBoost the power of GPU is way more significative!"},{"metadata":{},"cell_type":"markdown","source":"---\n### _Let's start! :)_\n\nFirst, in the notebook settings set **Internet connected**, and **GPU on**.\n\n#### Enabling LGBM GPU\nThen execute the 4 following cells. You can find the explanation in this [kernel](https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/)![](http://)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM\n!apt-get install -y -qq libboost-all-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Great, we are ready to go!**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.\n\n# sklearn, LGBM and CatBoost\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport catboost as ctb\n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading datasets\ndf_train = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\ndf_test = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n\nprint(\"Train shape: \" + str(df_train.shape))\nprint(\"Test shape: \" + str(df_test.shape))\n\n# Splitting the target variable and the features\nX_train = df_train.loc[:,'var_0':]\ny_train = df_train.loc[:,'target']\n\nprint(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorting only *True Test data*... Yes, you already know the [kernel](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) ;)\n\nOne of the true winners: [YaG320](https://www.kaggle.com/yag320)!"},{"metadata":{"trusted":true},"cell_type":"code","source":"synthetic_samples_indexes = pd.read_csv('../input/synthetic-samples-indexes/synthetic_samples_indexes.csv')\n\ndf_test_real = df_test.copy()\ndf_test_real = df_test_real[~df_test_real.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))]\nX_test = df_test_real.loc[:,'var_0':]\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency Encoding\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_count(df):\n    '''\n    Function that adds frequency columns for each variable (excluding 'ID_code', 'target').\n    New columns names will be like 'var_X_count'.\n    '''\n    for var in [i for i in df.columns if i not in ['ID_code','target']]:\n        df[var+'_count'] = df.groupby(var)[var].transform('count')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using both train and (real) test datasets to creates frequencies columns\n\nX_tot = pd.concat([X_train, X_test])\nprint(X_tot.shape)\n\nstart = time.time()\nX_tot = get_count(X_tot)\nend = time.time()\nprint('Frequency encoding took %.2f seconds\\nShape: ' %(end - start))\nprint(X_tot.shape)\n\nX_train_count = X_tot.iloc[0:200000]\nX_test_count = X_tot.iloc[200000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.8 train, 0.2 dev\nX_train,X_valid,y_train,y_valid = train_test_split(X_train_count, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint('X_train shape: {}\\n'.format(X_train.shape))\nprint('y_train shape: {}\\n'.format(y_train.shape))\nprint('X_valid shape: {}\\n'.format(X_valid.shape))\nprint('y_valid shape: {}'.format(y_valid.shape))\n\n# List of all the features\nfeatures = [c for c in X_train.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmentation\n---\n\nThis data augmentation function is described [here](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) by [Jiwei Liu](https://www.kaggle.com/jiweiliu).\n\nI only modified the fact that the FE features are considered togheter with their own original features.\n\nFor instance: `var_0` and `var_0_count` are augmented togheter."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Augmentation\n\ndef augment(x,y,t=2, model = 'lgbm'):\n    '''\n    Data Augmentation tx if y = 1 , (t/2)x if y = 0. Default t=6.\n    Model could be 'lgbm' (Default) or 'catboost'\n    '''\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(int(x1.shape[1]/2)):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n        xs.append(x1)\n\n    if model == 'lgbm':\n        for i in range(t//2):\n            mask = y==0\n            x1 = x[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(int(x1.shape[1]/2)):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n            xn.append(x1)\n    elif model == 'catboost':\n        for i in range(t//3):\n            mask = y==0\n            x1 = x[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(int(x1.shape[1]/2)):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n            xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n# Augmentation Only for training set!\nX_tr_lgbm, y_tr_lgbm = augment(X_train.values, y_train.values, t=2, model = 'lgbm')\n# X_tr_lgbm, y_tr_lgbm = augment(X_train.values, y_train.values, t=6, model = 'lgbm')\n# With t=6 we can see a larger gap between gpu and cpu lgbm!\n\nprint('X_tr_lgbm Augmented shape: {}'.format(X_tr_lgbm.shape))\nend = time.time()\nprint('t=2 Augmentation took %.2f seconds' %(end - start))\n\nstart2 = time.time()\nX_tr_cat, y_tr_cat = augment(X_train.values, y_train.values, t=15, model = 'catboost')\nprint('X_tr_cat Augmented shape: {}'.format(X_tr_cat.shape))\nend2 = time.time()\nprint('t=15 Augmentation took %.2f seconds' %(end2 - start2))\n\nX_tr_lgbm = pd.DataFrame(data=X_tr_lgbm,columns=X_train.columns)\ny_tr_lgbm = pd.DataFrame(data=y_tr_lgbm)\ny_tr_lgbm.columns = ['target']\n\nX_tr_cat = pd.DataFrame(data=X_tr_cat,columns=X_train.columns)\ny_tr_cat = pd.DataFrame(data=y_tr_cat)\ny_tr_cat.columns = ['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LBGM Comparison\n---"},{"metadata":{},"cell_type":"markdown","source":"### 1st MODEL: LGBM with CPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CPU parameters\n\nlgb_params = {\n        'bagging_fraction': 0.77,\n        'bagging_freq': 2,\n        'lambda_l1': 0.7,\n        'lambda_l2': 2,\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'min_data_in_leaf': 22,\n        'min_gain_to_split': 0.07,\n        'min_sum_hessian_in_leaf': 19,\n        'num_leaves': 20,\n        'feature_fraction': 1,\n        'save_binary': True,\n        'seed': 42,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'drop_seed': 42,\n        'data_random_seed': 42,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': 'false',\n        'num_threads': 6\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_data = lgb.Dataset(X_tr_lgbm, label=y_tr_lgbm) # Augmentation\nval_data = lgb.Dataset(X_valid, label=y_valid)\n\nstart = time.time()\n# Training\nclf = lgb.train(lgb_params, trn_data, 100000, valid_sets = [val_data], verbose_eval=-1, early_stopping_rounds = 3000)\nend = time.time()\n\nlgb_cpu = end - start\nprint('>> CPU: It took %.2f seconds' %(lgb_cpu))\n\nval_pred = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['lgb_cpu'] = lgb_cpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2nd MODEL: LGBM with GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GPU parameters\n\nlgb_params = {\n        'bagging_fraction': 0.77,\n        'bagging_freq': 2,\n        'lambda_l1': 0.7,\n        'lambda_l2': 2,\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'min_data_in_leaf': 22,\n        'min_gain_to_split': 0.07,\n        'min_sum_hessian_in_leaf': 19,\n        'num_leaves': 20,\n        'feature_fraction': 1,\n        'save_binary': True,\n        'seed': 42,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'drop_seed': 42,\n        'data_random_seed': 42,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': 'false',\n        'num_threads': 6,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_data = lgb.Dataset(X_tr_lgbm, label=y_tr_lgbm) # Augmentation\nval_data = lgb.Dataset(X_valid, label=y_valid)\n\nstart = time.time()\n# Training\nclf = lgb.train(lgb_params, trn_data, 100000, valid_sets = [val_data], verbose_eval=-1, early_stopping_rounds = 3000)\nend = time.time()\n\nlgb_gpu = end - start\nprint('>> GPU: It took %.2f seconds' %(lgb_gpu))\n\nval_pred = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['lgb_gpu'] = lgb_gpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost\n---"},{"metadata":{},"cell_type":"markdown","source":"### MODEL: CatBoost with GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":" cat_params = {\n        'max_depth' : 7,\n        'learning_rate' : 0.04,\n        'colsample_bylevel' : 1.0,\n        'objective' : \"Logloss\",\n        'eval_metric' : 'AUC',\n        'task_type' : \"GPU\",\n        'random_seed': 42,\n        'iterations': 100000,\n        'use_best_model': True\n }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nclf = ctb.CatBoostClassifier(**cat_params)\nclf.fit(X=X_tr_cat, y=y_tr_cat, eval_set=[(X_valid, y_valid)], verbose=5000, early_stopping_rounds = 3000)\nend = time.time()\n\ncat_gpu = end - start\nprint('>> GPU: It took %.2f seconds' %(cat_gpu))\n\nval_pred = clf.predict_proba(X_valid)[:,1]\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['cat_gpu'] = cat_gpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_result(first_time, first_name, second_time, second_name, title):\n    results = (first_time/60)-(second_time/60) # converting to minutes\n    print('>> {} result:'.format(title))\n    if results < 0:\n        print(str(first_name)+' spent '+str(int(abs(results)))+' minutes less than '+str(second_name))\n        print('It was %.2f times faster!'% (second_time/first_time))\n    elif results > 0:\n        print(str(second_name)+' spent '+str(int(abs(results)))+' minutes less than '+str(first_name))\n        print('It was %.2f times faster!'% (first_time/second_time))\n    else:\n        print('CPU and GPU spent the exact same time: %.2f minutes'% first_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_result(lgb_cpu,\"LGBM CPU\",lgb_gpu,\"LGBM GPU\", 'LGBM CPU vs GPU')\nprint_result(cat_gpu,\"Cat GPU\",lgb_cpu,\"LGBM CPU\",'CatBoost GPU vs LGBM CPU')\nprint_result(cat_gpu,\"Cat GPU\",lgb_gpu,\"LGBM GPU\",'CatBoost GPU vs LGBM GPU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying results - Horizontal Barplot\n\nperformance_df = pd.DataFrame.from_dict(performance, orient='index')\nperformance_df.columns = ['time']\nperformance_df.reset_index(inplace=True)\nperformance_df['time'] = round(performance_df['time']/60, 2)\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 5))\n\n# Plot the total crashes\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"time\", y='index', data=performance_df, color=\"b\", tick_label=True)\nf.suptitle('Model Training Time', fontsize=16)\nax.set_xlabel('Minutes', fontsize=14)\nax.set_ylabel('Model', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional comments\n\n[](http://)\nWe can see the advantage of LGBM trained using GPU on a large enough dataset.\n\nAnyway, we can observe the real GPU power looking at CatBoost (last model trained more than 1 mln rows much faster than others with less rows!!). Its performances are incredibly fast and even more accurate!\n\n[Here](https://github.com/FedericoRaimondi/me/blob/master/Santander_Customer_Transaction_Prediction/PredictiveAnalysis_ModelTuning/PredictiveAnalysis_ModelTuning_GPU.ipynb) you can find the 200 models (2 feat each, LGBM) comparision and see how things change in that situation!\n\nA SPECIAL THANKS to Chris Deotte for your tips and your contribution!\n\n### Hope you liked this kernel... if you did, please upvote!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}