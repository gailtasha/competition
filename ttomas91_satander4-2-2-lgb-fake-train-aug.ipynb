{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi all.\n\nI am a beginner in DS. I really try break 0.9 lvl, but it is over my skills. And my English is bad (из бед и огорчений) =). Please, do not judge strictly.\n\nI hope you find this kernel is useful or at least naive.\n\nIn my kernel i use idea Jiwei Liu from [this kernel](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) about generation trane data by shuffle.\nAnd Spoiler Alert's idea that data in train and test data different. At [this kernel](https://www.kaggle.com/triplex/more-unique-values-in-train-set-than-test-set)."},{"metadata":{},"cell_type":"markdown","source":"First, import the packages."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd \nimport datetime\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport lightgbm as lgb\n\nimport seaborn as sns\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', None)\n\nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = (10, 10)\ntitle_config = {'fontsize': 20, 'y': 1.05}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting data"},{"metadata":{"_uuid":"8928ffe0b75642f9b31034f77f86db656712d629","trusted":true},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"inputs/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9f293a11e7700ca746ddaf6a9ab9c289cced0d7","trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv(PATH+\"train.csv\")\ntest = pd.read_csv(PATH+\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalize data"},{"metadata":{"trusted":true},"cell_type":"code","source":"col=train.columns[2:]\nscaler_df = preprocessing.normalize(train.iloc[:,2:], copy=True)\nscaled_df = pd.DataFrame(scaler_df,columns=col)\ntrain2=train.iloc[:,:2]\ntrain2[train.columns[2:]]=scaled_df\ntrain=train2\ndel train2, scaler_df, scaled_df\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=test.columns[1:]\nscaler_df = preprocessing.normalize(test.iloc[:,1:], copy=True)\nscaled_df = pd.DataFrame(scaler_df,columns=col)\ntest2=test.iloc[:,:1]\ntest2[test.columns[1:]]=scaled_df\ntest=test2\ndel test2, scaler_df, scaled_df\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"look on overlap nunique train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_train_unique'] = train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(train[col].unique(), test[col]).sum())\n    \ndf['origin_train']=df.n_train_unique-df.n_overlap\n\ndf['origin_test']=df.n_test_unique-df.n_overlap\ndf.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a bad. Need to reduce the dimension. Let's round it."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor feature in train.columns[2:]:\n    train[feature] = np.round(train[feature], 5)\n    test[feature] = np.round(test[feature], 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 symbol doing all variables as categorical.\n5 just right. "},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_train_unique'] = train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(train[col].unique(), test[col]).sum())\n    \ndf['origin_train']=df.n_train_unique-df.n_overlap\n\ndf['origin_test']=df.n_test_unique-df.n_overlap\ndf.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sort_values(by='n_train_unique').reset_index(drop=True)\ndf[['n_train_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sort_values(by='n_train_unique').reset_index(drop=True)\ndf[['origin_train', 'origin_test']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can see original matter in trane and test. Good exam for model. "},{"metadata":{},"cell_type":"markdown","source":"Let's look on overlap rows: "},{"metadata":{"trusted":true},"cell_type":"code","source":"def inform_overlap(train,test,ans,silent=True):\n    variable=[c for c in train.columns if c not in ['ID_code','target']]\n    vari=[]\n    res_train_out=[]\n    res_test_out=[]\n    res_reclose_train=[]\n    res_reclose_test=[]\n    for var in variable:\n        vari.append(var)\n        if silent==False:\n            print ('\\n Calulate {}'.format(var))\n        valu = train[var].isin(test[var].value_counts().index)\n        rows = valu[valu==False].index\n        \n        df_include = train.drop(index=rows)\n        df_not_include = train.drop(index=df_include.index)\n        \n        res_train_out.append(len(df_not_include))\n        res_reclose_train.append(len(df_include))\n        \n        \n        valu = test[var].isin(train[var].value_counts().index)\n        rows = valu[valu==False].index\n        \n        df_include = test.drop(index=rows)\n        df_not_include = test.drop(index=df_include.index)\n        \n        res_test_out.append(len(df_not_include))\n        res_reclose_test.append(len(df_include))\n        \n    ans['fetures']=vari\n    \n    ans['train_out']=res_train_out\n    ans['reclose_train']=res_reclose_train\n    ans['test_out']=res_test_out\n    ans['reclose_test']=res_reclose_test\n    \n    return ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nansver_orig = inform_overlap(train,test,pd.DataFrame())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansver_orig['dif_train']=ansver_orig.train_out/ansver_orig.reclose_train\nansver_orig['dif_test']=ansver_orig.test_out/ansver_orig.reclose_test\nansver_orig.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create fake trane data without original matter. "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def train_to_test_aug(train,test,silent=True):\n    variable=[c for c in train.columns if c not in ['ID_code','target']]\n    \n    for var in variable:\n        if silent==False:\n            print ('\\n Calulate {}'.format(var))\n        valu = train[var].isin(test[var].value_counts().index)\n        rows = valu[valu==False].index\n        df_include = train.drop(index=rows)\n        df_not_include = train.drop(index=df_include.index)\n\n        df_include_True = df_include[df_include.target==True]\n        df_include_False = df_include[df_include.target==False]\n        df_not_include_True = df_not_include[df_not_include.target==True]\n        df_not_include_False = df_not_include[df_not_include.target==False]\n        tmp=df_include_True.copy()\n        for x in range(len(df_not_include_True)//len(df_include_True)):\n            tmp=pd.concat([tmp,df_include_True],ignore_index=True)\n        \n        if silent==False:\n            print ('Target == True:')\n            print (\"Count row's not include: {} . Count row's exemple: {} .\".format(len( df_not_include_True[var]),\n                                                                                    len(df_include_True[var])))\n     \n        df_not_include_True[var]=tmp[var].sample(n=len(df_not_include_True[var])).tolist()\n        \n        \n        tmp=df_include_False.copy()\n        for x in range(len(df_not_include_False)//len(df_include_False)+1):\n            tmp=pd.concat([tmp,df_include_False],ignore_index=True)\n        \n        if silent==False:\n            print ('Target == False:')\n            print (\"Count row's not include: {} . Count row's exemple: {} .\".format(len( df_not_include_False[var]),\n                                                                                    len(df_include_False[var])))\n        \n        df_not_include_False[var]=tmp[var].sample(n=len(df_not_include_False[var])).tolist()\n        \n        train=pd.concat([df_include_True,df_include_False,df_not_include_True,df_not_include_False])\n        \n        train.sort_index(axis=0,inplace = True)\n    \n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfake_train = train_to_test_aug(train, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look on overlap fake train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var = fake_train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_fake_trane_unique'] = fake_train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(fake_train[col].unique(), test[col]).sum())\ndf.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sort_values(by='n_test_unique').reset_index(drop=True)\ndf[['n_fake_trane_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nansver_fake = inform_overlap(fake_train,test,pd.DataFrame())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansver_fake['dif_train']=ansver_fake.train_out/ansver_fake.reclose_train\nansver_fake['dif_test']=ansver_fake.test_out/ansver_fake.reclose_test\nansver_fake.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can see that fake train data overlap all rows. "},{"metadata":{},"cell_type":"markdown","source":"Let's create a lot of data for train lgb. Augment data not include first data."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([xs,xn])\n    y = np.concatenate([ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features= [c for c in fake_train.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_t, Y_t = augment(fake_train[features].values, fake_train['target'].values, 8)\n\ntr = pd.DataFrame(X_t)\n        \ntr = tr.add_prefix('var_')\n\ntr['target'] = Y_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_pretrain = {\n    'bagging_freq': 1,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 100,\n    'min_sum_hessian_in_leaf': 40.0,\n    'num_leaves': 16,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_t, x_v = tr.iloc[:][features], fake_train.iloc[:][features]\nY_t, y_v = tr.iloc[:]['target'], fake_train.iloc[:]['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I create 3 dataset: train - only augment data, valid - fake train data(not include in train) and valid2 - original train data (wich values not include in train and valid)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsn=datetime.datetime.now()\n\ntrain_res = np.zeros(len(X_t))\nvalid_res = np.zeros(len(x_v))\n\ntrn_data = lgb.Dataset(X_t, label=Y_t)\nval_data = lgb.Dataset(x_v, label=y_v)\nval_data2 = lgb.Dataset(train[features], label=train['target'])\n        \nnum_round = 60000\n        \nevals_result = {}\n        \nclf=lgb.train(param_pretrain, trn_data, num_round, valid_sets = [trn_data, val_data,val_data2], \n              verbose_eval=1000, early_stopping_rounds = 4000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nvalid_res = clf.predict(x_v, num_iteration=clf.best_iteration)\ndel1=datetime.datetime.now()-sn\nprint (\"AUG time :  {} . Valid score : {:<8.5f}\".format(str(del1), roc_auc_score(y_v,valid_res)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntr_result=clf.predict(train[features], num_iteration=clf.best_iteration)\nprint (\"Train score : {:<8.5f}\".format(roc_auc_score(train['target'],tr_result)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This score is most Interesting, because original data same have original value  as in test data."},{"metadata":{},"cell_type":"markdown","source":"Let's look on features importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfeat_importance_df = pd.DataFrame()\nfeat_importance_df[\"feature\"] = features\nfeat_importance_df[\"importance\"] = clf.feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nplt.figure(figsize=(14,36))\nsns.barplot(x=\"importance\", y=\"feature\", data=feat_importance_df.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance')\nplt.tight_layout()\nplt.savefig('fet_impotance.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult=clf.predict(test[features], num_iteration=clf.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_df[\"target\"] = result\n\nsub_df.to_csv(\"lgb_fake_test_submission{}.csv\".format(422), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsub_df = pd.DataFrame({\"ID_code\":train[\"ID_code\"].values})\nsub_df[\"target\"] = train[\"target\"].values\nsub_df[\"predict\"] = tr_result\nsub_df.to_csv(\"lgb_fake_train_submission{}.csv\".format(422), index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will try to do more visualization later.\n\nThanks for attention!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}