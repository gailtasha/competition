{"cells":[{"metadata":{"trusted":true,"_uuid":"044b1583605897d47ab713f59206c8eed8329d0c"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom matplotlib import pyplot as plt\nimport os\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3ccabc80fdca31b1702bfb94d0c6d14b83d5c7f"},"cell_type":"code","source":"random_state = 42\nnp.random.seed(random_state)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"539b4d5419db582f06df4dd2ca0ab231f83fa796"},"cell_type":"markdown","source":"# Load dataset and separate Validation"},{"metadata":{"trusted":true,"_uuid":"e8b5997af0f2979b6d9718238a6500ca1922a668"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\n# Uses the las 40000 for Validation (I am assuming that they are already shuffled in CSV)\ndf_train = df[:160000]\n# df_train = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e267e1280657fd692a54fe531df86aef7132de7c"},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2f0b9f1a38c24e54db0cf27e868b9745eeced9"},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true,"_uuid":"253fd816b937698c70477efb82c8bbaa597527de"},"cell_type":"code","source":"# Calculate \ndef get_stats(df_train, resolution = 501):\n    means = df_train.drop(columns=['target', 'ID_code']).mean()\n    stds = df_train.drop(columns=['target', 'ID_code']).std()\n    mins = df_train.drop(columns=['target', 'ID_code']).min()\n    maxs = df_train.drop(columns=['target', 'ID_code']).max()\n    max_z_zcore = np.ceil(((maxs-means)/stds).max())\n    min_z_zcore = np.floor(((mins-means)/stds).min())\n    z_scores = np.linspace(min_z_zcore, max_z_zcore, resolution)\n    return means, stds, z_scores, min_z_zcore, max_z_zcore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78c4cb759b0d2cb63bfe607489dd66b475223957"},"cell_type":"code","source":"means, stds, z_scores, min_z_zcore, max_z_zcore = get_stats(df_train, resolution = 501)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbf7cee82eab22007bb16bd7f94a367c237b1529"},"cell_type":"code","source":"N_vars = df_train.shape[1]-2\nprint('Number of independent variables:', N_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e50dc8710b9d22d3ad39c3f44f6914cf92c6867c"},"cell_type":"code","source":"print('Z-score range is from {} to {}'.format(min_z_zcore, max_z_zcore))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ea49917175bfeb126355105515ea2ec9b30efd9"},"cell_type":"markdown","source":"# Bayes Theorem"},{"metadata":{"_uuid":"76bdb062bddb68145dc6a9d9857ab6d49b9523ae"},"cell_type":"markdown","source":"Target = 1  \n$\\large P(t=1|V_0, V_1, ..., V_{199}) = \\frac{P(V_0, V_1, ..., V_{199}|t=1)P(t=1)}{P(V_0,V_1, ..., V_{199})} \\quad$\n\nTarget = 0  \n$\\large P(t=0|V_0, V_1, ..., V_{199}) = \\frac{P(V_0, V_1, ..., V_{199}|t=0)P(t=0)}{P(V_0,V_1, ..., V_{199})} \\quad$"},{"metadata":{"_uuid":"3d8e4ea74ae2f3db272d3d3a7fa43a9ab97b4425"},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"_uuid":"592464d47b0938334507db2e8711d3c22c5fe7ec"},"cell_type":"markdown","source":"$\\large \\large P(t=1|V_0, V_1, ..., V_{199}) = \\frac{P(V_0|t=1) P(V_1|t=1) ... P(V_{199}|t=1) P(t=1)}{P(V_0,...,V_{199})} \\quad$ Independence of Conditionals\n\n$\\large \\large P(t=0|V_0, V_1, ..., V_{199}) = \\frac{P(V_0|t=0) P(V_1|t=0) ... P(V_{199}|t=0) P(t=0)}{P(V_0,...,V_{199})} \\quad$ Independence of Conditionals"},{"metadata":{"_uuid":"caa4c3dd4508714d14b30a93f9079cee0b95a6e7"},"cell_type":"markdown","source":"# Likelihoods calculation:\n\n$\\large P(V_i|t=1)$ and $\\large P(V_i|t=0)$"},{"metadata":{"trusted":true,"_uuid":"1719e7b028e8293e948f64ad261c427ad8bdd2fa"},"cell_type":"code","source":"# All the observation where target = 1\ndf_target_1 = df_train[df_train['target']==1]\n# All the observation where target = 0\ndf_target_0 = df_train[df_train['target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af08b70c93fec56c149587e1215f42af1fe2de14"},"cell_type":"code","source":"def likelihoods_frequency(v, i=0, c=3, df_target_1=df_target_1, df_target_0=df_target_0):\n    #\n    # Counts the observations in region [v-stds[i]/c, v+stds[i]/c] for variable v_i and normalized it with the range\n    # \n    \"\"\"\n    v: center of region\n    i: variable index (var_i)\n    c: smoothing for moving average\n    \"\"\"\n    # From the observations where the target is 0 count the number of observations that are between v-stds[i]/c and v+stds[i]/c\n    N_interval_0 = len(df_target_0[(df_target_0['var_'+str(i)]>v-stds[i]/c) &(df_target_0['var_'+str(i)]<v+stds[i]/c)])\n    \n    # From the observations where the target is 1 count the number of observations that are between v-stds[i]/c and v+stds[i]/c\n    N_interval_1 = len(df_target_1[(df_target_1['var_'+str(i)]>v-stds[i]/c) &(df_target_1['var_'+str(i)]<v+stds[i]/c)])\n    # Returns the estimation of the likelihood at v\n    return N_interval_0/(2*stds[i]/c), N_interval_1/(2*stds[i]/c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4021fc06ac051a95c764a2aa740d2f3818175037"},"cell_type":"code","source":"def get_pdf(var_i, means=means, stds=stds, z_scores=z_scores, smoothing=1, c=3, N_vars=N_vars, df_target_0=df_target_0, df_target_1=df_target_1):\n    # Estimates the likelihood probability density function of variable v_i for all v_i's sample space\n    \"\"\"\n    var_i: variable index\n    c: smoothing for moving average\n    smoothing: laplacian smoothing\n    \"\"\"\n    ps_0 = []\n    ps_1 = []\n    N_0 = len(df_target_0) + smoothing*N_vars\n    N_1 = len(df_target_1) + smoothing*N_vars\n    for z in z_scores:\n        # Unnormalize\n        v = z*stds[var_i] + means[var_i]\n        l0, l1 = likelihoods_frequency(v, var_i, c=c)\n        ps_0.append(l0 + smoothing)\n        ps_1.append(l1 + smoothing)\n    return np.array(ps_0)/N_0, np.array(ps_1)/N_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a28c8c38fe68d293a155f441fcbcde2e035b5df9"},"cell_type":"code","source":"var_i = 0\n%time l0_3,l1_3 = get_pdf(var_i, c=3)\nplt.plot(z_scores, l0_3, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_3, label='$P(V_i|t=1)$')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d38a0cec94fc85f792627138b321593dd5bdc90c"},"cell_type":"markdown","source":"### Effect of moving average smoothing"},{"metadata":{"trusted":true,"_uuid":"9952070a7e646b662af9f03f2b22389d1086fb8e"},"cell_type":"code","source":"%time l0_05,l1_05 = get_pdf(var_i, c=0.5)\nplt.plot(z_scores, l0_05, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_05, label='$P(V_i|t=1)$')\nplt.title('C = 0.5')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8a89023cc24d344478ad2ad9f8098d2d8f3b717"},"cell_type":"code","source":"%time l0_10,l1_10 = get_pdf(var_i, c=10)\nplt.plot(z_scores, l0_10, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_10, label='$P(V_i|t=1)$')\nplt.title('C = 10')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"914238414f24082070f8ca18d32098e9bdf09fee"},"cell_type":"code","source":"plt.plot(z_scores, l0_3, label='C=3')\nplt.plot(z_scores, l0_10, label='C=10')\nplt.plot(z_scores, l0_05, label='C=0.5')\nplt.title('Moving average compare - likelihood t=0')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d681e7d8758b42c2580d6b19b0c1dd3dcc06c63d"},"cell_type":"markdown","source":"# Efect of laplacian smoothing"},{"metadata":{"trusted":true,"_uuid":"76c0abf53e65ecfe80c4f05d7828697c88c13987"},"cell_type":"code","source":"l0_ls1, l1_ls1 = get_pdf(var_i, c=3, smoothing=1)\nplt.plot(z_scores, l0_ls1, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_ls1, label='$P(V_i|t=1)$')\nplt.title('laplacian smoothing 1')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2bbfcae569fd898c9b7090ac939b34fb733737"},"cell_type":"code","source":"l0_ls10, l1_ls10 = get_pdf(var_i, c=3, smoothing=10)\nplt.plot(z_scores, l0_ls10, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_ls10, label='$P(V_i|t=1)$')\nplt.title('laplacian smoothing 10')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b6fdc7aa6098ef105e4af6a136197de6df6e3d"},"cell_type":"code","source":"l0_ls01, l1_ls01 = get_pdf(var_i, c=3, smoothing=0.1)\nplt.plot(z_scores, l0_ls01, label='$P(V_i|t=0)$')\nplt.plot(z_scores, l1_ls01, label='$P(V_i|t=1)$')\nplt.title('laplacian smoothing 0.1')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60ba1ea008c1d6b2e873f65f46ac667c0f64985b"},"cell_type":"code","source":"plt.plot(z_scores, l1_ls01, label='0.1')\nplt.plot(z_scores, l1_ls1, label='1')\nplt.plot(z_scores, l1_ls10, label='10')\nplt.title('Varing laplacian smoothing')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11d2167f3f4166caaca0c662fba0b4899f6551d3"},"cell_type":"markdown","source":"# Odds\n$\\large \\frac{P(t=1|V_0, V_1, ..., V_{199})}{P(t=0|V_0, V_1, ..., V_{199})} > 1\\quad$ implies that target 1 is more probable than target 0 \n\nDoing the quotient from Naive Bayes"},{"metadata":{"_uuid":"50cbcd555908b4f719b8130ae5514b7fdf3e234a"},"cell_type":"markdown","source":"$\\huge \\frac{\\frac{P(V_0|t=1) P(V_1|t=1) ... P(V_{199}|t=1) P(t=1)}{P(V_0,...,V_{199})}}{\\frac{P(V_0|t=0) P(V_1|t=0) ... P(V_{199}|t=0) P(t=0)}{P(V_0,...,V_{199})}} = \\frac{P(V_0|t=1) P(V_1|t=1) ... P(V_{199}|t=1) P(t=1)}{P(V_0|t=0) P(V_1|t=0) ... P(V_{199}|t=0) P(t=0)}$"},{"metadata":{"_uuid":"059b58dc3be965eb62691386b3801f41e8793e3a"},"cell_type":"markdown","source":"### Odds for one variable\n$\\large \\frac{P(V_1|t=1)P(t=1)}{P(V_1|t=0)P(t=0)}$\n"},{"metadata":{"trusted":true,"_uuid":"8254ae27f8af44ee11a7ccf1bce436ede9496523"},"cell_type":"code","source":"# Odds for just one variable\n# P(t=1) is the estimated as the proportion of observations with target 1\n# P(t=0) is the estimated as the proportion of observations with target 0\np_1 = len(df_target_1)/len(df_train)\np_0 = len(df_target_0)/len(df_train)\nprint(p_1, p_0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e97b8bd5f8f56ef38a11f5714bb82305dc541e"},"cell_type":"code","source":"var_i = 50\nl0,l1 = get_pdf(var_i, c=3, smoothing=1)\nodds = l1/l0 * p_1/p_0\nplt.plot(z_scores, odds, label='smoothing=1')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"331ff74dc563042b3689ff45a8bfb6351941d0b2"},"cell_type":"markdown","source":"### Effect of laplacian smoothing"},{"metadata":{"trusted":true,"_uuid":"3d7663b0194e19d94e69b2f968db3536ab4c721c"},"cell_type":"code","source":"l0,l1 = get_pdf(var_i, c=3, smoothing=10)\nodds = l1/l0 * p_1/p_0\nplt.plot(z_scores, odds, label='smoothing=100')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dc4f730d41e39e626bd2e56755a50fd1330099b"},"cell_type":"code","source":"l0,l1 = get_pdf(var_i, c=3, smoothing=0.01)\nodds = l1/l0 * p_1/p_0\nplt.plot(z_scores, odds, label='smoothing=0.01')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"716a550d6997dd43f870f1346ec700bcd9c785f8"},"cell_type":"markdown","source":"# Calculate marginal likelihoods for all $V_i$s\n$P(V_0|t=1), P(V_1|t=1), ..., P(V_{199}|t=1)$\n\n$P(V_0|t=0), P(V_1|t=0), ..., P(V_{199}|t=0)$"},{"metadata":{"_uuid":"3a1afb93c22053920c1f745f516603be37c605b7"},"cell_type":"markdown","source":"The two functions above: get_pdf and likelihoods_frequency, are inefficient but are easier to understand how the calculations are done. It takes almost 10 seconds in a pentium 7 with 12 processors and 16GB of memory when c=1\n\nDoing calculations of 200 variables it takes aprox 10s*200/60 = 33 minutes"},{"metadata":{"_uuid":"089593b0dcfc765b5c3148b8198c5f246607474f"},"cell_type":"markdown","source":"## First: choose c and laplacian smoothing"},{"metadata":{"trusted":true,"_uuid":"b681751366170f977152d7da5a9811cb564b27b1"},"cell_type":"code","source":"# Chossen c and smoothing\nc = 3\nsmoothing = 1\nl0,l1 = get_pdf(1, c=3, smoothing=0.1)\nodds = l1/l0 * p_1/p_0\nplt.plot(z_scores, odds, label='smoothing=0.01')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bcc48980fcdd393f0a1c9cfaca4a259ee59fc1f"},"cell_type":"markdown","source":"## Second: calculate al V_i's"},{"metadata":{"trusted":true,"_uuid":"5f5bfa28a4cb50e3d187975d2a5eb6483223d563"},"cell_type":"code","source":"####\n# In a pentium 7 with 12 processors and 16GB of memory\n####\n# For c=3\n# CPU times: user 13min 34s, sys: 103 ms, total: 13min 34s\n# Wall time: 13min 34s\n####\n# For c=0.5\n# CPU times: user 32min 46s, sys: 22min 14s, total: 55min 1s\n# Wall time: 55min 1s\n####\n\ndef calculate_all_V_i_inefficient(df_train=df_train, smoothing=smoothing, c=c):\n    # Check is the file is already in disk, if not calculate it and save it to disk\n\n    filename_l0 = 'likelihood_matrix_0_smooth_{}_c_{}_{}.npy'.format(smoothing, c, len(df_train))\n    filename_l1 = 'likelihood_matrix_1_smooth_{}_c_{}_{}.npy'.format(smoothing, c, len(df_train))\n\n    if os.path.isfile(filename_l0) and os.path.isfile(filename_l1):\n        likelihood_matrix_0_np = np.load(filename_l0)\n        likelihood_matrix_1_np = np.load(filename_l1)\n        print('Skip likelihoods calculations, files {} and {} already exists'.format(filename_l0, filename_l1))\n    else:\n        likelihood_matrix_0 = []\n        likelihood_matrix_1 = []\n        for i in range(df_train.shape[1] - 2):\n            # This for is very inefficient because it calculates each variable separately\n            print('\\rCalculating likelihoods for var_'+str(i), end=\"\")\n            var_i_0, var_i_1 = get_pdf(i, smoothing=smoothing, c=c)\n            likelihood_matrix_0.append(var_i_0)\n            likelihood_matrix_1.append(var_i_1)\n        # To numpy    \n        likelihood_matrix_0_np = np.array(likelihood_matrix_0).T\n        likelihood_matrix_1_np = np.array(likelihood_matrix_1).T\n        # Save them\n        np.save(filename_l0, likelihood_matrix_0_np)\n        np.save(filename_l1, likelihood_matrix_1_np)\n\n    # The odds here is not multiplied by p_1/p_0, it will be done later\n    odds = likelihood_matrix_1_np/likelihood_matrix_0_np\n    filename_odds = 'odds_smooth_{}_c_{}_{}.npy'.format(smoothing, c, len(df_train))\n    np.save(filename_odds, odds)\n    print()\n    return likelihood_matrix_0_np, likelihood_matrix_1_np, odds\n\n# %time likelihood_matrix_0, likelihood_matrix_1, odds = calculate_all_V_i_inefficient(df_train=df_train, smoothing=smoothing, c=c)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a525586f29acea796e6bf62053db0298724b4ae"},"cell_type":"markdown","source":"### Vectorization to accelarate processing"},{"metadata":{"trusted":true,"_uuid":"3c2548ff2ea73568f61d302113e4c613a0dc8c1e"},"cell_type":"code","source":"####\n# In a pentium 7 with 12 processors and 16GB of memory\n####\n# For c=3\n# CPU times: user 2min 36s, sys: 51.3 s, total: 3min 27s\n# Wall time: 3min 27s\n####\n# For c=0.5\n# CPU times: user 2min 55s, sys: 51.7 s, total: 3min 46s\n# Wall time: 3min 46s\n####\ndef likelihoods_frequency_vect(v, c=3, df_target_1=df_target_1, df_target_0=df_target_0):\n    # This version calculates all variables v_i in one shot\n    N_interval_0 = ((df_target_0.drop(columns=['ID_code', 'target'])>v-stds/c) \n                    & (df_target_0.drop(columns=['ID_code', 'target'])<v+stds/c)).sum(axis=0)\n    N_interval_1 = ((df_target_1.drop(columns=['ID_code', 'target'])>v-stds/c) \n                    & (df_target_1.drop(columns=['ID_code', 'target'])<v+stds/c)).sum(axis=0)\n    return N_interval_0/(2*stds/c), N_interval_1/(2*stds/c)\n\ndef get_pdf_vect(means=means, stds=stds, z_scores=z_scores, smoothing=1, c=3, df_target_1=df_target_1, df_target_0=df_target_0):\n    # Calculates the probability density function of all V_is\n    # Same as calculate_all_V_i_inefficient but in a more efficient way\n    \"\"\"\n    var_i: variable index\n    c: smoothing for moving average\n    smoothing: laplacian smoothing\n    \"\"\"\n    N_0 = len(df_target_0) + smoothing*N_vars\n    N_1 = len(df_target_1) + smoothing*N_vars\n    ps_0 = []\n    ps_1 = []\n    for z in z_scores:\n        print('\\r z =', z, end = '')\n        # Unnormalize\n        v = z*stds + means\n        l0, l1 = likelihoods_frequency_vect(v, c=c)\n        ps_0.append(l0 + smoothing)\n        ps_1.append(l1 + smoothing)\n    likelihood_matrix_0 = np.array(ps_0)/N_0\n    likelihood_matrix_1 = np.array(ps_1)/N_1\n    odds = likelihood_matrix_1/likelihood_matrix_0\n    print()\n    return likelihood_matrix_0, likelihood_matrix_1, odds\n%time likelihood_matrix_0, likelihood_matrix_1, odds = get_pdf_vect(smoothing=smoothing, c=c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efbfee78d503a2720c1f9efbc6a56eaa31a245b9"},"cell_type":"code","source":"var_i = 0\nplt.plot(z_scores, likelihood_matrix_0[:,var_i])\nplt.plot(z_scores, likelihood_matrix_1[:,var_i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b25502dbd16e373222357fd9120aa441e5f2242"},"cell_type":"markdown","source":"### Multiprocessing  "},{"metadata":{"trusted":true,"_uuid":"ccdd300192600b9e9f683af1dac358af71f7b5c1"},"cell_type":"code","source":"from multiprocessing import Pool\nfrom functools import partial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a769aba5c3bfb00b4dfc343cd4a6e59838e632d"},"cell_type":"code","source":"####\n# In a pentium 7 with 12 processors and 16GB of memory\n####\n# For c=3\n# CPU times: user 91.6 ms, sys: 144 ms, total: 235 ms\n# Wall time: 48.7 s\n####\n# For c=0.5\n# CPU times: user 80.2 ms, sys: 142 ms, total: 222 ms\n# Wall time: 48.3 s\n####\n\ndef get_pdf_vect_parallel(z_scores_interval, means=means, stds=stds, smoothing=1, c=3, df_target_1=df_target_1, df_target_0=df_target_0):\n    # Calculates the probability density function of variable v_i for all v's\n    \"\"\"\n    var_i: variable index\n    c: smoothing for moving average\n    smoothing: laplacian smoothing\n    \"\"\"\n    N_0 = len(df_target_0) + smoothing*N_vars\n    N_1 = len(df_target_1) + smoothing*N_vars\n    ps_0 = []\n    ps_1 = []\n    for z in z_scores_interval:\n        # Unnormalize\n        v = z*stds + means\n        l0, l1 = likelihoods_frequency_vect(v, c=c, df_target_1=df_target_1, df_target_0=df_target_0)\n        ps_0.append(l0 + smoothing)\n        ps_1.append(l1 + smoothing)\n    return np.array(ps_0)/N_0, np.array(ps_1)/N_1\n\n\n\ndef train_parallel(df, N = 10, smoothing=1, c=3, resolution=501):\n    means, stds, z_scores, min_z_zcore, max_z_zcore = get_stats(df_train, resolution = resolution)\n    # All the observation where target = 1\n    df_target_1 = df_train[df_train['target']==1]\n    # All the observation where target = 0\n    df_target_0 = df_train[df_train['target']==0]\n    \n    N_paral = int(len(z_scores)/N)\n    z_scores_list = []\n    for i in range(N):\n        z_scores_min = i*N_paral\n        if i == N-1:\n            z_scores_max = len(z_scores)\n        else:\n            z_scores_max = (i+1)*N_paral\n        z_scores_list.append(list(z_scores[z_scores_min: z_scores_max]))\n    likelihoods_pdfs = []\n    with Pool(N) as p:\n        likelihoods_pdfs = p.map(partial(get_pdf_vect_parallel, smoothing=smoothing, c=c), z_scores_list)\n    likelihood_matrix_0 = np.empty((0, 200))\n    likelihood_matrix_1 = np.empty((0, 200))\n    for i, (l0, l1) in enumerate(likelihoods_pdfs):\n        likelihood_matrix_0 = np.append(likelihood_matrix_0, l0, axis=0)\n        likelihood_matrix_1 = np.append(likelihood_matrix_1, l1, axis=0)\n    odds = likelihood_matrix_1/likelihood_matrix_0\n    p_1 = len(df_target_1)/len(df_train)\n    p_0 = len(df_target_0)/len(df_train)\n    return likelihood_matrix_0, likelihood_matrix_1, odds, p_1, p_0, means, stds, z_scores\n\n%time likelihood_matrix_0, likelihood_matrix_1, odds, p_1, p_0, means, stds, z_scores = train_parallel(df_train, N = 10, smoothing=smoothing, c=c)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41a0dc10a89c0593c84259911efaa1195f77fd6b"},"cell_type":"markdown","source":"### Plot likelihoods"},{"metadata":{"trusted":true,"_uuid":"aa32095a05615064584e75de28fee1244def8dbf"},"cell_type":"code","source":"plt.plot(z_scores, likelihood_matrix_0[:,:60])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b700a8f5ce262a3eab00d563cd00aaf3c8c0f514"},"cell_type":"code","source":"plt.plot(z_scores, likelihood_matrix_1[:,:60])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d8463c50f68ec44ab3b54fb56794df1a6e8218"},"cell_type":"code","source":"var_i = 0\nplt.plot(z_scores, likelihood_matrix_0[:,var_i])\nplt.plot(z_scores, likelihood_matrix_1[:,var_i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df3b86cafe525f539fe01055b3b213f7ab0ef77b"},"cell_type":"markdown","source":"### Plot odds"},{"metadata":{"trusted":true,"_uuid":"1639c9412bd8a00bd4bcda736720b078ac3cca3b"},"cell_type":"code","source":"plt.plot(z_scores, odds * p_1/p_0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d888c73b6974c6090e782072ae50ab21fa012790"},"cell_type":"code","source":"plt.plot(z_scores, odds[:,0] * p_1/p_0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19181fb7147753363f5c1a97f96d4ad8c94685a8"},"cell_type":"markdown","source":"# Estimate Observations"},{"metadata":{"trusted":true,"_uuid":"d9b6838d0720fb498f24222aa215580074ba451f"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"796730c859339a4d41d3b274413cba351603973b"},"cell_type":"markdown","source":"## Train model (160.000 observations)"},{"metadata":{"trusted":true,"_uuid":"1143a04090561fa94c60d13a98d24f7b23d9d39f"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\n# Uses the las 40000 for Validation (I am assuming that they are already shuffled in CSV)\ndf_train = df[:160000]\n# df_train = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce16e0bfd3986bf4a924ef3d25ae330fd2c55e0"},"cell_type":"code","source":"_, _, odds, p_1, p_0, means, stds, z_scores = train_parallel(df_train, N = 10, smoothing=1, c=3, resolution=501)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed56b6b1c47d02c9ad2478faaa6cabc8f1b9e16"},"cell_type":"markdown","source":"### Predict on train"},{"metadata":{"trusted":true,"_uuid":"8b7bacd508bba512e46ba4fc6ac6a833da5d875f"},"cell_type":"code","source":"def predict(df, odds, p_1, p_0, means, stds, z_scores, resolution=501):\n    min_z_zcore, max_z_zcore = min(z_scores), max(z_scores)\n    if 'target' in df:\n        observations = df.drop(columns=['target', 'ID_code']).values\n    else:\n        observations = df.drop(columns=['ID_code']).values\n    # Normalize\n    observations_normalized = (observations - means.values)/stds.values\n    observations_odds = []\n    for var_i in range(df.shape[1] - 2):\n        indexes = np.array(np.round(((observations_normalized[:,var_i]-min_z_zcore)/(max_z_zcore - min_z_zcore))*resolution), dtype=int)\n        observations_odds.append(odds[indexes, var_i])\n    observations_odds = np.array(observations_odds).T\n    log_odds = np.sum(np.log(observations_odds), axis=1) + np.log(p_1/p_0)\n    prod_odds = np.exp(log_odds)\n    auc = None\n    acc = None\n    if 'target' in df:\n        auc = roc_auc_score(df['target'], prod_odds)\n        acc = (df['target'] == (prod_odds>=1)).sum()/len(df)\n        \n    return observations_normalized, observations_odds, log_odds, prod_odds, auc, acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ad73e8a1ca4fb4aa4af3155723ff3b4b574284b"},"cell_type":"code","source":"observations_normalized, observations_odds, log_odds, prod_odds, auc, acc = predict(df_train, odds, p_1, p_0, means, stds, z_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c61814e7911f7d0648a926ca13860806f807bc"},"cell_type":"code","source":"print('AUC = {}, for c={} and smothing={}'.format(auc, c, smoothing))\nprint('Acc = {}, for c={} and smothing={}'.format(acc, c, smoothing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e8c26da4723a8f8b4353add76eaabd6d4125724"},"cell_type":"code","source":"var_j = 1\nplt.plot(z_scores, odds[:,var_j])\nplt.scatter(observations_normalized[:,var_j], observations_odds[:,var_j], s=20, marker='.', c='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"154002ad0ae9db6fc43e245351431a8de181e78a"},"cell_type":"code","source":"_ = plt.hist(log_odds, 50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7346d20af4a99abca28c7b4f8256be96bf62e8a3"},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true,"_uuid":"cc5f5ccd4bb20abd244d3a02b469d0163f7b5ccf"},"cell_type":"code","source":"df_valid = pd.read_csv('../input/train.csv')[160000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69ca23a5190359feaff97762227c5b7f77e340c"},"cell_type":"code","source":"observations_normalized_valid, observations_odds_valid, log_odds_valid, prod_odds_valid, auc_valid, acc_valid = predict(df_valid, odds, p_1, p_0, means, stds, z_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f132d79c3f4a3426653424c0014bc85f0fd96142"},"cell_type":"code","source":"print('Validation AUC = {}, for c={} and smothing={}'.format(auc_valid, c, smoothing))\nprint('Validation Acc = {}, for c={} and smothing={}'.format(acc_valid, c, smoothing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98bb630bcd01535e0c8a2bc13fb3d7b83ec4ee68"},"cell_type":"code","source":"var_j = 1\nplt.plot(z_scores, odds[:,var_j])\nplt.scatter(observations_normalized_valid[:,var_j], observations_odds_valid[:,var_j], s=20, marker='.', c='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e68fba5d71cc05109fcdd2f671c1b86b1e0303a"},"cell_type":"code","source":"_ = plt.hist(log_odds_valid, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"700dd1da74680cc0f04d745cb6284b75269a4a3e"},"cell_type":"code","source":"# Validation AUC = 0.9007052140114506 smoothing = 10 c=3\n# Validation AUC = 0.902612244256969 smoothing = 1 c=3\n# Validation AUC = 0.9026132128924224 smoothing = 0.5 c=3\n# Validation AUC = 0.8951584158287289 smoothing = 0.5 c=10\n# Validation AUC = 0.9024698960638358 smoothing = 0.5 c=2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100249e7746bef21297981a5863444194ae4c306"},"cell_type":"markdown","source":"# Retrain with all dataset"},{"metadata":{"trusted":true,"_uuid":"995baaabb1660f9ebd86f86feb654e3c1ad853c4"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59d5b5aada587398e97dc893c2ec6bb534ed2c87"},"cell_type":"code","source":"_, _, odds, p_1, p_0, means, stds, z_scores = train_parallel(df_train, N = 10, smoothing=1, c=3, resolution=501)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baad06b656f3b7c05d5458f341aaa49a38279792"},"cell_type":"code","source":"observations_normalized_full, observations_odds_full, log_odds_full, prod_odds_full, auc_full, acc_full = predict(df_train, odds, p_1, p_0, means, stds, z_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2adf4baa3ed0dd0fdbdea59cd9ba922a05ee3f4b"},"cell_type":"code","source":"print('Validation AUC = {}, for c={} and smothing={}'.format(auc_full, c, smoothing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c1d40a51a48c0e1e08615e573d9f378f46cc0c"},"cell_type":"code","source":"_ = plt.hist(log_odds_full, 30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b68bedfcea3776b9636ea1be193b649f7a002d9c"},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true,"_uuid":"bf2ffbd51f540fc16f130b2ebc84e4e9e9613e77"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ad7bf811bfe82c096ab5d9ac2aa597facad8c82"},"cell_type":"code","source":"_, _, log_odds_full, prod_odds_test, _, _ = predict(df_test, odds, p_1, p_0, means, stds, z_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9aca5bcfbfdec6b87ea4bedea12e3440bee161e"},"cell_type":"code","source":"_ = plt.hist(log_odds_full, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83c88b8f1b5dddf80d0ef49aa37149ef87e571a1"},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = prod_odds_test\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"222019587dea91f75f83ed10a943177a68ac28ef"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}