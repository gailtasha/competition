{"cells":[{"metadata":{"_uuid":"71a0e294f2ff30b362a94faf0cb1252c8448c7bf"},"cell_type":"markdown","source":"# _**üåπüèÜRoseGold üèÜüåπ**_\n\nContents:\n1. Exploratory Data Analysis\n2. Principle Components Analysis\n3. Build LightGBM model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b25e0c0efef4f5ce4b7432911c607849c72ed8f"},"cell_type":"code","source":"# Condense display values for EDA.\npd.options.display.float_format = '{:,.0f}'.format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15e043ead216875c3c2f6a3a397126c938a0b92e"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c53bc856868e9a4f7d1d768e171f4b61ff99af65"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7442bdc9286453d9fb983ad3e2666d580e93862"},"cell_type":"code","source":"# How many features are there?\nlen(train.drop(['ID_code', 'target'], axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3359588215f2b3f02c6334193ce88b97ba956417"},"cell_type":"code","source":"# Plot first 100 features.\ntrain.iloc[:, 2:100].plot(kind='box', figsize=[16,8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae3c85e989835b8adad8ee876c8b0b5bf0699eab"},"cell_type":"code","source":"# Plot last 100 features.\ntrain.iloc[:, 100:].plot(kind='box', figsize=[16,8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"020a5975a994ef2df90a52abd4b456c4a35e7a46"},"cell_type":"code","source":"# Plot densities.\n# Densities are easier to visualize if we remove outliers first.\ntrain_x = train.iloc[:, 2:]\ntrain_no_outliers = train_x[train_x.apply(lambda x :(x-x.mean()).abs()<(2*x.std()) ).all(1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03f33fabcdb121003ee4cfc9ae1d6914afdf3bda"},"cell_type":"code","source":"# Plot densities 1-100.\ntrain_no_outliers.iloc[:, :100].plot.density(ind=1000, figsize=[16,8], legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06e2ebb1b429d2330d55a5040cfd4d37ab29d1cb"},"cell_type":"code","source":"# There is one feature that has extremely high density near 5 or 10.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00dbd2acad2f37fd1b5b91648d61bbb856c0682c"},"cell_type":"code","source":"# Plot densities 100-200.\ntrain_no_outliers.iloc[:, 100:].plot.density(ind=1000, figsize=[16,8], legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4436564f754330840e324e1915ff7d71c9255e06"},"cell_type":"code","source":"# What does the target look like?\ntrain.target.value_counts().plot(kind=\"bar\")\nplt.figure()\nsns.violinplot(x=train.target.values, y=train.index.values, palette=\"husl\")\nplt.figure()\nsns.stripplot(x=train.target.values, y=train.index.values,\n              jitter=True, color=\"black\", size=0.5, alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have class imbalanced class problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale data\n# The plots above are hard to read. Lets scale our data.\n# Scaling also is good when doing pca. See https://stats.stackexchange.com/a/69159/7167\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_scaled = train.copy()\ntrain_scaled.iloc[:, 2:] = scaler.fit_transform(train.iloc[:, 2:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scaled.plot(kind='box', figsize=[16,8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scaled.iloc[:, 2:100].plot.density(ind=30, figsize=[16,8], legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scaled.iloc[:, 100:].plot.density(ind=30, figsize=[16,8], legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most correlated features\ncorrelations = train.iloc[:, 2:].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA\n\nBorrowed from DataScience handbook Chapter 5\n\nhttps://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"},{"metadata":{"trusted":true,"_uuid":"40f19ab683188f8dcc81022e55c0c533c0ec9b5a"},"cell_type":"code","source":"# Separate out the features.\nx = train_scaled.iloc[:, 2:].values\n# Separate out the target.\ny = train_scaled.iloc[:, 1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba5247a7d8fbfe8b37c427242f18301f1ad53ae7"},"cell_type":"code","source":"#sns.boxplot('var_0','target',data=train, hue='target')\n# plot boxplots by target value 0, 1\n# imbar on ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e0839fc4ee598e53924d5f959205238316f8293"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(2)\nprojected = pca.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74d53631bc68b7781b187bf957c3890fee9300bb"},"cell_type":"code","source":"print(projected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3ffa93f35b456f44a7a45b1418f23a34b3071c8"},"cell_type":"code","source":"plt.scatter(projected[:, 0], projected[:, 1],\n           c=y, edgecolor='none', alpha=0.5,\n           cmap=plt.cm.get_cmap('copper', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title('Santander 2d PCA scaled')\nplt.colorbar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most descriptive feature in the dataset (component 1) is positively correlated with the target!"},{"metadata":{"trusted":true,"_uuid":"280e5c5efc25db4cf23f1f7d122aeda53c45f232"},"cell_type":"code","source":"pca = PCA().fit(x)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander scaled PCA cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee457e22eda70ffd705ddf9bb95f22f0a2c87208"},"cell_type":"code","source":"# The 100 most descriptive features explain 50% of the variance.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2e34978c7566c722d5eb1d12d83008adb2953ff"},"cell_type":"code","source":"# Let's try randomized pca to ignore outliers. Pick the 100 most descriptive features for rpca.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"090b35219c3656298f8328456e62e98bc84906a9"},"cell_type":"code","source":"rpca = PCA(n_components=100, svd_solver='randomized')\nrpca.fit(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8695dfcd13b85737f27e6aec9a1717a88056f28e"},"cell_type":"code","source":"plt.plot(np.cumsum(rpca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander scaled Radomized PCA cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa336989eacbb3131707e5adc5d3a338e7ec2a55"},"cell_type":"code","source":"# Ignoring outliers doesn't reduce the number of features we need for explanation.\n# This dataset is likely the 200 principle components of some larger dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA without scaling.\n# If we don't scale we can't visualize the correlation between component 1 and the target.\n# This is still good to look at as we will visualize that we can get\n# 90% cumulative explained variance with 100 unscaled features.\n\nx_raw = train.iloc[:, 2:].values\ny_raw = train.iloc[:, 1].values\npca_raw = PCA(2)\nprojected_raw = pca_raw.fit_transform(x_raw)\nplt.scatter(projected_raw[:, 0], projected_raw[:, 1],\n           c=y, edgecolor='none', alpha=0.5,\n           cmap=plt.cm.get_cmap('copper', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title('Santander 2d PCA unscaled')\nplt.colorbar();\n\nplt.figure()\npca_raw = PCA().fit(x_raw)\nplt.plot(np.cumsum(pca_raw.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander unscaled PCA cumulative explained variance')\n\nplt.figure()\nrpca_raw = PCA(n_components=100, svd_solver='randomized')\nrpca_raw.fit(x_raw)\nplt.plot(np.cumsum(rpca_raw.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander unscaled Radomized PCA cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true,"_uuid":"9bdb84eab0730f13b53feabca819e1f07abba21b"},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cefae89b3c3c3c1652ff0dd4639e5e3da3050d1"},"cell_type":"code","source":"# Create stratified validation split.\n# Stratifying makes the splits have the same class distribution (purchase/no-purchase).\ntrain_x, validation_x, train_y, validation_y = train_test_split(x, y, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bef23871081f9c75e22156ab63ccc7c58f2fe3a2"},"cell_type":"code","source":"train_data = lgb.Dataset(train_x, label=train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82dfe1bf443cc696c50234412c6670736b7793ff"},"cell_type":"code","source":"validation_data = lgb.Dataset(validation_x, label=validation_y, reference=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e16350c3aa0ecaf2d5316242f1e66641f11d7ee","scrolled":true},"cell_type":"code","source":"bst = lgb.train({\n    'boosting': 'gbdt', #'dart', # Dropouts meet Multiple Additive Regression Trees, default='gbdt'\n    'learning_rate': 0.01, # smaller increases accuracy, default=0.1\n    'max_bin': 511, # larger increases accuracy, default=255\n    'metric': 'auc',\n    'num_leaves': 63, # larger increases accuracy, default=31\n    'num_trees': 100,\n    'num_iteration': 500, # default=100\n    'objective': 'binary',\n    },\n    train_data,\n    num_boost_round=500, # may be redundant with params#num_iteration\n    valid_sets=[validation_data],\n    early_stopping_rounds=100,\n    verbose_eval=100, # logs every 100 trees\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeab654aa79f21c94c026a85fc99a108dd3e42ec"},"cell_type":"code","source":"bst.save_model('model.txt', num_iteration=bst.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(bst, figsize=(16,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.create_tree_digraph(bst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d6f9bdeb3eb8c39a1c3217d6510f72e6823e784"},"cell_type":"code","source":"# Generate submission\ntest = pd.read_csv('../input/test.csv')\ntest_x = test.iloc[:, 1:].values # Drop the ID_code\nypred = bst.predict(test_x)\ntest_code = test.iloc[:, 0]\nsubmission = pd.concat([test_code, pd.Series(ypred, name='target')], axis=1)\nsubmission.to_csv('submissions.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f5c3bdb953c7897ff7c6606d6e8feb9453fc36"},"cell_type":"code","source":"nunique  = train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9957fd1aca37fdc78a3e863a648e4d3443b26756"},"cell_type":"code","source":"!head submissions.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score is 0.863 on leaderboard (lb)."},{"metadata":{},"cell_type":"markdown","source":"TODO\n* https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n* https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}