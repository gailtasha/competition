{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n%matplotlib inline\nplt.style.use('seaborn')\n\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import Ridge,ElasticNet, SGDRegressor, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse\nfrom scipy.stats import norm, skew\n\nfrom sklearn.decomposition import TruncatedSVD, PCA, LatentDirichletAllocation, NMF\nfrom sklearn.manifold import TSNE\n\nimport copy\n\nimport os\nimport time\nimport warnings\nimport gc\nimport os\nimport pickle\nfrom six.moves import urllib\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"260af2dec325980d2e1221fcafb8b2e6921a2b3a"},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_kg_hide-input":true,"_uuid":"affb580eaa5b56b493dba995d19356b6fd992962","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nfull_df = pd.concat((train_df, test_df))\n\nprint(\"Train data:\", train_df.info())\nprint(\"Test data:\", test_df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8460a3710da71cee627c37a0fa8b82e8ab3b5231"},"cell_type":"markdown","source":"## Define variables that are useful for later use"},{"metadata":{"_uuid":"bee3aa026c396af5d2407c42142097be61b5c9ce","trusted":true},"cell_type":"code","source":"\nnum_vars = []\n\ncat_vars = []\nfor var, dtype in full_df.dtypes.items():\n    if 'float' in str(dtype) or 'int' in str(dtype):\n        num_vars.append(var)\n    if 'object' in str(dtype):\n        cat_vars.append(var)\n        \nid_var = 'ID_code'\ncat_vars.remove(id_var)\ntarget_var = 'target'\nnum_vars.remove(target_var)\n\nprint (\"There are %d numerical features: %s\" \n       % (len(num_vars), num_vars))\n\nprint (\"There are %d numerical features: %s\" \n       % (len(cat_vars), cat_vars))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c134eea8ddbc9babc6a9d930724048e0c9a87ec7"},"cell_type":"markdown","source":"# EDA"},{"metadata":{"_uuid":"cb702fdc590e60a2e3707ac275b3cffe183d12ee"},"cell_type":"markdown","source":"## Description of datasets:"},{"metadata":{"_kg_hide-input":true,"_uuid":"d61d7d5a99be2669da1d86c756e8f6713c141897","trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"9f3ff4d626a5078e53e6c1634a8bff589cbe8def","trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b1ca29ffe40d5d043be02487d6c99854e1b243f"},"cell_type":"markdown","source":"## Target distribution"},{"metadata":{"_kg_hide-input":true,"_uuid":"d6f4d941fc04144f2b08cb23fee0d2f3cd48951d","trusted":true},"cell_type":"code","source":"sns.countplot(train_df['target'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"252293b05cf7256855abd8e7820b622600a24515"},"cell_type":"markdown","source":"## Missing values"},{"metadata":{"_uuid":"524bd4fa7ee7f694c22dd6f0a466bafdcfd6db66","trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)  \n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a4fdf0236b018244a558835c875fc3ca62000f9","trusted":true},"cell_type":"code","source":"missing_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"142ee3a5783ff0e8a3b64ec29fbd5ec1c400af87","trusted":true},"cell_type":"code","source":"missing_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b97a5451ed79c1c50d66de0190f2ba580dda6357"},"cell_type":"markdown","source":"## Unique values"},{"metadata":{"_uuid":"1336e6fe05ab77c19ae254bd8f4241f3cdc3f28a","trusted":true},"cell_type":"code","source":"train_unique_df = train_df[num_vars].nunique().reset_index().\\\n        rename(columns={'index':'feature',0:'unique'}).\\\n        sort_values('unique')\nsns.barplot(x='feature', y='unique',color='blue',\n    data=train_unique_df)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e279a13346fac40810173b7740da4411ed30073b","trusted":true},"cell_type":"code","source":"test_unique_df = test_df[num_vars].nunique().reset_index().\\\n        rename(columns={'index':'feature',0:'unique'}).\\\n        sort_values('unique')\nsns.barplot(x='feature', y='unique',color='blue',\n    data=test_unique_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba90e2f6a55284c98f4190ddef2cb557df45196d"},"cell_type":"markdown","source":"## Feature correlation"},{"metadata":{"_uuid":"39d6d579867a3896b6f4787bcecf27d791c679ec","trusted":true},"cell_type":"code","source":"corr_df = full_df[num_vars].corr()\ncorr_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffc6e882e9620db6d4200c6add3f13360770456","trusted":true},"cell_type":"code","source":"cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr_df, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f96ba0bc172f038ad830ebff4eafa26580a3a77"},"cell_type":"markdown","source":" ## Normality tests\n"},{"metadata":{"_uuid":"31787ccdf05c4531fcbcafeb6ede7db77409cbea","trusted":true},"cell_type":"code","source":"train_norm_df = train_df[num_vars].apply(lambda x:stats.normaltest(x)[1])\n\nprint(\"There are %d features normally distributed.\" % ((train_norm_df<0.05).sum())) \n\n\nprint(\"Top 5 features with highest P value:\")\ntrain_norm_df.sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6714aa0f978816a308f080f30a2ade757a483d2","trusted":true},"cell_type":"code","source":"test_norm_df = test_df[num_vars].apply(lambda x:stats.normaltest(x)[1])\n\nprint(\"There are %d features normally distributed.\" %((test_norm_df<0.05).sum()))\n\nprint(\"Top 5 features with highest P value:\")\ntest_norm_df.sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6b52ea9c46d38616965387be7fa017c6132f7ac","trusted":true},"cell_type":"code","source":"\nsns.distplot(train_df['var_146'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c31212e5a6af3f15c3684a5b2253267d916b7f0"},"cell_type":"markdown","source":"\n## Dimensionality Reduction\n"},{"metadata":{"_uuid":"5ae437d0a7fae0680501ff0e9d99844c740f3d81","trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de73b185f8ad5f1d698a1b27158a6b4884918601"},"cell_type":"markdown","source":"### TSNE"},{"metadata":{"_uuid":"3cdfe818ed9d792fa47df6b8b868baebdec7a6b8","trusted":true},"cell_type":"code","source":"%%time\n\ntsne = TSNE(n_components=1)\ntsne1d = tsne.fit_transform(train_df[num_vars][:10000].values)\ntsne1d_df = pd.DataFrame({'tsne_0':tsne1d.reshape(-1), 'target':train_df['target'][:10000].values})\nsns.distplot(tsne1d_df.query('target==0')['tsne_0'], label='target:0')\nsns.distplot(tsne1d_df.query('target==1')['tsne_0'], label='target:1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65359b5985286583ef514533b95a06f8300b2a96","trusted":true},"cell_type":"code","source":"%%time\n\ntsne = TSNE(n_components=2)\ntsne2d = tsne.fit_transform(train_df[num_vars][:10000].values)\ntsne2d_df = pd.DataFrame({'tsne_0':tsne2d[:,0],'tsne_1':tsne2d[:,1], \n                          'target':train_df['target'][:10000].values})\nsns.lmplot(x='tsne_0', y='tsne_1', data=tsne2d_df, hue='target', fit_reg=False)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39be2573d97034d84e882a983322f14d0febdb19"},"cell_type":"markdown","source":"### PCA"},{"metadata":{"_uuid":"d03a1f07b9bb917a4a56bf247a56966c3d286452","trusted":true},"cell_type":"code","source":"%%time\n\npca = PCA(n_components=2) \npca2d = pca.fit_transform(train_df[num_vars][:10000].values)\nprint (pca.explained_variance_ratio_) \nprint (pca.explained_variance_) \n\npca2d_df = pd.DataFrame({'pca_0':pca2d[:,0],\n                         'pca_1':pca2d[:,1], \n                          'target':train_df['target'][:10000].values})\n\nsns.lmplot(x='pca_0', y='pca_1', data=pca2d_df, hue='target', fit_reg=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1189cb5e3692815af19112899c77c458982ca7a8"},"cell_type":"markdown","source":"### TruncatedSVD"},{"metadata":{"_uuid":"61c8d703e6c0cb77296b19b98bb977396d952a54","trusted":true},"cell_type":"code","source":"%%time\n\nsvd = TruncatedSVD(n_components=2)\nsvd2d = svd.fit_transform(train_df[num_vars][:10000].values)\nsvd2d_df = pd.DataFrame({'svd_0':svd2d[:,0],'svd_1':svd2d[:,1], \n                          'target':train_df['target'][:10000].values})\nsns.lmplot(x='svd_0', y='svd_1', data=svd2d_df, hue='target', fit_reg=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c537f27ccf179e07f69c5df29cbbabfe4d2f893f"},"cell_type":"markdown","source":"# Feature engineering\n"},{"metadata":{"_uuid":"39278c8fc3c133b195a4599d74e5915e4afce7c2"},"cell_type":"markdown","source":"## Standardization"},{"metadata":{"_uuid":"c8cc46349f1136c24c1228a2479bad01fbca3402","trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\n\nstd_scaler.fit(full_df[num_vars].values) \ntrain_std_df = pd.DataFrame(std_scaler.transform(train_df[num_vars].values), columns=num_vars)\ntest_std_df = pd.DataFrame(std_scaler.transform(test_df[num_vars].values) , columns=num_vars)\n\ntrain_std_df['target'] = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493a92855ccf114c362909ca1f4550fe2cf240e3","trusted":true},"cell_type":"code","source":"train_std_df[num_vars].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75b114e90383add73b738fdc380dfd714b101e3b"},"cell_type":"markdown","source":"### Cross validation with raw feature"},{"metadata":{"_uuid":"ce4e0eeb65c67148bfe65c8d0ccc36bff1628c56","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ntrain_x = train_df[num_vars].values\ntrain_y = train_df['target'].values\ntest_x = test_df[num_vars].values\n\n\nlr_cv_raw = cross_val_score(LogisticRegression(),   \n                            train_x[:10000], train_y[:10000], \n                            scoring='roc_auc', \n                            cv=5,   \n                            n_jobs=-1) \n\nprint(\"Logistic regression CV score with raw features:\", lr_cv_raw.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cac3108167572e52577059cc795f580ec6de4a5"},"cell_type":"markdown","source":"### Cross validation with standardized features"},{"metadata":{"_uuid":"ca45d92d9708fb9448cf6d45f47c1f260b7b4ac9","trusted":true},"cell_type":"code","source":"%%time\ntrain_x = train_std_df[num_vars].values\ntrain_y = train_std_df['target'].values\ntest_x = test_std_df[num_vars].values\n\nlr_cv_std = cross_val_score(LogisticRegression(), \n                            train_x[:10000], train_y[:10000], \n                            scoring='roc_auc',\n                            cv=5, \n                            n_jobs=-1)\n\nprint(\"Logistic regression CV score with standardized features:\", lr_cv_std.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad89ca96c5ff2e0555c4af823adfd7ba8ee23ac"},"cell_type":"markdown","source":"#### Visualization "},{"metadata":{"_uuid":"90d0d917314f1ef3cb410f927e1c18198255a3dc","trusted":true},"cell_type":"code","source":"%%time\n\npca = PCA(n_components=2)\npca2d = pca.fit_transform(train_std_df[num_vars][:10000].values)\npca2d_df = pd.DataFrame({'pca_0':pca2d[:,0],'pca_1':pca2d[:,1], \n                          'target':train_df['target'][:10000].values})\nsns.lmplot(x='pca_0', y='pca_1', data=pca2d_df, hue='target', fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3e929c562a00b116291b24fbdb8ac0c27324852","trusted":true},"cell_type":"code","source":"%%time\n\npca = PCA(n_components=1)\npca1d = pca.fit_transform(train_std_df[num_vars][:10000].values)\npca1d_df = pd.DataFrame({'pca_0':pca1d.reshape(-1), 'target':train_df['target'][:10000].values})\nsns.distplot(pca1d_df.query('target==0')['pca_0'], label='target:0')\nsns.distplot(pca1d_df.query('target==1')['pca_0'], label='target:1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## L2 norm\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_std_df['norm_2'] = train_std_df[num_vars].apply(lambda x:np.linalg.norm(x), axis=1)\ntest_std_df['norm_2'] = test_std_df[num_vars].apply(lambda x:np.linalg.norm(x), axis=1)\n\nsns.distplot(train_std_df.query('target==0')['norm_2'], label='target:0')\nsns.distplot(train_std_df.query('target==1')['norm_2'], label='target:1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"roc_auc_score(train_y, train_std_df['norm_2'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71770806130c8eaba238fb443bfbca31d5b2f1e1"},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"_uuid":"922469a73a773cf076a6ee865a5a8aea9b626cc8"},"cell_type":"markdown","source":"## Feature importance from Logistic Regression"},{"metadata":{"_uuid":"9e374a7e2275c351d6cdc30f605c164e1d4b458c","trusted":true},"cell_type":"code","source":"train_x = train_std_df[num_vars].values\ntrain_y = train_std_df['target'].values\ntest_x = test_std_df[num_vars].values\n\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b685eee1cc3ec0bf9496af066ba8203ee9204942","trusted":true},"cell_type":"code","source":"lr_feature_importance = pd.DataFrame({'feature':num_vars, 'lr_importance':lr.coef_.reshape(-1), \n                                      'abs_lr_importance': abs(lr.coef_.reshape(-1))})\n                            \nlr_feature_importance.sort_values('abs_lr_importance', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01854d08867d63261aedf574ea2c03f3908fd168"},"cell_type":"markdown","source":"## Feature importance from LightGBM"},{"metadata":{"_uuid":"8300519accca37f83fc1671f1e6ea93e79d2f275","trusted":true},"cell_type":"code","source":"lgb_clf = lgb.LGBMClassifier(n_jobs=-1)\nlgb_clf.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0e79f861f5f0f30017ab8f923c83bfb64b5b663","trusted":true},"cell_type":"code","source":"lgb_feature_importance = pd.DataFrame({'feature':num_vars, \n                                       'lgb_importance':lgb_clf.feature_importances_.reshape(-1)})\n                                        \nlgb_feature_importance.sort_values('lgb_importance', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8f9eeaa80cbc52dce901d1c18020ca890f4dc6"},"cell_type":"markdown","source":"### Combined feature importance"},{"metadata":{"_uuid":"010267933d9fe2af00f42ac7264a08c871f2ecf2","trusted":true},"cell_type":"code","source":"feature_importance = pd.merge(lr_feature_importance, lgb_feature_importance, on='feature')\nfeature_importance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_std_df['var_53'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Target 1\nsns.distplot(train_std_df.query('target==1')['var_53'], label='target:0')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"effb024e4a786147c79a6ca98db0f2b4d676adf4","scrolled":true,"trusted":true},"cell_type":"code","source":"# Target 0\nsns.distplot(train_std_df.query('target==0')['var_53'], label='target:0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d663fb282a2dad31f27fc7c7ff0cbdaa4b880b","scrolled":true,"trusted":true},"cell_type":"code","source":"# var_53 against target\nsns.distplot(train_std_df.query('target==0')['var_53'], label='target:0')\nsns.distplot(train_std_df.query('target==1')['var_53'], label='target:1')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43effbf2a2ccba59d8f1c6190e0030c8527667c3","trusted":true},"cell_type":"code","source":"# var_81 against target\nsns.distplot(train_std_df.query('target==0')['var_81'], label='target:0')\nsns.distplot(train_std_df.query('target==1')['var_81'], label='target:1')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training our first model"},{"metadata":{"_uuid":"e72a9f371daaaf06ad70e3a7663ba2374046ef93","trusted":true},"cell_type":"code","source":"lgb_params = {\n    \"boost_from_average\": \"false\",\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 15,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\" : 0.8,\n    \"feature_fraction\" : 0.7,\n    \"verbosity\" : 1,\n    \"seed\": 42\n}\n\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\noof = train_df[['ID_code', 'target']]\noof['predict'] = 0\npredictions = test_df[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\n\nX_test = test_std_df[num_vars]\n\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(train_std_df, train_std_df['target'])):\n    X_train, y_train = train_std_df.iloc[trn_idx][num_vars], train_std_df.iloc[trn_idx]['target']\n    X_valid, y_valid = train_std_df.iloc[val_idx][num_vars], train_std_df.iloc[val_idx]['target']\n     \n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    \n    evals_result = {}\n    lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )    \n \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = num_vars\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) \n    \n    p_valid = lgb_clf.predict(X_valid)\n    oof['predict'][val_idx] = p_valid\n     \n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n   \n    predictions['fold{}'.format(fold+1)] = lgb_clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns \n                                             if col not in ['ID_code', 'target']]].values, axis=1)\n\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\n\nprint(predictions.head())\nprint(sub_df.head())\n\nsub_df.to_csv(\"lgb_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model tuning\n## LightGBM\n### Manual tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect() \nprint (\"starting...\")\n\nfull_vars = num_vars\ncat_vars = None\n\nfull_vars = num_vars\ntrain_x = train_df[full_vars].values\ntrain_y = train_df[target_var].values\ntest_x = test_df[full_vars].values\n\nimport copy\ndefault_lgb_params = {}\ndefault_lgb_params[\"learning_rate\"] = 0.1 \ndefault_lgb_params[\"metric\"] = 'auc'\ndefault_lgb_params[\"bagging_freq\"] = 1\ndefault_lgb_params[\"seed\"] = 42\ndefault_lgb_params[\"objective\"] = \"binary\"\ndefault_lgb_params[\"boost_from_average\"] = \"false\"\n\nparams_lgb_space = {}\nparams_lgb_space['feature_fraction'] = np.arange(0.1, 1, 0.1)\nparams_lgb_space['num_leaves'] = [2, 4, 8, 16, 32]  \nparams_lgb_space['max_depth'] = [3 ,4 ,5 ,6, -1]\nparams_lgb_space['min_gain_to_split'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\nparams_lgb_space['bagging_fraction'] = np.arange(0.1, 1, 0.1)\nparams_lgb_space['min_sum_hessian_in_leaf'] = [1, 5, 10, 30, 100]\nparams_lgb_space['lambda_l1'] = [0, 0.01, 0.1, 1, 10, 100, 300]\nparams_lgb_space['lambda_l2'] = [0, 0.01, 0.1, 1, 10, 100, 300]\n\ngreater_is_better = True\n\nbest_lgb_params = copy.copy(default_lgb_params)\n\n\nfor p in params_lgb_space: \n    print (\"\\n Tuning parameter %s in %s\" % (p, params_lgb_space[p]))\n    params = best_lgb_params\n    scores = []    \n    for v in params_lgb_space[p]: \n        gc.collect()\n        print ('\\n    %s: %s' % (p, v), end=\"\\n\")\n        params[p] = v\n        \n        cv_results = lgb.cv(params, \n                        lgb.Dataset(train_x, label=train_y), \n                        stratified=True,\n                        shuffle=True,\n                        nfold=5,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100,\n                        verbose_eval=0)\n        \n        best_lgb_score = max(cv_results['auc-mean'])\n        print ('Score: %f ' % (best_lgb_score))\n        scores.append([v, best_lgb_score])\n\n    # best param value in the space\n    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n    best_lgb_params[p] = best_param_value\n    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n\n    \nbest_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\nbest_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\nbest_lgb_params[p] = best_param_value\nprint (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\nprint ('\\n Best manually tuned parameters:', best_lgb_params)   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrain the model with manually tuned parameters\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_param_value = {'learning_rate': 0.1, 'metric': 'auc', \n                    'seed': 42, 'objective': 'binary', \"boost_from_average\": \"false\",\n                    'feature_fraction': 0.1, 'bagging_freq': 1, \n                    'num_leaves': 2, 'max_depth': 3, 'min_gain_to_split': 0, \n                    'bagging_fraction': 0.4, 'min_sum_hessian_in_leaf': 30, \n                    'lambda_l2': 0.01, 'lambda_l1': 0.01}\n\nbest_param_value['learning_rate'] = 0.01","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e72a9f371daaaf06ad70e3a7663ba2374046ef93","trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\noof = train_df[['ID_code', 'target']]\noof['predict'] = 0\npredictions = test_df[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\n\nX_test = test_df[num_vars]\n\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n    X_train, y_train = train_df.iloc[trn_idx][num_vars], train_df.iloc[trn_idx]['target']\n    X_valid, y_valid = train_df.iloc[val_idx][num_vars], train_df.iloc[val_idx]['target']\n    \n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    evals_result = {}\n    \n    lgb_clf = lgb.train(best_param_value,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )\n    \n  \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = num_vars\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    p_valid = lgb_clf.predict(X_valid)\n    oof['predict'][val_idx] = p_valid\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    \n    predictions['fold{}'.format(fold+1)] = lgb_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns \n                                             if col not in ['ID_code', 'target']]].values, \n                                axis=1)\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\n\nsub_df.to_csv(\"lgb_sub_manual_tuned.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automated tuning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train_df[num_vars].values\ntrain_y = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\n\n\ndef lgb_evaluate(\n    num_leaves,\n    max_depth,\n    min_sum_hessian_in_leaf,\n    min_gain_to_split,\n    feature_fraction,\n    bagging_fraction,\n    lambda_l2,\n    lambda_l1\n):\n    params = dict()\n    params['objective'] = 'binary'\n    params['learning_rate'] = 0.1\n    params['seed'] = 1234\n    params['num_leaves'] = int(num_leaves)\n    params['max_depth'] = int(max_depth)\n    params['min_sum_hessian_in_leaf'] = int(min_sum_hessian_in_leaf)\n    params['min_gain_to_split'] = min_gain_to_split\n    params['feature_fraction'] = feature_fraction\n    params['bagging_fraction'] = bagging_fraction\n    params['bagging_freq'] = 1\n    params['lambda_l2'] = lambda_l2\n    params['lambda_l1'] = lambda_l1\n    params[\"metric\"] = 'auc'\n\n    cv_results = lgb.cv(params,\n                        lgb.Dataset(train_x, label=train_y),\n                        stratified=True,\n                        shuffle=True,\n                        nfold=5,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100,\n                        verbose_eval=0)\n    best_lgb_score = max(cv_results['auc-mean'])\n    print ('Score: %f ' % (best_lgb_score))\n    return best_lgb_score\n\n\nlgb_BO = BayesianOptimization(lgb_evaluate,\n                              {\n                                  'num_leaves': (2,72),\n                                  'max_depth': (-1, -1),\n                                  'min_sum_hessian_in_leaf': (0, 100),\n                                  'min_gain_to_split': (0, 100),\n                                  'feature_fraction': (0.005, 0.1),\n                                  'bagging_fraction': (0.3, 0.7),\n                                  'lambda_l2': (0, 1),\n                                  'lambda_l1': (0, 1)\n                              }\n                              )\n\nlgb_BO.maximize(init_points=3, n_iter=7)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_BO.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_BO.res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [{**x, **x.pop('params')} for x in xgb_BO.max]\nxgb_BO_scores = pd.DataFrame(a)\nxgb_BO_max = pd.DataFrame(xgb_BO.max).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params= lgb_BO_max.iloc[1].to_dict()\n#params = lgb_BO_scores.iloc[0].to_dict()\nbest_lgb_auto_params = dict()\nbest_lgb_auto_params['objective'] = 'binary'\nbest_lgb_auto_params[\"metric\"] = 'auc'\nbest_lgb_auto_params['learning_rate'] = 0.01 # Smaller learning rate\nbest_lgb_auto_params['num_leaves'] = int(params['num_leaves'])    \nbest_lgb_auto_params['max_depth'] = int(params['max_depth'])    \nbest_lgb_auto_params['min_sum_hessian_in_leaf'] = params['min_sum_hessian_in_leaf']\nbest_lgb_auto_params['min_gain_to_split'] = params['min_gain_to_split']     \nbest_lgb_auto_params['feature_fraction'] = params['feature_fraction']\nbest_lgb_auto_params['bagging_fraction'] = params['bagging_fraction']\nbest_lgb_auto_params['bagging_freq'] = 1\nbest_lgb_auto_params['lambda_l2'] = params['lambda_l2']\nbest_lgb_auto_params['lambda_l1'] = params['lambda_l1']\nbest_lgb_auto_params['random_state'] = 4590\nbest_lgb_auto_params[\"n_jobs\"] = 8\n\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\noof = train_df[['ID_code', 'target']]\noof['predict'] = 0\npredictions = test_df[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\n\nX_test = test_df[num_vars]\n\n\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n    X_train, y_train = train_df.iloc[trn_idx][num_vars], train_df.iloc[trn_idx]['target']\n    X_valid, y_valid = train_df.iloc[val_idx][num_vars], train_df.iloc[val_idx]['target']\n    \n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    evals_result = {}\n    lgb_clf = lgb.train(best_lgb_auto_params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = num_vars\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    p_valid = lgb_clf.predict(X_valid)\n    oof['predict'][val_idx] = p_valid\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    \n    predictions['fold{}'.format(fold+1)] = lgb_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgb_binary_stack(rgr_params, train_x, train_y, test_x, kfolds, stratified=False,  random_state=42,\n                     early_stopping_rounds=0, missing=None, full_vars=None, cat_vars=None, y_dummy=None, verbose=False):\n    if stratified:\n        kf = StratifiedKFold(n_splits=kfolds, shuffle=True,\n                             random_state=random_state)\n        kf_ids = list(kf.split(train_x, y_dummy))\n    else:\n        kf = KFold(n_splits=kfolds, random_state=random_state)\n        kf_ids = list(kf.split(train_y))\n\n    train_blend_x = np.zeros((train_x.shape[0], len(rgr_params)))\n    test_blend_x = np.zeros((test_x.shape[0], len(rgr_params)))\n    blend_scores = np.zeros((kfolds, len(rgr_params)))\n    if verbose:\n        print(\"Start stacking.\")\n    for j, params in enumerate(rgr_params):\n        if verbose:\n            print(\"Stacking model\", j+1, params)\n        test_blend_x_j = np.zeros((test_x.shape[0]))\n        for i, (train_ids, val_ids) in enumerate(kf_ids):\n            start = time.time()\n            if verbose:\n                print(\"Model %d fold %d\" % (j+1, i+1))\n            train_x_fold = train_x[train_ids]\n            train_y_fold = train_y[train_ids]\n            val_x_fold = train_x[val_ids]\n            val_y_fold = train_y[val_ids]\n            if verbose:\n                print(i, params)\n\n            train_dataset = lgb.Dataset(train_x_fold,\n                                        train_y_fold,\n                                        feature_name=full_vars,\n                                        categorical_feature=cat_vars\n                                        )\n            valid_dataset = lgb.Dataset(val_x_fold,\n                                        val_y_fold,\n                                        feature_name=full_vars,\n                                        categorical_feature=cat_vars\n                                        )\n\n            if early_stopping_rounds == 0:\n                num_boost_round = copy.deepcopy(params['num_boost_round'])\n                model = lgb.train(params,\n                                  train_dataset,\n                                  num_boost_round=num_boost_round,\n                                  valid_sets=[train_dataset, valid_dataset],\n                                  valid_names=['train', 'valid'],\n                                  verbose_eval=verbose\n                                  )\n                val_y_predict_fold = model.predict(val_x_fold)\n                score = roc_auc_score(val_y_fold, val_y_predict_fold)\n                if verbose:\n                    print(\"Score for Model %d fold %d: %f \" %\n                          (j+1, i+1, score))\n                blend_scores[i, j] = score\n                train_blend_x[val_ids, j] = val_y_predict_fold\n                test_blend_x_j = test_blend_x_j + model.predict(test_x)\n                if verbose:\n                    print(\"Model %d fold %d finished in %d seconds.\" %\n                          (j+1, i+1, time.time()-start))\n            else:\n                model = lgb.train(params,\n                                  train_dataset,\n                                  valid_sets=[train_dataset, valid_dataset],\n                                  valid_names=['train', 'valid'],\n                                  num_boost_round=150000,\n                                  early_stopping_rounds=early_stopping_rounds,\n                                  verbose_eval=verbose\n                                  )\n                best_iteration = model.best_iteration\n                if verbose:\n                    print(model.best_score['valid']['auc'])\n                val_y_predict_fold = model.predict(\n                    val_x_fold)\n                score = roc_auc_score(val_y_fold, val_y_predict_fold)\n                if verbose:\n                    print(\"Score for Model %d fold %d: %f \" %\n                          (j+1, i+1, score))\n                blend_scores[i, j] = score\n                train_blend_x[val_ids, j] = val_y_predict_fold\n                test_blend_x_j = test_blend_x_j + \\\n                    model.predict(test_x)\n                if verbose:\n                    print(\"Model %d fold %d finished in %d seconds.\" %\n                          (j+1, i+1, time.time()-start))\n\n        test_blend_x[:, j] = test_blend_x_j/kfolds\n        print(\"Score for model %d is %f\" % (j+1, np.mean(blend_scores[:, j])))\n    return train_blend_x, test_blend_x, blend_scores\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef sk_binary_stack(models, train_x, train_y, test_x, kfolds, random_state=42, verbose_eval=1,\n                    stratified=True):\n\n    if stratified:\n        kf = StratifiedKFold(n_splits=kfolds, shuffle=True,\n                             random_state=random_state)\n        kf_ids = list(kf.split(train_x, train_y))\n    else:\n        kf = KFold(n_splits=kfolds, random_state=random_state)\n        kf_ids = kf.split(train_y)\n\n    train_blend_x = np.zeros((train_x.shape[0], len(models)))\n    test_blend_x = np.zeros((test_x.shape[0], len(models)))\n    blend_scores = np.zeros((kfolds, len(models)))\n\n    if verbose_eval > 0:\n        print(\"Start stacking.\")\n    for j, model in enumerate(models):\n        if verbose_eval > 0:\n            print(\"Stacking model\", j+1, model)\n        test_blend_x_j = np.zeros((test_x.shape[0]))\n        for i, (train_ids, val_ids) in enumerate(kf_ids):\n            start = time.time()\n            if verbose_eval > 0:\n                print(\"Model %d fold %d\" % (j+1, i+1))\n            train_x_fold = train_x[train_ids, :]\n            train_y_fold = train_y[train_ids]\n            val_x_fold = train_x[val_ids, :]\n            val_y_fold = train_y[val_ids]\n            if verbose_eval > 0:\n                print(i, model)\n\n            model.fit(train_x_fold, train_y_fold)\n            val_y_predict_fold = model.predict_proba(val_x_fold)[:, 1]\n            score = roc_auc_score(val_y_fold, val_y_predict_fold)\n            if verbose_eval > 0:\n                print(\"Score for Model %d fold %d: %f \" % (j+1, i+1, score))\n            blend_scores[i, j] = score\n            train_blend_x[val_ids, j] = val_y_predict_fold\n            test_blend_x_j = test_blend_x_j + model.predict_proba(test_x)[:, 1]\n            if verbose_eval > 0:\n                print(\"Model %d fold %d finished in %d seconds.\" %\n                      (j+1, i+1, time.time()-start))\n\n        test_blend_x[:, j] = test_blend_x_j/kfolds\n        if verbose_eval > 0:\n            print(\"Score for model %d is %f\" %\n                  (j+1, np.mean(blend_scores[:, j])))\n    return train_blend_x, test_blend_x, blend_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Level 1 LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_vars = num_vars\ntrain_x = train_df[full_vars].values\ntrain_y = train_df[target_var].values\ntest_x = test_df[full_vars].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [{**x, **x.pop('params')} for x in lgb_BO.res]\nlgb_BO_scores = pd.DataFrame(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_stack_params = []\nfor i in range(3):\n    params = lgb_BO_scores.iloc[i].to_dict()\n    lgb_params = dict()\n    lgb_params['objective'] = 'binary'\n    lgb_params[\"metric\"] = 'auc'\n    lgb_params['learning_rate'] = 0.01 # Smaller learning rate\n    lgb_params['num_leaves'] = int(params['num_leaves'])    \n    lgb_params['max_depth'] = int(params['max_depth'])    \n    lgb_params['min_sum_hessian_in_leaf'] = params['min_sum_hessian_in_leaf']\n    lgb_params['min_gain_to_split'] = params['min_gain_to_split']     \n    lgb_params['feature_fraction'] = params['feature_fraction']\n    lgb_params['bagging_fraction'] = params['bagging_fraction']\n    lgb_params['bagging_freq'] = 1\n    lgb_params['lambda_l2'] = params['lambda_l2']\n    lgb_params['lambda_l1'] = params['lambda_l1']\n    lgb_params['random_state'] = 42\n    lgb_params[\"n_jobs\"] = 8\n    lgb_stack_params.append(lgb_params)\n\nprint (lgb_stack_params)\n\ntrain_stack_x_lgb, test_stack_x_lgb, blend_scores_lgb = \\\n        lgb_binary_stack(lgb_stack_params, \n                         train_x, train_y, test_x, \n                         5, \n                         early_stopping_rounds=200, \n                         stratified=True, \n                         random_state=4590,\n                         full_vars=full_vars, \n                         cat_vars=None,\n                         y_dummy=train_y )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Level 2 - Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ntrain_stack_x_l1 = copy.copy(train_stack_x_lgb)\ntest_stack_x_l1 = copy.copy(test_stack_x_lgb)\n\n\nl2_stack_models = [LogisticRegression()\n                  ]\ntrain_sk_stack_x_l2, test_sk_stack_x_l2, _ = \\\n        sk_binary_stack(l2_stack_models, \n                        train_stack_x_l1, train_y, test_stack_x_l1, \n                        5, \n                        #y_dummy=train_y, \n                        random_state=42,\n                        stratified=True)\n\nprint('All AUC for level 2 Logistic Regression:', roc_auc_score( train_y, train_sk_stack_x_l2.mean(axis=1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create submission\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = test_sk_stack_x_l2.mean(axis=1)\nsub_df.to_csv(\"sub_l1_lgb_l2_lr.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Level 2 - LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stack_x_l1 = np.hstack((train_x, train_stack_x_lgb))\ntest_stack_x_l1 = np.hstack((test_x, test_stack_x_lgb))\n\ntrain_stack_x_lgb_l2, test_stack_x_lgb_l2, blend_scores_lgb = \\\n        lgb_binary_stack(lgb_stack_params, \n                         train_stack_x_l1, train_y, test_stack_x_l1, \n                         5, \n                         early_stopping_rounds=200, \n                         stratified=True, \n                         random_state=4590,\n                         full_vars=full_vars, \n                         cat_vars=None,\n                         y_dummy=train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create submission\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = test_stack_x_lgb_l2.mean(axis=1)\nsub_df.to_csv(\"sub_l1_lgb_l2_lgb.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"279px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}