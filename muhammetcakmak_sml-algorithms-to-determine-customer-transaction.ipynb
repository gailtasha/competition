{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Supervised Machine Learning Classifications\nIn this tutorial I will apply supervised machine learning classifications to the canser data sets in order to determine if tested data has heart diseases or not. I will use KNN classification, decision tree classification, random forest classification,Support vector machine, logistig regression and naive bayes algorithms. I will show also how to determine accuracy of the each classificaiton and make evaluation by using confusion matrix.\n\n1. [EDA(Exploratory Data Analaysis)](#1)\n2. [Logistic Regression Classification](#2)\n4. [Decision Tree Classification](#3)\n5. [Random Forest Classification](#4)\n7. [Naive Bayes Classification](#5)\n6. [Conclusion](#6)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":16,"outputs":[{"output_type":"stream","text":"['sample_submission.csv', 'test.csv', 'train.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read the data and assign it as df_train and df_test \ndf_train=pd.read_csv(\"../input/train.csv\")\ndf_test=pd.read_csv(\"../input/test.csv\")","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n1. EDA(Exploratory Data Analaysis)\n\nEDA is very important to look at what is inside the data. For example, if there is object(string) in the data, we need to change it to integer or float because sci-learn is not handling with object data. There are also missdata in the datasets, we need to handle them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a quick look into data.This code shows first 5 rows and all columns\ndf_train.head()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a quick look into data.This code shows first 5 rows and all columns\ndf_test.head()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"  ID_code    var_0    var_1   ...     var_197  var_198  var_199\n0  test_0  11.0656   7.7798   ...     10.7200  15.4722  -8.7197\n1  test_1   8.5304   1.2543   ...      9.8714  19.1293 -20.9760\n2  test_2   5.4827 -10.3581   ...      7.0618  19.8956 -23.1794\n3  test_3   8.5374  -1.3222   ...      9.2295  13.0168  -4.2108\n4  test_4  11.7058  -0.1327   ...      7.2882  13.9260  -9.1846\n\n[5 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>11.0656</td>\n      <td>7.7798</td>\n      <td>12.9536</td>\n      <td>9.4292</td>\n      <td>11.4327</td>\n      <td>-2.3805</td>\n      <td>5.8493</td>\n      <td>18.2675</td>\n      <td>2.1337</td>\n      <td>8.8100</td>\n      <td>-2.0248</td>\n      <td>-4.3554</td>\n      <td>13.9696</td>\n      <td>0.3458</td>\n      <td>7.5408</td>\n      <td>14.5001</td>\n      <td>7.7028</td>\n      <td>-19.0919</td>\n      <td>15.5806</td>\n      <td>16.1763</td>\n      <td>3.7088</td>\n      <td>18.8064</td>\n      <td>1.5899</td>\n      <td>3.0654</td>\n      <td>6.4509</td>\n      <td>14.1192</td>\n      <td>-9.4902</td>\n      <td>-2.1917</td>\n      <td>5.7107</td>\n      <td>3.7864</td>\n      <td>-1.7981</td>\n      <td>9.2645</td>\n      <td>2.0657</td>\n      <td>12.7753</td>\n      <td>11.3334</td>\n      <td>8.1462</td>\n      <td>-0.0610</td>\n      <td>3.5331</td>\n      <td>9.7804</td>\n      <td>...</td>\n      <td>5.9232</td>\n      <td>5.4113</td>\n      <td>3.8302</td>\n      <td>5.7380</td>\n      <td>-8.6105</td>\n      <td>22.9530</td>\n      <td>2.5531</td>\n      <td>-0.2836</td>\n      <td>4.3416</td>\n      <td>5.1855</td>\n      <td>4.2603</td>\n      <td>1.6779</td>\n      <td>29.0849</td>\n      <td>8.4685</td>\n      <td>18.1317</td>\n      <td>12.2818</td>\n      <td>-0.6912</td>\n      <td>10.2226</td>\n      <td>-5.5579</td>\n      <td>2.2926</td>\n      <td>-4.5358</td>\n      <td>10.3903</td>\n      <td>-15.4937</td>\n      <td>3.9697</td>\n      <td>31.3521</td>\n      <td>-1.1651</td>\n      <td>9.2874</td>\n      <td>-23.5705</td>\n      <td>13.2643</td>\n      <td>1.6591</td>\n      <td>-2.1556</td>\n      <td>11.8495</td>\n      <td>-1.4300</td>\n      <td>2.4508</td>\n      <td>13.7112</td>\n      <td>2.4669</td>\n      <td>4.3654</td>\n      <td>10.7200</td>\n      <td>15.4722</td>\n      <td>-8.7197</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>8.5304</td>\n      <td>1.2543</td>\n      <td>11.3047</td>\n      <td>5.1858</td>\n      <td>9.1974</td>\n      <td>-4.0117</td>\n      <td>6.0196</td>\n      <td>18.6316</td>\n      <td>-4.4131</td>\n      <td>5.9739</td>\n      <td>-1.3809</td>\n      <td>-0.3310</td>\n      <td>14.1129</td>\n      <td>2.5667</td>\n      <td>5.4988</td>\n      <td>14.1853</td>\n      <td>7.0196</td>\n      <td>4.6564</td>\n      <td>29.1609</td>\n      <td>0.0910</td>\n      <td>12.1469</td>\n      <td>3.1389</td>\n      <td>5.2578</td>\n      <td>2.4228</td>\n      <td>16.2064</td>\n      <td>13.5023</td>\n      <td>-5.2341</td>\n      <td>-3.6648</td>\n      <td>5.7080</td>\n      <td>2.9965</td>\n      <td>-10.4720</td>\n      <td>11.4938</td>\n      <td>-0.9660</td>\n      <td>15.3445</td>\n      <td>10.6361</td>\n      <td>0.8966</td>\n      <td>6.7428</td>\n      <td>2.3421</td>\n      <td>12.8678</td>\n      <td>...</td>\n      <td>30.9641</td>\n      <td>5.6723</td>\n      <td>3.6873</td>\n      <td>13.0429</td>\n      <td>-10.6572</td>\n      <td>15.5134</td>\n      <td>3.2185</td>\n      <td>9.0535</td>\n      <td>7.0535</td>\n      <td>5.3924</td>\n      <td>-0.7720</td>\n      <td>-8.1783</td>\n      <td>29.9227</td>\n      <td>-5.6274</td>\n      <td>10.5018</td>\n      <td>9.6083</td>\n      <td>-0.4935</td>\n      <td>8.1696</td>\n      <td>-4.3605</td>\n      <td>5.2110</td>\n      <td>0.4087</td>\n      <td>12.0030</td>\n      <td>-10.3812</td>\n      <td>5.8496</td>\n      <td>25.1958</td>\n      <td>-8.8468</td>\n      <td>11.8263</td>\n      <td>-8.7112</td>\n      <td>15.9072</td>\n      <td>0.9812</td>\n      <td>10.6165</td>\n      <td>8.8349</td>\n      <td>0.9403</td>\n      <td>10.1282</td>\n      <td>15.5765</td>\n      <td>0.4773</td>\n      <td>-1.4852</td>\n      <td>9.8714</td>\n      <td>19.1293</td>\n      <td>-20.9760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>5.4827</td>\n      <td>-10.3581</td>\n      <td>10.1407</td>\n      <td>7.0479</td>\n      <td>10.2628</td>\n      <td>9.8052</td>\n      <td>4.8950</td>\n      <td>20.2537</td>\n      <td>1.5233</td>\n      <td>8.3442</td>\n      <td>-4.7057</td>\n      <td>-3.0422</td>\n      <td>13.6751</td>\n      <td>3.8183</td>\n      <td>10.8535</td>\n      <td>14.2126</td>\n      <td>9.8837</td>\n      <td>2.6541</td>\n      <td>21.2181</td>\n      <td>20.8163</td>\n      <td>12.4666</td>\n      <td>12.3696</td>\n      <td>4.7473</td>\n      <td>2.7936</td>\n      <td>5.2189</td>\n      <td>13.5670</td>\n      <td>-15.4246</td>\n      <td>-0.1655</td>\n      <td>7.2633</td>\n      <td>3.4310</td>\n      <td>-9.1508</td>\n      <td>9.7320</td>\n      <td>3.1062</td>\n      <td>22.3076</td>\n      <td>11.9593</td>\n      <td>9.9255</td>\n      <td>4.0702</td>\n      <td>4.9934</td>\n      <td>8.0667</td>\n      <td>...</td>\n      <td>39.3654</td>\n      <td>5.5228</td>\n      <td>3.3159</td>\n      <td>4.3324</td>\n      <td>-0.5382</td>\n      <td>13.3009</td>\n      <td>3.1243</td>\n      <td>-4.1731</td>\n      <td>1.2330</td>\n      <td>6.1513</td>\n      <td>-0.0391</td>\n      <td>1.4950</td>\n      <td>16.8874</td>\n      <td>-2.9787</td>\n      <td>27.4035</td>\n      <td>15.8819</td>\n      <td>-10.9660</td>\n      <td>15.6415</td>\n      <td>-9.4056</td>\n      <td>4.4611</td>\n      <td>-3.0835</td>\n      <td>8.5549</td>\n      <td>-2.8517</td>\n      <td>13.4770</td>\n      <td>24.4721</td>\n      <td>-3.4824</td>\n      <td>4.9178</td>\n      <td>-2.0720</td>\n      <td>11.5390</td>\n      <td>1.1821</td>\n      <td>-0.7484</td>\n      <td>10.9935</td>\n      <td>1.9803</td>\n      <td>2.1800</td>\n      <td>12.9813</td>\n      <td>2.1281</td>\n      <td>-7.1086</td>\n      <td>7.0618</td>\n      <td>19.8956</td>\n      <td>-23.1794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>8.5374</td>\n      <td>-1.3222</td>\n      <td>12.0220</td>\n      <td>6.5749</td>\n      <td>8.8458</td>\n      <td>3.1744</td>\n      <td>4.9397</td>\n      <td>20.5660</td>\n      <td>3.3755</td>\n      <td>7.4578</td>\n      <td>0.0095</td>\n      <td>-5.0659</td>\n      <td>14.0526</td>\n      <td>13.5010</td>\n      <td>8.7660</td>\n      <td>14.7352</td>\n      <td>10.0383</td>\n      <td>-15.3508</td>\n      <td>2.1273</td>\n      <td>21.4797</td>\n      <td>14.5372</td>\n      <td>12.5527</td>\n      <td>2.9707</td>\n      <td>4.2398</td>\n      <td>13.7796</td>\n      <td>14.1408</td>\n      <td>1.0061</td>\n      <td>-1.3479</td>\n      <td>5.2570</td>\n      <td>6.5911</td>\n      <td>6.2161</td>\n      <td>9.5540</td>\n      <td>2.3628</td>\n      <td>10.2124</td>\n      <td>10.8047</td>\n      <td>-2.5588</td>\n      <td>6.0720</td>\n      <td>3.2613</td>\n      <td>16.5632</td>\n      <td>...</td>\n      <td>19.7251</td>\n      <td>5.3882</td>\n      <td>3.6775</td>\n      <td>7.4753</td>\n      <td>-11.0780</td>\n      <td>24.8712</td>\n      <td>2.6415</td>\n      <td>2.2673</td>\n      <td>7.2788</td>\n      <td>5.6406</td>\n      <td>7.2048</td>\n      <td>3.4504</td>\n      <td>2.4130</td>\n      <td>11.1674</td>\n      <td>14.5499</td>\n      <td>10.6151</td>\n      <td>-5.7922</td>\n      <td>13.9407</td>\n      <td>7.1078</td>\n      <td>1.1019</td>\n      <td>9.4590</td>\n      <td>9.8243</td>\n      <td>5.9917</td>\n      <td>5.1634</td>\n      <td>8.1154</td>\n      <td>3.6638</td>\n      <td>3.3102</td>\n      <td>-19.7819</td>\n      <td>13.4499</td>\n      <td>1.3104</td>\n      <td>9.5702</td>\n      <td>9.0766</td>\n      <td>1.6580</td>\n      <td>3.5813</td>\n      <td>15.1874</td>\n      <td>3.1656</td>\n      <td>3.9567</td>\n      <td>9.2295</td>\n      <td>13.0168</td>\n      <td>-4.2108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>11.7058</td>\n      <td>-0.1327</td>\n      <td>14.1295</td>\n      <td>7.7506</td>\n      <td>9.1035</td>\n      <td>-8.5848</td>\n      <td>6.8595</td>\n      <td>10.6048</td>\n      <td>2.9890</td>\n      <td>7.1437</td>\n      <td>5.1025</td>\n      <td>-3.2827</td>\n      <td>14.1013</td>\n      <td>8.9672</td>\n      <td>4.7276</td>\n      <td>14.5811</td>\n      <td>11.8615</td>\n      <td>3.1480</td>\n      <td>18.0126</td>\n      <td>13.8006</td>\n      <td>1.6026</td>\n      <td>16.3059</td>\n      <td>6.7954</td>\n      <td>3.6015</td>\n      <td>13.6569</td>\n      <td>13.8807</td>\n      <td>8.6228</td>\n      <td>-2.2654</td>\n      <td>5.2255</td>\n      <td>7.0165</td>\n      <td>-15.6961</td>\n      <td>10.6239</td>\n      <td>-4.7674</td>\n      <td>17.5447</td>\n      <td>11.8668</td>\n      <td>3.0154</td>\n      <td>4.2546</td>\n      <td>6.7601</td>\n      <td>5.9613</td>\n      <td>...</td>\n      <td>22.8700</td>\n      <td>5.6688</td>\n      <td>6.1159</td>\n      <td>13.2433</td>\n      <td>-11.9785</td>\n      <td>26.2040</td>\n      <td>3.2348</td>\n      <td>-5.5775</td>\n      <td>5.7036</td>\n      <td>6.1717</td>\n      <td>-1.6039</td>\n      <td>-2.4866</td>\n      <td>17.2728</td>\n      <td>2.3640</td>\n      <td>14.0037</td>\n      <td>12.9165</td>\n      <td>-12.0311</td>\n      <td>10.1161</td>\n      <td>-8.7562</td>\n      <td>6.0889</td>\n      <td>-1.3620</td>\n      <td>10.3559</td>\n      <td>-7.4915</td>\n      <td>9.4588</td>\n      <td>3.9829</td>\n      <td>5.8580</td>\n      <td>8.3635</td>\n      <td>-24.8254</td>\n      <td>11.4928</td>\n      <td>1.6321</td>\n      <td>4.2259</td>\n      <td>9.1723</td>\n      <td>1.2835</td>\n      <td>3.3778</td>\n      <td>19.5542</td>\n      <td>-0.2860</td>\n      <td>-5.1612</td>\n      <td>7.2882</td>\n      <td>13.9260</td>\n      <td>-9.1846</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If there is unknown,missing or unproper data, this codes shows the number of them\n# We can also learn about features such as data type of the features\ndf_train.info()","execution_count":20,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200000 entries, 0 to 199999\nColumns: 202 entries, ID_code to var_199\ndtypes: float64(200), int64(1), object(1)\nmemory usage: 308.2+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":21,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200000 entries, 0 to 199999\nColumns: 201 entries, ID_code to var_199\ndtypes: float64(200), object(1)\nmemory usage: 306.7+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statistical data is important to learn about balance inside or among the features.\ndf_train.describe()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"              target          var_0      ...              var_198        var_199\ncount  200000.000000  200000.000000      ...        200000.000000  200000.000000\nmean        0.100490      10.679914      ...            15.870720      -3.326537\nstd         0.300653       3.040051      ...             3.010945      10.438015\nmin         0.000000       0.408400      ...             6.299300     -38.852800\n25%         0.000000       8.453850      ...            13.829700     -11.208475\n50%         0.000000      10.524750      ...            15.934050      -2.819550\n75%         0.000000      12.758200      ...            18.064725       4.836800\nmax         1.000000      20.315000      ...            26.079100      28.500700\n\n[8 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.100490</td>\n      <td>10.679914</td>\n      <td>-1.627622</td>\n      <td>10.715192</td>\n      <td>6.796529</td>\n      <td>11.078333</td>\n      <td>-5.065317</td>\n      <td>5.408949</td>\n      <td>16.545850</td>\n      <td>0.284162</td>\n      <td>7.567236</td>\n      <td>0.394340</td>\n      <td>-3.245596</td>\n      <td>14.023978</td>\n      <td>8.530232</td>\n      <td>7.537606</td>\n      <td>14.573126</td>\n      <td>9.333264</td>\n      <td>-5.696731</td>\n      <td>15.244013</td>\n      <td>12.438567</td>\n      <td>13.290894</td>\n      <td>17.257883</td>\n      <td>4.305430</td>\n      <td>3.019540</td>\n      <td>10.584400</td>\n      <td>13.667496</td>\n      <td>-4.055133</td>\n      <td>-1.137908</td>\n      <td>5.532980</td>\n      <td>5.053874</td>\n      <td>-7.687740</td>\n      <td>10.393046</td>\n      <td>-0.512886</td>\n      <td>14.774147</td>\n      <td>11.434250</td>\n      <td>3.842499</td>\n      <td>2.187230</td>\n      <td>5.868899</td>\n      <td>10.642131</td>\n      <td>...</td>\n      <td>24.259300</td>\n      <td>5.633293</td>\n      <td>5.362896</td>\n      <td>11.002170</td>\n      <td>-2.871906</td>\n      <td>19.315753</td>\n      <td>2.963335</td>\n      <td>-4.151155</td>\n      <td>4.937124</td>\n      <td>5.636008</td>\n      <td>-0.004962</td>\n      <td>-0.831777</td>\n      <td>19.817094</td>\n      <td>-0.677967</td>\n      <td>20.210677</td>\n      <td>11.640613</td>\n      <td>-2.799585</td>\n      <td>11.882933</td>\n      <td>-1.014064</td>\n      <td>2.591444</td>\n      <td>-2.741666</td>\n      <td>10.085518</td>\n      <td>0.719109</td>\n      <td>8.769088</td>\n      <td>12.756676</td>\n      <td>-3.983261</td>\n      <td>8.970274</td>\n      <td>-10.335043</td>\n      <td>15.377174</td>\n      <td>0.746072</td>\n      <td>3.234440</td>\n      <td>7.438408</td>\n      <td>1.927839</td>\n      <td>3.331774</td>\n      <td>17.993784</td>\n      <td>-0.142088</td>\n      <td>2.303335</td>\n      <td>8.908158</td>\n      <td>15.870720</td>\n      <td>-3.326537</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.300653</td>\n      <td>3.040051</td>\n      <td>4.050044</td>\n      <td>2.640894</td>\n      <td>2.043319</td>\n      <td>1.623150</td>\n      <td>7.863267</td>\n      <td>0.866607</td>\n      <td>3.418076</td>\n      <td>3.332634</td>\n      <td>1.235070</td>\n      <td>5.500793</td>\n      <td>5.970253</td>\n      <td>0.190059</td>\n      <td>4.639536</td>\n      <td>2.247908</td>\n      <td>0.411711</td>\n      <td>2.557421</td>\n      <td>6.712612</td>\n      <td>7.851370</td>\n      <td>7.996694</td>\n      <td>5.876254</td>\n      <td>8.196564</td>\n      <td>2.847958</td>\n      <td>0.526893</td>\n      <td>3.777245</td>\n      <td>0.285535</td>\n      <td>5.922210</td>\n      <td>1.523714</td>\n      <td>0.783367</td>\n      <td>2.615942</td>\n      <td>7.965198</td>\n      <td>2.159891</td>\n      <td>2.587830</td>\n      <td>4.322325</td>\n      <td>0.541614</td>\n      <td>5.179559</td>\n      <td>3.119978</td>\n      <td>2.249730</td>\n      <td>4.278903</td>\n      <td>...</td>\n      <td>10.880263</td>\n      <td>0.217938</td>\n      <td>1.419612</td>\n      <td>5.262056</td>\n      <td>5.457784</td>\n      <td>5.024182</td>\n      <td>0.369684</td>\n      <td>7.798020</td>\n      <td>3.105986</td>\n      <td>0.369437</td>\n      <td>4.424621</td>\n      <td>5.378008</td>\n      <td>8.674171</td>\n      <td>5.966674</td>\n      <td>7.136427</td>\n      <td>2.892167</td>\n      <td>7.513939</td>\n      <td>2.628895</td>\n      <td>8.579810</td>\n      <td>2.798956</td>\n      <td>5.261243</td>\n      <td>1.371862</td>\n      <td>8.963434</td>\n      <td>4.474924</td>\n      <td>9.318280</td>\n      <td>4.725167</td>\n      <td>3.189759</td>\n      <td>11.574708</td>\n      <td>3.944604</td>\n      <td>0.976348</td>\n      <td>4.559922</td>\n      <td>3.023272</td>\n      <td>1.478423</td>\n      <td>3.992030</td>\n      <td>3.135162</td>\n      <td>1.429372</td>\n      <td>5.454369</td>\n      <td>0.921625</td>\n      <td>3.010945</td>\n      <td>10.438015</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.408400</td>\n      <td>-15.043400</td>\n      <td>2.117100</td>\n      <td>-0.040200</td>\n      <td>5.074800</td>\n      <td>-32.562600</td>\n      <td>2.347300</td>\n      <td>5.349700</td>\n      <td>-10.505500</td>\n      <td>3.970500</td>\n      <td>-20.731300</td>\n      <td>-26.095000</td>\n      <td>13.434600</td>\n      <td>-6.011100</td>\n      <td>1.013300</td>\n      <td>13.076900</td>\n      <td>0.635100</td>\n      <td>-33.380200</td>\n      <td>-10.664200</td>\n      <td>-12.402500</td>\n      <td>-5.432200</td>\n      <td>-10.089000</td>\n      <td>-5.322500</td>\n      <td>1.209800</td>\n      <td>-0.678400</td>\n      <td>12.720000</td>\n      <td>-24.243100</td>\n      <td>-6.166800</td>\n      <td>2.089600</td>\n      <td>-4.787200</td>\n      <td>-34.798400</td>\n      <td>2.140600</td>\n      <td>-8.986100</td>\n      <td>1.508500</td>\n      <td>9.816900</td>\n      <td>-16.513600</td>\n      <td>-8.095100</td>\n      <td>-1.183400</td>\n      <td>-6.337100</td>\n      <td>...</td>\n      <td>-7.452200</td>\n      <td>4.852600</td>\n      <td>0.623100</td>\n      <td>-6.531700</td>\n      <td>-19.997700</td>\n      <td>3.816700</td>\n      <td>1.851200</td>\n      <td>-35.969500</td>\n      <td>-5.250200</td>\n      <td>4.258800</td>\n      <td>-14.506000</td>\n      <td>-22.479300</td>\n      <td>-11.453300</td>\n      <td>-22.748700</td>\n      <td>-2.995300</td>\n      <td>3.241500</td>\n      <td>-29.116500</td>\n      <td>4.952100</td>\n      <td>-29.273400</td>\n      <td>-7.856100</td>\n      <td>-22.037400</td>\n      <td>5.416500</td>\n      <td>-26.001100</td>\n      <td>-4.808200</td>\n      <td>-18.489700</td>\n      <td>-22.583300</td>\n      <td>-3.022300</td>\n      <td>-47.753600</td>\n      <td>4.412300</td>\n      <td>-2.554300</td>\n      <td>-14.093300</td>\n      <td>-2.691700</td>\n      <td>-3.814500</td>\n      <td>-11.783400</td>\n      <td>8.694400</td>\n      <td>-5.261000</td>\n      <td>-14.209600</td>\n      <td>5.960600</td>\n      <td>6.299300</td>\n      <td>-38.852800</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>8.453850</td>\n      <td>-4.740025</td>\n      <td>8.722475</td>\n      <td>5.254075</td>\n      <td>9.883175</td>\n      <td>-11.200350</td>\n      <td>4.767700</td>\n      <td>13.943800</td>\n      <td>-2.317800</td>\n      <td>6.618800</td>\n      <td>-3.594950</td>\n      <td>-7.510600</td>\n      <td>13.894000</td>\n      <td>5.072800</td>\n      <td>5.781875</td>\n      <td>14.262800</td>\n      <td>7.452275</td>\n      <td>-10.476225</td>\n      <td>9.177950</td>\n      <td>6.276475</td>\n      <td>8.627800</td>\n      <td>11.551000</td>\n      <td>2.182400</td>\n      <td>2.634100</td>\n      <td>7.613000</td>\n      <td>13.456400</td>\n      <td>-8.321725</td>\n      <td>-2.307900</td>\n      <td>4.992100</td>\n      <td>3.171700</td>\n      <td>-13.766175</td>\n      <td>8.870000</td>\n      <td>-2.500875</td>\n      <td>11.456300</td>\n      <td>11.032300</td>\n      <td>0.116975</td>\n      <td>-0.007125</td>\n      <td>4.125475</td>\n      <td>7.591050</td>\n      <td>...</td>\n      <td>15.696125</td>\n      <td>5.470500</td>\n      <td>4.326100</td>\n      <td>7.029600</td>\n      <td>-7.094025</td>\n      <td>15.744550</td>\n      <td>2.699000</td>\n      <td>-9.643100</td>\n      <td>2.703200</td>\n      <td>5.374600</td>\n      <td>-3.258500</td>\n      <td>-4.720350</td>\n      <td>13.731775</td>\n      <td>-5.009525</td>\n      <td>15.064600</td>\n      <td>9.371600</td>\n      <td>-8.386500</td>\n      <td>9.808675</td>\n      <td>-7.395700</td>\n      <td>0.625575</td>\n      <td>-6.673900</td>\n      <td>9.084700</td>\n      <td>-6.064425</td>\n      <td>5.423100</td>\n      <td>5.663300</td>\n      <td>-7.360000</td>\n      <td>6.715200</td>\n      <td>-19.205125</td>\n      <td>12.501550</td>\n      <td>0.014900</td>\n      <td>-0.058825</td>\n      <td>5.157400</td>\n      <td>0.889775</td>\n      <td>0.584600</td>\n      <td>15.629800</td>\n      <td>-1.170700</td>\n      <td>-1.946925</td>\n      <td>8.252800</td>\n      <td>13.829700</td>\n      <td>-11.208475</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>10.524750</td>\n      <td>-1.608050</td>\n      <td>10.580000</td>\n      <td>6.825000</td>\n      <td>11.108250</td>\n      <td>-4.833150</td>\n      <td>5.385100</td>\n      <td>16.456800</td>\n      <td>0.393700</td>\n      <td>7.629600</td>\n      <td>0.487300</td>\n      <td>-3.286950</td>\n      <td>14.025500</td>\n      <td>8.604250</td>\n      <td>7.520300</td>\n      <td>14.574100</td>\n      <td>9.232050</td>\n      <td>-5.666350</td>\n      <td>15.196250</td>\n      <td>12.453900</td>\n      <td>13.196800</td>\n      <td>17.234250</td>\n      <td>4.275150</td>\n      <td>3.008650</td>\n      <td>10.380350</td>\n      <td>13.662500</td>\n      <td>-4.196900</td>\n      <td>-1.132100</td>\n      <td>5.534850</td>\n      <td>4.950200</td>\n      <td>-7.411750</td>\n      <td>10.365650</td>\n      <td>-0.497650</td>\n      <td>14.576000</td>\n      <td>11.435200</td>\n      <td>3.917750</td>\n      <td>2.198000</td>\n      <td>5.900650</td>\n      <td>10.562700</td>\n      <td>...</td>\n      <td>23.864500</td>\n      <td>5.633500</td>\n      <td>5.359700</td>\n      <td>10.788700</td>\n      <td>-2.637800</td>\n      <td>19.270800</td>\n      <td>2.960200</td>\n      <td>-4.011600</td>\n      <td>4.761600</td>\n      <td>5.634300</td>\n      <td>0.002800</td>\n      <td>-0.807350</td>\n      <td>19.748000</td>\n      <td>-0.569750</td>\n      <td>20.206100</td>\n      <td>11.679800</td>\n      <td>-2.538450</td>\n      <td>11.737250</td>\n      <td>-0.942050</td>\n      <td>2.512300</td>\n      <td>-2.688800</td>\n      <td>10.036050</td>\n      <td>0.720200</td>\n      <td>8.600000</td>\n      <td>12.521000</td>\n      <td>-3.946950</td>\n      <td>8.902150</td>\n      <td>-10.209750</td>\n      <td>15.239450</td>\n      <td>0.742600</td>\n      <td>3.203600</td>\n      <td>7.347750</td>\n      <td>1.901300</td>\n      <td>3.396350</td>\n      <td>17.957950</td>\n      <td>-0.172700</td>\n      <td>2.408900</td>\n      <td>8.888200</td>\n      <td>15.934050</td>\n      <td>-2.819550</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>12.758200</td>\n      <td>1.358625</td>\n      <td>12.516700</td>\n      <td>8.324100</td>\n      <td>12.261125</td>\n      <td>0.924800</td>\n      <td>6.003000</td>\n      <td>19.102900</td>\n      <td>2.937900</td>\n      <td>8.584425</td>\n      <td>4.382925</td>\n      <td>0.852825</td>\n      <td>14.164200</td>\n      <td>12.274775</td>\n      <td>9.270425</td>\n      <td>14.874500</td>\n      <td>11.055900</td>\n      <td>-0.810775</td>\n      <td>21.013325</td>\n      <td>18.433300</td>\n      <td>17.879400</td>\n      <td>23.089050</td>\n      <td>6.293200</td>\n      <td>3.403800</td>\n      <td>13.479600</td>\n      <td>13.863700</td>\n      <td>-0.090200</td>\n      <td>0.015625</td>\n      <td>6.093700</td>\n      <td>6.798925</td>\n      <td>-1.443450</td>\n      <td>11.885000</td>\n      <td>1.469100</td>\n      <td>18.097125</td>\n      <td>11.844400</td>\n      <td>7.487725</td>\n      <td>4.460400</td>\n      <td>7.542400</td>\n      <td>13.598925</td>\n      <td>...</td>\n      <td>32.622850</td>\n      <td>5.792000</td>\n      <td>6.371200</td>\n      <td>14.623900</td>\n      <td>1.323600</td>\n      <td>23.024025</td>\n      <td>3.241500</td>\n      <td>1.318725</td>\n      <td>7.020025</td>\n      <td>5.905400</td>\n      <td>3.096400</td>\n      <td>2.956800</td>\n      <td>25.907725</td>\n      <td>3.619900</td>\n      <td>25.641225</td>\n      <td>13.745500</td>\n      <td>2.704400</td>\n      <td>13.931300</td>\n      <td>5.338750</td>\n      <td>4.391125</td>\n      <td>0.996200</td>\n      <td>11.011300</td>\n      <td>7.499175</td>\n      <td>12.127425</td>\n      <td>19.456150</td>\n      <td>-0.590650</td>\n      <td>11.193800</td>\n      <td>-1.466000</td>\n      <td>18.345225</td>\n      <td>1.482900</td>\n      <td>6.406200</td>\n      <td>9.512525</td>\n      <td>2.949500</td>\n      <td>6.205800</td>\n      <td>20.396525</td>\n      <td>0.829600</td>\n      <td>6.556725</td>\n      <td>9.593300</td>\n      <td>18.064725</td>\n      <td>4.836800</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>20.315000</td>\n      <td>10.376800</td>\n      <td>19.353000</td>\n      <td>13.188300</td>\n      <td>16.671400</td>\n      <td>17.251600</td>\n      <td>8.447700</td>\n      <td>27.691800</td>\n      <td>10.151300</td>\n      <td>11.150600</td>\n      <td>18.670200</td>\n      <td>17.188700</td>\n      <td>14.654500</td>\n      <td>22.331500</td>\n      <td>14.937700</td>\n      <td>15.863300</td>\n      <td>17.950600</td>\n      <td>19.025900</td>\n      <td>41.748000</td>\n      <td>35.183000</td>\n      <td>31.285900</td>\n      <td>49.044300</td>\n      <td>14.594500</td>\n      <td>4.875200</td>\n      <td>25.446000</td>\n      <td>14.654600</td>\n      <td>15.675100</td>\n      <td>3.243100</td>\n      <td>8.787400</td>\n      <td>13.143100</td>\n      <td>15.651500</td>\n      <td>20.171900</td>\n      <td>6.787100</td>\n      <td>29.546600</td>\n      <td>13.287800</td>\n      <td>21.528900</td>\n      <td>14.245600</td>\n      <td>11.863800</td>\n      <td>29.823500</td>\n      <td>...</td>\n      <td>58.394200</td>\n      <td>6.309900</td>\n      <td>10.134400</td>\n      <td>27.564800</td>\n      <td>12.119300</td>\n      <td>38.332200</td>\n      <td>4.220400</td>\n      <td>21.276600</td>\n      <td>14.886100</td>\n      <td>7.089000</td>\n      <td>16.731900</td>\n      <td>17.917300</td>\n      <td>53.591900</td>\n      <td>18.855400</td>\n      <td>43.546800</td>\n      <td>20.854800</td>\n      <td>20.245200</td>\n      <td>20.596500</td>\n      <td>29.841300</td>\n      <td>13.448700</td>\n      <td>12.750500</td>\n      <td>14.393900</td>\n      <td>29.248700</td>\n      <td>23.704900</td>\n      <td>44.363400</td>\n      <td>12.997500</td>\n      <td>21.739200</td>\n      <td>22.786100</td>\n      <td>29.330300</td>\n      <td>4.034100</td>\n      <td>18.440900</td>\n      <td>16.716500</td>\n      <td>8.402400</td>\n      <td>18.281800</td>\n      <td>27.928800</td>\n      <td>4.272900</td>\n      <td>18.321500</td>\n      <td>12.000400</td>\n      <td>26.079100</td>\n      <td>28.500700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seaborn countplot gives the number of data in the each class\nsns.countplot(x=\"target\", data=df_train)","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff525c8e0b8>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n2.Logistic Regression Classification\n\nIt is very powerfull algorithm to use with binary classification.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y has target data (clases) such as 1 and 0. \ny_train_data = df_train.target.values.reshape(-1,1)\n# This means that take target and ID_code data out from the datasets and assign them to variable\nx_train_data = df_train.drop([\"target\",\"ID_code\"],axis=1)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalization is used to handle with unbalanced features\n#This gives the values to the features which range from zero to 1.\nx = (x_train_data - np.min(x_train_data))/(np.max(x_train_data)-np.min(x_train_data)).values","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preperation of testing data\nx_test_data = df_test.drop([\"ID_code\"],axis=1)\n#x_test_data.head()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Logistic Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train_data,y_train_data)\n\ny_lr_test_data = lr.predict(x_test_data)","execution_count":27,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_code_data = df_test.ID_code.values\nfrom numpy import array\nfrom numpy import vstack\nheader=[['ID_code','target']]\nlr_array = vstack((ID_code_data, y_lr_test_data)).T\nframe_lr = pd.DataFrame(lr_array, columns=header)\nprint (frame_lr)","execution_count":28,"outputs":[{"output_type":"stream","text":"            ID_code target\n0            test_0      0\n1            test_1      0\n2            test_2      0\n3            test_3      0\n4            test_4      0\n5            test_5      0\n6            test_6      0\n7            test_7      0\n8            test_8      0\n9            test_9      0\n10          test_10      0\n11          test_11      0\n12          test_12      0\n13          test_13      0\n14          test_14      0\n15          test_15      0\n16          test_16      0\n17          test_17      0\n18          test_18      0\n19          test_19      0\n20          test_20      0\n21          test_21      0\n22          test_22      0\n23          test_23      0\n24          test_24      0\n25          test_25      0\n26          test_26      0\n27          test_27      0\n28          test_28      0\n29          test_29      0\n...             ...    ...\n199970  test_199970      0\n199971  test_199971      0\n199972  test_199972      0\n199973  test_199973      0\n199974  test_199974      0\n199975  test_199975      0\n199976  test_199976      0\n199977  test_199977      0\n199978  test_199978      0\n199979  test_199979      0\n199980  test_199980      0\n199981  test_199981      0\n199982  test_199982      0\n199983  test_199983      0\n199984  test_199984      0\n199985  test_199985      0\n199986  test_199986      0\n199987  test_199987      0\n199988  test_199988      0\n199989  test_199989      0\n199990  test_199990      0\n199991  test_199991      0\n199992  test_199992      0\n199993  test_199993      0\n199994  test_199994      0\n199995  test_199995      0\n199996  test_199996      0\n199997  test_199997      0\n199998  test_199998      0\n199999  test_199999      0\n\n[200000 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n3.Decision Tree Classification\n\n\"Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed\".\n\nAccording to ınformation entropy, we can determine which feature is the most important. And we put the most important one to the top of the related tree.\n\nDecision tree classification can be used for both binary and multi classes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Decision Tree Classification Model\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)\ndt.fit(x_train_data,y_train_data)\ny_dt_test_data = dt.predict(x_test_data)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"header=[['ID_code','target']]\ndt_array = vstack((ID_code_data, y_dt_test_data)).T\nframe_dt = pd.DataFrame(dt_array, columns=header)\nprint (frame_dt)","execution_count":30,"outputs":[{"output_type":"stream","text":"            ID_code target\n0            test_0      1\n1            test_1      0\n2            test_2      0\n3            test_3      1\n4            test_4      0\n5            test_5      0\n6            test_6      0\n7            test_7      0\n8            test_8      0\n9            test_9      0\n10          test_10      0\n11          test_11      0\n12          test_12      0\n13          test_13      1\n14          test_14      0\n15          test_15      0\n16          test_16      1\n17          test_17      0\n18          test_18      0\n19          test_19      0\n20          test_20      0\n21          test_21      0\n22          test_22      0\n23          test_23      0\n24          test_24      0\n25          test_25      0\n26          test_26      0\n27          test_27      1\n28          test_28      1\n29          test_29      0\n...             ...    ...\n199970  test_199970      0\n199971  test_199971      0\n199972  test_199972      0\n199973  test_199973      0\n199974  test_199974      0\n199975  test_199975      0\n199976  test_199976      0\n199977  test_199977      0\n199978  test_199978      0\n199979  test_199979      0\n199980  test_199980      0\n199981  test_199981      0\n199982  test_199982      0\n199983  test_199983      0\n199984  test_199984      0\n199985  test_199985      0\n199986  test_199986      0\n199987  test_199987      0\n199988  test_199988      0\n199989  test_199989      0\n199990  test_199990      0\n199991  test_199991      0\n199992  test_199992      0\n199993  test_199993      0\n199994  test_199994      0\n199995  test_199995      0\n199996  test_199996      0\n199997  test_199997      0\n199998  test_199998      0\n199999  test_199999      0\n\n[200000 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n4.Random Forest Classification\n\nThis methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. And we use this avarage to determine the class of the test point.\n\nThis is one of ensamble method which uses multiple classes to predict the target, and very powerfull technique.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Random Forest Classification Model\nfrom sklearn.ensemble import RandomForestClassifier\n# n_estimators = 100 means this model will use 100 subsets.\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42)\nrf.fit(x_train_data,y_train_data)\ny_rf_test_data = rf.predict(x_test_data)","execution_count":31,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  \"\"\"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"header=[['ID_code','target']]\nrf_array = vstack((ID_code_data, y_rf_test_data)).T\nframe_rf = pd.DataFrame(rf_array, columns=header)\nprint (frame_rf)","execution_count":32,"outputs":[{"output_type":"stream","text":"            ID_code target\n0            test_0      0\n1            test_1      0\n2            test_2      0\n3            test_3      0\n4            test_4      0\n5            test_5      0\n6            test_6      0\n7            test_7      0\n8            test_8      0\n9            test_9      0\n10          test_10      0\n11          test_11      0\n12          test_12      0\n13          test_13      0\n14          test_14      0\n15          test_15      0\n16          test_16      0\n17          test_17      0\n18          test_18      0\n19          test_19      0\n20          test_20      0\n21          test_21      0\n22          test_22      0\n23          test_23      0\n24          test_24      0\n25          test_25      0\n26          test_26      0\n27          test_27      0\n28          test_28      0\n29          test_29      0\n...             ...    ...\n199970  test_199970      0\n199971  test_199971      0\n199972  test_199972      0\n199973  test_199973      0\n199974  test_199974      0\n199975  test_199975      0\n199976  test_199976      0\n199977  test_199977      0\n199978  test_199978      0\n199979  test_199979      0\n199980  test_199980      0\n199981  test_199981      0\n199982  test_199982      0\n199983  test_199983      0\n199984  test_199984      0\n199985  test_199985      0\n199986  test_199986      0\n199987  test_199987      0\n199988  test_199988      0\n199989  test_199989      0\n199990  test_199990      0\n199991  test_199991      0\n199992  test_199992      0\n199993  test_199993      0\n199994  test_199994      0\n199995  test_199995      0\n199996  test_199996      0\n199997  test_199997      0\n199998  test_199998      0\n199999  test_199999      0\n\n[200000 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n5.Naive Bayes Classification\n\n\"Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\"\n\nHere we basically determine similarity range and calculate probabilty of the X point in the A feature P(A_feature|x)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Naive Bayes Classification Model\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train_data,y_train_data)\ny_nb_test_data = nb.predict(x_test_data)","execution_count":33,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"header=[['ID_code','target']]\nnb_array = vstack((ID_code_data, y_nb_test_data)).T\nframe_nb = pd.DataFrame(nb_array, columns=header)\nprint (frame_nb)","execution_count":34,"outputs":[{"output_type":"stream","text":"            ID_code target\n0            test_0      0\n1            test_1      0\n2            test_2      0\n3            test_3      0\n4            test_4      0\n5            test_5      0\n6            test_6      0\n7            test_7      0\n8            test_8      0\n9            test_9      0\n10          test_10      0\n11          test_11      0\n12          test_12      0\n13          test_13      0\n14          test_14      0\n15          test_15      0\n16          test_16      0\n17          test_17      0\n18          test_18      0\n19          test_19      0\n20          test_20      0\n21          test_21      0\n22          test_22      0\n23          test_23      0\n24          test_24      0\n25          test_25      0\n26          test_26      0\n27          test_27      0\n28          test_28      0\n29          test_29      0\n...             ...    ...\n199970  test_199970      0\n199971  test_199971      0\n199972  test_199972      0\n199973  test_199973      0\n199974  test_199974      0\n199975  test_199975      0\n199976  test_199976      0\n199977  test_199977      0\n199978  test_199978      0\n199979  test_199979      0\n199980  test_199980      0\n199981  test_199981      0\n199982  test_199982      0\n199983  test_199983      0\n199984  test_199984      0\n199985  test_199985      0\n199986  test_199986      1\n199987  test_199987      0\n199988  test_199988      0\n199989  test_199989      0\n199990  test_199990      0\n199991  test_199991      0\n199992  test_199992      0\n199993  test_199993      0\n199994  test_199994      0\n199995  test_199995      0\n199996  test_199996      0\n199997  test_199997      0\n199998  test_199998      0\n199999  test_199999      0\n\n[200000 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n6.Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# These pandas DataFrames have 2 columns as ID_code and the predicted values of y_test.\nLogistic_regression = frame_lr\nDecision_tree_classification = frame_dt\nRandom_forest_classification = frame_rf\nNaive_bayes_classification = frame_nb","execution_count":35,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}