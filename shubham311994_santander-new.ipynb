{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import norm, rankdata\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import preprocessing\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd437ec94e364f299e81730bab1e60098c0a6394"},"cell_type":"markdown","source":"## We read the data from the give dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4084ba35fac8aac67c52b693e756ef0f26c26ec4"},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac7433e1f3dbdbcf6656ede77b0f5c5352a5b4cc"},"cell_type":"code","source":"t_d = test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b9f0f175c1847d225a8fdcc640f56d69e83fd7d"},"cell_type":"code","source":"test_data.drop(columns=['ID_code'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47911fa463025994610379d3934e21994653fdab"},"cell_type":"markdown","source":"## We understood what exactly is there in the data.\n1. ### Number of independent variables (X) are 200\n2. ### There's ID column avaliable\n3. ### Dependent variable is 1(Y) i.e target\n4. ### Total Number of rows are 200000"},{"metadata":{"trusted":true,"_uuid":"06144a38e12d95cd251bbeea5ad14d64160beec4"},"cell_type":"code","source":"#data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6493c193843d30cdda352746266ba0a323f44c63"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee091e01b270da9d1a035fa7c71eb300f865b80"},"cell_type":"markdown","source":"### Here We divided the data into X and Y"},{"metadata":{"trusted":true,"_uuid":"10e85c5dfb3007c7277e5f422f0f6ae7426613c8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e38390a634fadee53bc7b7cb5d68c975c0c4d545"},"cell_type":"markdown","source":"### Checking the columns of independent variable it includes Id as well"},{"metadata":{"trusted":true,"_uuid":"150431680d73865c46f4981416ef9ee8376779bd"},"cell_type":"code","source":"#X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"270877a0b1be6fe1c5681cf2dfdae3cdf3e2568d"},"cell_type":"code","source":"#Y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4873df4a5dbc428d0acf2173e89677acd353fb0"},"cell_type":"markdown","source":"### Applying PCA to check the variance of each variable and deciding the number of components for the data."},{"metadata":{"trusted":true,"_uuid":"e592ecae982dca2ad4bcb0a4be57a71c0aa94ac7"},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=110)\n# X_dat_pca = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d92fead16492f43cc8542313ad7c019ab775643e"},"cell_type":"code","source":"#X_dat_pca[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc93cbd16e8d42c4dd66c678cc8ca7d7f0b92f46"},"cell_type":"code","source":"#test_data_pca=pca.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108582d0edcce070ec316bb8e07c18feb3dc4d3d"},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X_dat_pca,Y.values,test_size=0.10,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c79d72b08542169dc6d31690ec85b2502dac91f"},"cell_type":"code","source":"one_indices= np.array(data[data.target==1].index)\nzero_indices = np.array(data[data.target==0].index)\n#now let us a define a function for make undersample data with different proportion\n#different proportion means with different proportion of normal classes of data\ndef undersample(zero_indices,one_indices,times):#times denote the normal data = times*fraud data\n    Normal_indices_undersample = np.array(np.random.choice(zero_indices,(times*Count_insincere_transacation),replace=False))\n    print(len(Normal_indices_undersample))\n    undersample_data= np.concatenate([one_indices,Normal_indices_undersample])\n\n    undersample_data = data.iloc[undersample_data,:]\n    #print(undersample_data)\n    print(len(undersample_data))\n\n#     print(\"the normal transacation proportion is :\",len(undersample_data[undersample_data.target==0])/len(undersample_data))\n#     print(\"the fraud transacation proportion is :\",len(undersample_data[undersample_data.target==1])/len(undersample_data))\n    print(\"total number of record in resampled data is:\",len(undersample_data))\n    return(undersample_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6d671ea87aff3f369f3b356584995fee647a4eb"},"cell_type":"code","source":"Count_insincere_transacation = len(data[data[\"target\"]==1]) # fraud by 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b909c36ef84d4a22399487139b82e5c88c2606df"},"cell_type":"code","source":"Undersample_data = undersample(zero_indices,one_indices,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c77846e4485cd7f5fd2849a4c3dbb6038e58b2e"},"cell_type":"code","source":"df = Undersample_data.copy()\nY = df['target']\ndf.drop(columns=['target','ID_code'],axis=1,inplace=True)\nX = df.loc[:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05920a4d6c1f0c114b6a1404e1e61754841a7bab"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=110)\nX = preprocessing.normalize(X)\nX = preprocessing.scale(X)\n\nX_dat_pca = pca.fit_transform(X)\nX_dat_pca = preprocessing.normalize(X_dat_pca)\nX_dat_pca = preprocessing.scale(X_dat_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"675919e4a76c430beb236aff1a91b3edbc04b7cf"},"cell_type":"code","source":"Y=Y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74e775d029ec5dac41080fe31546785f4c8f1bd8"},"cell_type":"code","source":"np.isnan(np.min(X_dat_pca))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec42be51c80afded797fd49c64864e93677a5ef"},"cell_type":"code","source":"test_data = preprocessing.normalize(test_data)\ntest_data = preprocessing.scale(test_data)\n\ntest_data_pca=pca.transform(test_data)\ntest_data_pca = preprocessing.normalize(test_data_pca)\ntest_data_pca = preprocessing.scale(test_data_pca)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46ee9f3cc33edb6911826eebc7ec923bd52c7dd0"},"cell_type":"code","source":"X_dat_pca=X_dat_pca.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"725ff331bfc264f2655fdd5e782f4ebc004de3d3"},"cell_type":"code","source":"test_data_pca=test_data_pca.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52e39cecaf1b9e2d1c0ead2d3bcc6ad44fff3fa5"},"cell_type":"code","source":"NFOLDS = 30\nRANDOM_STATE = 120\n\n\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True,random_state=RANDOM_STATE)\noof_preds = np.zeros((len(X_dat_pca), 1))\ntest_preds = np.zeros((len(test_data_pca), 1))\nroc_cv =[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8417619bbd3ca2f5a9142967a249b8dd7932ad92"},"cell_type":"code","source":"type(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a80a6c7a861f1e1c1e40a17651f454927f3b17da"},"cell_type":"code","source":"for fold_, (trn_, val_) in enumerate(folds.split(Y, Y)):\n    print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = X_dat_pca[trn_, :], Y[trn_]\n    val_x, val_y = X_dat_pca[val_, :], Y[val_]\n    #print(trn_)\n    #print(trn_y)\n    \n    clf = Pipeline([\n        ('lr_clf', LogisticRegression(solver='lbfgs', max_iter=10000))\n    ])\n\n    clf.fit(trn_x, trn_y)\n\n    val_pred = clf.predict_proba(val_x)[:,1]\n    test_fold_pred = clf.predict_proba(test_data_pca)[:,1]\n    \n    roc_cv.append(roc_auc_score(val_y, val_pred))\n    \n    print(\"AUC = {}\".format(roc_auc_score(val_y, val_pred)))\n    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n    test_preds += test_fold_pred.reshape((-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68b26970a9d8fac934a90fcacfc5fc0b3da16766"},"cell_type":"code","source":"# param = {\n#     'max_depth': 20,  # the maximum depth of each tree\n#     'eta': 0.001,  # the training step for each iteration\n#     'silent': 1,\n#     'objective': 'binary:logistic'\n#      } \n# param['nthread'] = 4\n# # the number of classes that exist in this datset\n# num_round = 1000  # the number of training iterations\n# for fold_, (trn_, val_) in enumerate(folds.split(Y, Y)):\n#     print(\"Current Fold: {}\".format(fold_))\n#     trn_x, trn_y = X_dat_pca[trn_, :], Y[trn_]\n#     val_x, val_y = X_dat_pca[val_, :], Y[val_]\n#     #print(trn_)\n#     #print(trn_y)\n#     dtrain = xgb.DMatrix(trn_x, label=trn_y)\n#     dtest = xgb.DMatrix(val_x, label=val_y)\n#     dtest1=xgb.DMatrix(test_data_pca)\n    \n#     bst = xgb.train(param, dtrain, num_round)\n\n#     preds = bst.predict(dtest)\n#     #preds=int(preds)\n#     #test_fold_pred = bst.predict(dtest1)\n#     #best_preds = np.asarray([np.argmax(line) for line in preds])\n    \n#     roc_cv.append(roc_auc_score(val_y, preds))\n    \n#     print(\"AUC = {}\".format(roc_auc_score(val_y, preds)))\n#     #oof_preds[val_, :] = preds.reshape((-1, 1))\n#     #test_preds += test_fold_pred.reshape((-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e3d66658f32a62c2a8165827787cbe41f1be238"},"cell_type":"code","source":"test_preds=test_preds/NFOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8ee09aeab961ab93ae5b454260e9d6f499973f"},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# lgr = LogisticRegression(solver='lbfgs',max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"430adc7cd43a7f6871a2a9435a734878570a6866"},"cell_type":"code","source":"# lgr.fit(X_train,y_train)\n# lgr_training_predictions = lgr.predict(X_train)\n# print(lgr_training_predictions.shape)\n# print(y_train.shape)\n# from sklearn.metrics import accuracy_score\n# print(accuracy_score(y_train,lgr_training_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6904d0a76b1aec6efc348386eb072fa79125602"},"cell_type":"code","source":"# lgr_test_pred=lgr.predict(X_test)\n# print(accuracy_score(y_test,lgr_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae1eed506e553f4c53b26bd6e2ef009d16497e86"},"cell_type":"code","source":"# test_data_predictions = lgr.predict(test_data_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ef1cf4e15c2f44a8f2979bb48edd780a28de9e6"},"cell_type":"code","source":"# test_data_predictions[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad74cd0f4b39431a02f63cbed9c6163e0be7371b"},"cell_type":"code","source":"print(\"Saving submission file\")\nsample = pd.read_csv('../input/sample_submission.csv')\nsample.target = test_preds.astype(float)\nsample.ID_code = t_d[\"ID_code\"]\nsample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc69a157400d37468b58381b77ebc4b05f767b7"},"cell_type":"code","source":"# submission_lgr = pd.DataFrame({\n#         \"ID_code\": t_d[\"ID_code\"],\n#         \"target\": test_preds\n#     })\n# submission_lgr.to_csv('submission_ens.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89792d9ecc70d3cffec5c7962ab2c0b34e428ffd"},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=None)\n# pca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9db2523ff244c013b3e9fb10f686ed6e332d6a66"},"cell_type":"code","source":"# a = np.array(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe5989eb681fec78fb42f413c2e38d01e4f51ddd"},"cell_type":"code","source":"# su=0.0\n# for i,b in enumerate(a):\n#     su = su + b\n#     if(su<=0.95):\n#         print(i)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}