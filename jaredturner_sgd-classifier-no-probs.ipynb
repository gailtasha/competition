{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here is a simple SVM using Stochastic Gradient Descent with `loss = 'hinge'`. Probabilities were not used in the prediction and transactions were predicted with high accuracy at the expense of false positives."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"810d3bdbd6ab35d40f0812448338e8ca3341547f"},"cell_type":"code","source":"X = pd.read_csv(\"../input/train.csv\")\nprint('\\n shape of raw training:', X.shape)\n\nknown_ids = X['ID_code']\ny = X['target']\nX = X.drop(['target', 'ID_code'], axis=1).values\nprint('\\n shape of ids:', known_ids.shape)\nprint('\\n shape of labels:', y.shape)\nprint('\\n shape of training data:', X.shape)\nprint(\"\\n train data loaded!\")","execution_count":3,"outputs":[{"output_type":"stream","text":"\n shape of raw training: (200000, 202)\n\n shape of ids: (200000,)\n\n shape of labels: (200000,)\n\n shape of training data: (200000, 200)\n\n train data loaded!\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"1a3b68a716caede0366129001fb25209be5b74f9"},"cell_type":"code","source":"def my_cv(X, y, model, folds=5, rand_st=1):\n    scores = []\n    for i in range(1,folds+1):\n        print('\\n fold: ', i)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                            test_size=1/folds, \n                                                            random_state=i+rand_st)\n        # scaler = MinMaxScaler(feature_range=(-1,1))\n        scaler = StandardScaler()\n        scaler.fit(X_train)\n        X_train = scaler.transform(X_train)\n        X_test = scaler.transform(X_test)\n        model.fit(X_train, y_train)\n        model_pred = model.predict(X_test)\n        # model_pred_proba =model.predict_proba(X_test)\n        scores.append(roc_auc_score(model_pred,y_test))\n        if i == 1:\n            conf_mat = confusion_matrix(y_test, model_pred)\n        else:\n            conf_mat += confusion_matrix(y_test, model_pred)\n    print('\\n',scores,'\\n',conf_mat)\n    return scores, conf_mat  ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"908e125b22062048d4baa32a23bbd3b1778e6ff7"},"cell_type":"code","source":"model = SGDClassifier(loss='hinge',\n                      class_weight='balanced', \n                      penalty = 'l1',\n                      l1_ratio = 0.7,\n                      alpha = 5e-4, \n                      max_iter = 1000,\n                      early_stopping=True,\n                      tol = 1e-3,\n                      n_iter_no_change = 10,\n                      verbose=1)\nscores, conf_mat = my_cv(X, y, model, folds=5)\nprint('class 0 accuracy: ', conf_mat[0,0]/sum(conf_mat[0,]))\nprint('class 1 accuracy: ', conf_mat[1,1]/sum(conf_mat[1,]))","execution_count":5,"outputs":[{"output_type":"stream","text":"\n fold:  1\n-- Epoch 1\nNorm: 335.87, NNZs: 105, Bias: -1.805509, T: 144000, Avg. loss: 19.225213\nTotal training time: 0.29 seconds.\n-- Epoch 2\nNorm: 337.92, NNZs: 108, Bias: -1.503145, T: 288000, Avg. loss: 0.872773\nTotal training time: 0.71 seconds.\n-- Epoch 3\nNorm: 338.60, NNZs: 137, Bias: -1.186787, T: 432000, Avg. loss: 0.711855\nTotal training time: 1.14 seconds.\n-- Epoch 4\nNorm: 338.91, NNZs: 130, Bias: -1.067306, T: 576000, Avg. loss: 0.647966\nTotal training time: 1.57 seconds.\n-- Epoch 5\nNorm: 339.09, NNZs: 132, Bias: -1.089144, T: 720000, Avg. loss: 0.607797\nTotal training time: 2.01 seconds.\n-- Epoch 6\nNorm: 339.21, NNZs: 127, Bias: -0.918743, T: 864000, Avg. loss: 0.584571\nTotal training time: 2.45 seconds.\n-- Epoch 7\nNorm: 339.29, NNZs: 142, Bias: -1.015596, T: 1008000, Avg. loss: 0.562916\nTotal training time: 2.88 seconds.\n-- Epoch 8\nNorm: 339.35, NNZs: 148, Bias: -0.972852, T: 1152000, Avg. loss: 0.553626\nTotal training time: 3.31 seconds.\n-- Epoch 9\nNorm: 339.40, NNZs: 145, Bias: -0.856377, T: 1296000, Avg. loss: 0.540148\nTotal training time: 3.74 seconds.\n-- Epoch 10\nNorm: 339.44, NNZs: 143, Bias: -0.817818, T: 1440000, Avg. loss: 0.537184\nTotal training time: 4.16 seconds.\n-- Epoch 11\nNorm: 339.47, NNZs: 154, Bias: -0.908111, T: 1584000, Avg. loss: 0.525355\nTotal training time: 4.59 seconds.\n-- Epoch 12\nNorm: 339.49, NNZs: 151, Bias: -0.757845, T: 1728000, Avg. loss: 0.520688\nTotal training time: 5.02 seconds.\n-- Epoch 13\nNorm: 339.51, NNZs: 155, Bias: -0.871343, T: 1872000, Avg. loss: 0.521418\nTotal training time: 5.44 seconds.\n-- Epoch 14\nNorm: 339.53, NNZs: 147, Bias: -0.872954, T: 2016000, Avg. loss: 0.514669\nTotal training time: 5.86 seconds.\n-- Epoch 15\nNorm: 339.54, NNZs: 159, Bias: -0.745634, T: 2160000, Avg. loss: 0.509364\nTotal training time: 6.29 seconds.\nConvergence after 15 epochs took 6.35 seconds\n\n fold:  2\n-- Epoch 1\nNorm: 323.58, NNZs: 115, Bias: -2.058578, T: 144000, Avg. loss: 18.088455\nTotal training time: 0.30 seconds.\n-- Epoch 2\nNorm: 325.68, NNZs: 112, Bias: -1.809277, T: 288000, Avg. loss: 0.867833\nTotal training time: 0.72 seconds.\n-- Epoch 3\nNorm: 326.33, NNZs: 109, Bias: -1.263571, T: 432000, Avg. loss: 0.707179\nTotal training time: 1.16 seconds.\n-- Epoch 4\nNorm: 326.65, NNZs: 138, Bias: -1.135309, T: 576000, Avg. loss: 0.643522\nTotal training time: 1.60 seconds.\n-- Epoch 5\nNorm: 326.84, NNZs: 140, Bias: -1.138641, T: 720000, Avg. loss: 0.609645\nTotal training time: 2.04 seconds.\n-- Epoch 6\nNorm: 326.96, NNZs: 141, Bias: -1.068560, T: 864000, Avg. loss: 0.581624\nTotal training time: 2.47 seconds.\n-- Epoch 7\nNorm: 327.04, NNZs: 140, Bias: -1.063308, T: 1008000, Avg. loss: 0.564882\nTotal training time: 2.90 seconds.\n-- Epoch 8\nNorm: 327.11, NNZs: 139, Bias: -0.765674, T: 1152000, Avg. loss: 0.549225\nTotal training time: 3.34 seconds.\n-- Epoch 9\nNorm: 327.15, NNZs: 135, Bias: -0.972214, T: 1296000, Avg. loss: 0.543858\nTotal training time: 3.76 seconds.\n-- Epoch 10\nNorm: 327.19, NNZs: 143, Bias: -0.931239, T: 1440000, Avg. loss: 0.530221\nTotal training time: 4.19 seconds.\n-- Epoch 11\nNorm: 327.22, NNZs: 151, Bias: -0.897851, T: 1584000, Avg. loss: 0.528685\nTotal training time: 4.62 seconds.\n-- Epoch 12\nNorm: 327.24, NNZs: 157, Bias: -0.900493, T: 1728000, Avg. loss: 0.523357\nTotal training time: 5.04 seconds.\n-- Epoch 13\nNorm: 327.27, NNZs: 149, Bias: -0.795117, T: 1872000, Avg. loss: 0.516463\nTotal training time: 5.46 seconds.\nConvergence after 13 epochs took 5.53 seconds\n\n fold:  3\n-- Epoch 1\nNorm: 322.89, NNZs: 115, Bias: -1.887763, T: 144000, Avg. loss: 16.502642\nTotal training time: 0.31 seconds.\n-- Epoch 2\nNorm: 324.96, NNZs: 118, Bias: -1.438184, T: 288000, Avg. loss: 0.867133\nTotal training time: 0.74 seconds.\n-- Epoch 3\nNorm: 325.64, NNZs: 123, Bias: -1.280491, T: 432000, Avg. loss: 0.703866\nTotal training time: 1.18 seconds.\n-- Epoch 4\nNorm: 325.96, NNZs: 130, Bias: -1.069348, T: 576000, Avg. loss: 0.643879\nTotal training time: 1.61 seconds.\n-- Epoch 5\nNorm: 326.15, NNZs: 125, Bias: -1.335531, T: 720000, Avg. loss: 0.608941\nTotal training time: 2.04 seconds.\n-- Epoch 6\nNorm: 326.27, NNZs: 139, Bias: -0.968671, T: 864000, Avg. loss: 0.583099\nTotal training time: 2.48 seconds.\n-- Epoch 7\nNorm: 326.36, NNZs: 146, Bias: -0.898401, T: 1008000, Avg. loss: 0.566050\nTotal training time: 2.91 seconds.\n-- Epoch 8\nNorm: 326.42, NNZs: 144, Bias: -0.906873, T: 1152000, Avg. loss: 0.550635\nTotal training time: 3.34 seconds.\n-- Epoch 9\nNorm: 326.47, NNZs: 143, Bias: -0.891683, T: 1296000, Avg. loss: 0.543816\nTotal training time: 3.77 seconds.\n-- Epoch 10\nNorm: 326.50, NNZs: 151, Bias: -0.828696, T: 1440000, Avg. loss: 0.532286\nTotal training time: 4.20 seconds.\n-- Epoch 11\nNorm: 326.53, NNZs: 155, Bias: -0.890635, T: 1584000, Avg. loss: 0.527785\nTotal training time: 4.62 seconds.\n-- Epoch 12\nNorm: 326.56, NNZs: 155, Bias: -0.877077, T: 1728000, Avg. loss: 0.520935\nTotal training time: 5.05 seconds.\n-- Epoch 13\nNorm: 326.58, NNZs: 156, Bias: -0.902827, T: 1872000, Avg. loss: 0.517197\nTotal training time: 5.47 seconds.\nConvergence after 13 epochs took 5.54 seconds\n\n fold:  4\n-- Epoch 1\nNorm: 317.63, NNZs: 104, Bias: -1.853442, T: 144000, Avg. loss: 12.981833\nTotal training time: 0.32 seconds.\n-- Epoch 2\nNorm: 319.78, NNZs: 116, Bias: -1.503787, T: 288000, Avg. loss: 0.870225\nTotal training time: 0.75 seconds.\n-- Epoch 3\nNorm: 320.48, NNZs: 120, Bias: -1.213933, T: 432000, Avg. loss: 0.705933\nTotal training time: 1.20 seconds.\n-- Epoch 4\nNorm: 320.81, NNZs: 136, Bias: -1.067446, T: 576000, Avg. loss: 0.640851\nTotal training time: 1.63 seconds.\n-- Epoch 5\nNorm: 321.00, NNZs: 130, Bias: -1.028806, T: 720000, Avg. loss: 0.606087\nTotal training time: 2.07 seconds.\n-- Epoch 6\nNorm: 321.12, NNZs: 139, Bias: -1.016203, T: 864000, Avg. loss: 0.585347\nTotal training time: 2.50 seconds.\n-- Epoch 7\nNorm: 321.21, NNZs: 137, Bias: -0.972666, T: 1008000, Avg. loss: 0.563376\nTotal training time: 2.94 seconds.\n-- Epoch 8\nNorm: 321.28, NNZs: 140, Bias: -0.713116, T: 1152000, Avg. loss: 0.547038\nTotal training time: 3.36 seconds.\n-- Epoch 9\nNorm: 321.32, NNZs: 151, Bias: -0.867074, T: 1296000, Avg. loss: 0.545449\nTotal training time: 3.80 seconds.\n-- Epoch 10\nNorm: 321.36, NNZs: 147, Bias: -0.820655, T: 1440000, Avg. loss: 0.535822\nTotal training time: 4.23 seconds.\n-- Epoch 11\nNorm: 321.39, NNZs: 149, Bias: -0.869026, T: 1584000, Avg. loss: 0.529618\nTotal training time: 4.65 seconds.\n-- Epoch 12\nNorm: 321.42, NNZs: 156, Bias: -0.844282, T: 1728000, Avg. loss: 0.523087\nTotal training time: 5.08 seconds.\n-- Epoch 13\nNorm: 321.44, NNZs: 161, Bias: -0.867836, T: 1872000, Avg. loss: 0.517347\nTotal training time: 5.50 seconds.\nConvergence after 13 epochs took 5.57 seconds\n\n fold:  5\n-- Epoch 1\nNorm: 324.12, NNZs: 103, Bias: -2.459540, T: 144000, Avg. loss: 15.332154\nTotal training time: 0.31 seconds.\n-- Epoch 2\nNorm: 326.27, NNZs: 110, Bias: -1.295015, T: 288000, Avg. loss: 0.857015\nTotal training time: 0.74 seconds.\n-- Epoch 3\nNorm: 326.94, NNZs: 122, Bias: -1.340763, T: 432000, Avg. loss: 0.701475\nTotal training time: 1.17 seconds.\n-- Epoch 4\nNorm: 327.26, NNZs: 133, Bias: -1.249310, T: 576000, Avg. loss: 0.648199\nTotal training time: 1.61 seconds.\n-- Epoch 5\nNorm: 327.45, NNZs: 127, Bias: -1.085507, T: 720000, Avg. loss: 0.601742\nTotal training time: 2.04 seconds.\n-- Epoch 6\nNorm: 327.57, NNZs: 142, Bias: -1.107905, T: 864000, Avg. loss: 0.588032\nTotal training time: 2.47 seconds.\n-- Epoch 7\nNorm: 327.65, NNZs: 138, Bias: -0.971357, T: 1008000, Avg. loss: 0.568398\nTotal training time: 2.90 seconds.\n-- Epoch 8\nNorm: 327.71, NNZs: 148, Bias: -0.883890, T: 1152000, Avg. loss: 0.555765\nTotal training time: 3.33 seconds.\n-- Epoch 9\nNorm: 327.76, NNZs: 142, Bias: -0.958824, T: 1296000, Avg. loss: 0.544843\nTotal training time: 3.75 seconds.\n-- Epoch 10\nNorm: 327.79, NNZs: 154, Bias: -0.876916, T: 1440000, Avg. loss: 0.531856\nTotal training time: 4.18 seconds.\n-- Epoch 11\nNorm: 327.82, NNZs: 156, Bias: -0.913075, T: 1584000, Avg. loss: 0.528205\nTotal training time: 4.61 seconds.\nConvergence after 11 epochs took 4.67 seconds\n\n [0.6125586358103912, 0.6092244591528602, 0.6138450976905905, 0.6137513602181197, 0.6111098781785751] \n [[135022  44839]\n [  4618  15521]]\nclass 0 accuracy:  0.7507019309355558\nclass 1 accuracy:  0.7706936789314266\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"15b1ddb433beebdbc6669e820e1e290a89d6b613"},"cell_type":"code","source":"print(scores)\nprint('\\n', sum(scores)/len(scores))\nprint('\\n', conf_mat)","execution_count":6,"outputs":[{"output_type":"stream","text":"[0.6125586358103912, 0.6092244591528602, 0.6138450976905905, 0.6137513602181197, 0.6111098781785751]\n\n 0.6120978862101073\n\n [[135022  44839]\n [  4618  15521]]\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"37662a75ac9b76493aba9c7b85328297683b1ed1"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\nsubmit = test[['ID_code']]\nX_test = test.drop(columns=['ID_code'])\n\nscaler = MinMaxScaler(feature_range=(-1,1))\nscaler.fit(X)\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X,y)\npreds = model.predict(X_test)\n\nsubmit['target'] = preds\nprint(submit.head)","execution_count":7,"outputs":[{"output_type":"stream","text":"-- Epoch 1\nNorm: 178.30, NNZs: 150, Bias: -2.462789, T: 180000, Avg. loss: 1.690845\nTotal training time: 0.41 seconds.\n-- Epoch 2\nNorm: 178.67, NNZs: 158, Bias: -2.334855, T: 360000, Avg. loss: 0.500543\nTotal training time: 0.93 seconds.\n-- Epoch 3\nNorm: 178.83, NNZs: 165, Bias: -2.314971, T: 540000, Avg. loss: 0.482378\nTotal training time: 1.45 seconds.\n-- Epoch 4\nNorm: 178.93, NNZs: 169, Bias: -2.311544, T: 720000, Avg. loss: 0.475773\nTotal training time: 1.96 seconds.\n-- Epoch 5\nNorm: 179.00, NNZs: 175, Bias: -2.182256, T: 900000, Avg. loss: 0.469999\nTotal training time: 2.48 seconds.\n-- Epoch 6\nNorm: 179.05, NNZs: 170, Bias: -2.275657, T: 1080000, Avg. loss: 0.468195\nTotal training time: 2.96 seconds.\n-- Epoch 7\nNorm: 179.10, NNZs: 171, Bias: -2.197871, T: 1260000, Avg. loss: 0.466247\nTotal training time: 3.46 seconds.\n-- Epoch 8\nNorm: 179.14, NNZs: 173, Bias: -2.237754, T: 1440000, Avg. loss: 0.468074\nTotal training time: 3.96 seconds.\n-- Epoch 9\nNorm: 179.17, NNZs: 175, Bias: -2.139232, T: 1620000, Avg. loss: 0.464841\nTotal training time: 4.44 seconds.\n-- Epoch 10\nNorm: 179.20, NNZs: 175, Bias: -2.255863, T: 1800000, Avg. loss: 0.465013\nTotal training time: 4.94 seconds.\n-- Epoch 11\nNorm: 179.23, NNZs: 175, Bias: -2.184054, T: 1980000, Avg. loss: 0.463999\nTotal training time: 5.43 seconds.\nConvergence after 11 epochs took 5.51 seconds\n<bound method NDFrame.head of             ID_code  target\n0            test_0       1\n1            test_1       1\n2            test_2       0\n3            test_3       1\n4            test_4       0\n5            test_5       0\n6            test_6       0\n7            test_7       1\n8            test_8       0\n9            test_9       0\n10          test_10       1\n11          test_11       1\n12          test_12       1\n13          test_13       0\n14          test_14       0\n15          test_15       0\n16          test_16       1\n17          test_17       0\n18          test_18       1\n19          test_19       0\n20          test_20       1\n21          test_21       0\n22          test_22       0\n23          test_23       0\n24          test_24       1\n25          test_25       0\n26          test_26       0\n27          test_27       0\n28          test_28       1\n29          test_29       1\n...             ...     ...\n199970  test_199970       1\n199971  test_199971       1\n199972  test_199972       0\n199973  test_199973       0\n199974  test_199974       0\n199975  test_199975       0\n199976  test_199976       1\n199977  test_199977       0\n199978  test_199978       1\n199979  test_199979       1\n199980  test_199980       0\n199981  test_199981       0\n199982  test_199982       0\n199983  test_199983       0\n199984  test_199984       0\n199985  test_199985       0\n199986  test_199986       1\n199987  test_199987       0\n199988  test_199988       0\n199989  test_199989       0\n199990  test_199990       0\n199991  test_199991       0\n199992  test_199992       1\n199993  test_199993       0\n199994  test_199994       1\n199995  test_199995       0\n199996  test_199996       0\n199997  test_199997       0\n199998  test_199998       0\n199999  test_199999       1\n\n[200000 rows x 2 columns]>\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"286acec0fb534df59797b0ad83bb957eae8f8413"},"cell_type":"code","source":"submit.to_csv('sgdclassifier.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}