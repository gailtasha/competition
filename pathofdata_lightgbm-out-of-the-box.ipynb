{"cells":[{"metadata":{"_uuid":"302bf95bb4da79ce82c4f4ffbc46d72e5a70558f"},"cell_type":"markdown","source":"This notebook presents an out of the box LGBM implementation for this competition, then explores various parameter optimization schemes using Bayes optimizers. This kernel uses code found on the Kaggle Days Paris kernel (https://kaggle.com/lucamassaron/kaggle-days-paris-gbdt-workshop)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import ParameterGrid, train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_convergence, plot_objective, plot_evaluations\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DF = '../input/train.csv'\nTEST_DF = '../input/test.csv'\nSUB_DF = '../input/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cb1bc23ed8065833dee493c122a4adc48060d8f"},"cell_type":"code","source":"data = pd.read_csv(DF)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c4973eb64c78d498faaceb492a2a328ef5c8c1f"},"cell_type":"markdown","source":"Our data consists of 200 features. At this point we'll only scale the data."},{"metadata":{"trusted":true,"_uuid":"28ead29464df22204d111651020b525377297860"},"cell_type":"code","source":"X = data.drop(['ID_code', 'target'], axis=1).values\nscaler = MinMaxScaler(copy=False)\nscaler.fit_transform(X)\nY = data.target.values\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d13729d2aaf0973ccb1d18f43f6ec44f0c60cf"},"cell_type":"code","source":"import keras as k\nfrom keras.models import Sequential\nfrom keras.layers import *\n\ndef linear_clf(x_train_, y_train_, x_val_, y_val_):\n    model = Sequential()\n    model.add(Dense(1, activation='sigmoid', input_dim=200))\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    history = model.fit(x_train_, y_train_, epochs=10, validation_data=(x_val_, y_val_))\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"03d97b156ded18c7fa6c31bd91746a403ce0100f"},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True)\n\nfor i, (train_index, test_index) in enumerate(skf.split(X,Y)):\n    \n    # Create data for this fold\n    y_train, y_valid = Y[train_index], Y[test_index]\n    X_train, X_valid = X[train_index,:], X[test_index,:]\n        \n    print( \"\\nFold \", i)\n\n    linear_m = linear_clf(X_train, y_train,\n                         X_valid, y_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4821aade96fbf27aa6fc698fc286ffd108460b50"},"cell_type":"code","source":"test_df = pd.read_csv(TEST_DF)\ntest_df.head()\nX_test = test_df.drop(['ID_code'], axis=1)\nX_test = scaler.fit_transform(X_test.values)\npredictions = linear_m.predict(X_test)\n\nsubmission_df = pd.read_csv(SUB_DF)\nsubmission_df.target = predictions\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8888c36afec31911ef000a31bc8a5922e4136c44"},"cell_type":"markdown","source":"A nice parameter set found on public kernels. Good as a starting point."},{"metadata":{"trusted":true,"_uuid":"eaae57a492159935cc4001a008ff127aaefa9246"},"cell_type":"code","source":"# params = {\n#     'num_leaves': 6,\n#     'max_bin': 263,\n#     'min_data_in_leaf': 90,\n#     'learning_rate': 0.01,\n#     'min_sum_hessian_in_leaf': 0.000446,\n#     'bagging_fraction': 0.55,\n#     'bagging_freq': 5,\n#     'max_depth': 14,\n#     'save_binary': True,\n#     'seed': 31452,\n#     'drop_seed': 31452,\n#     'data_random_seed': 31452,\n#     'objective': 'binary',\n#     'boosting_type': 'gbdt',\n#     'verbose': 1,\n#     'metric': 'auc',\n#     'is_unbalance': True,\n#     'boost_from_average': False,\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"84fda0f8d46c2d6cbb04e94108c4f256cd081de0"},"cell_type":"code","source":"# skf = StratifiedKFold(n_splits=5, shuffle=True)\n\n# for i, (train_index, test_index) in enumerate(skf.split(X,Y)):\n    \n#     # Create data for this fold\n#     y_train, y_valid = Y[train_index], Y[test_index]\n#     X_train, X_valid = X[train_index,:], X[test_index,:]\n        \n#     print( \"\\nFold \", i)\n\n#     # Running models for this fold\n    \n#     # ->LightGBM\n#     lgb_gbm = lgb.train(params, \n#                           lgb.Dataset(X_train, label=y_train), \n#                           MAX_ROUNDS, \n#                           lgb.Dataset(X_valid, label=y_valid), \n#                           verbose_eval=False, \n#                           #feval= auc, \n#                           early_stopping_rounds=350)\n    \n#     print( \" Best iteration lgb = \", lgb_gbm.best_iteration)\n    \n#     # Storing and reporting results of the fold\n#     lgb_iter1 = np.append(lgb_iter1, lgb_gbm.best_iteration)\n#     pred = lgb_gbm.predict(X_valid, num_iteration=lgb_gbm.best_iteration)\n#     auc = roc_auc_score(y_valid, pred)\n#     print('lgb score: ', auc)\n#     lgb_ap1 = np.append(lgb_ap1, auc)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1623fce47ab5292cbee0ed7367257a12337d3f93"},"cell_type":"markdown","source":"Much more realistic. Now let's attempt to improve the parameters with Bayes Search"},{"metadata":{"trusted":true,"_uuid":"4a2118ade508390d77134bd5f43f8c6585cbdbf6"},"cell_type":"code","source":"# auc = make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True)\n# skf = StratifiedKFold(n_splits=2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2665d9628d82705dbc9c1849411ccd74c06cd1a9"},"cell_type":"code","source":"# counter = 0\n# def onstep(res):\n#     global counter\n#     x0 = res.x_iters   # List of input points\n#     y0 = res.func_vals # Evaluation of input points\n# #     print('Last eval: ', x0[-1], \n# #           ' - Score ', y0[-1])\n#     print('Current iter: ', counter, \n#           ' - Score ', res.fun, \n#           ' - Args: ', res.x)\n#     counter += 1\n\n# dimensions = [Real(0.01, 1.0, name=\"learning_rate\"),\n#               Integer(2, 20, name=\"num_leaves\"),\n#               Integer(30, 300, name=\"Max_bin\"),\n#               Real(0.000001, 1.0, name=\"min_sum_hessian_in_leaf\"),\n#               Integer(10, 1500, name=\"n_estimators\"),\n#               Real(0.5, 1.0, name=\"subsample\"),\n#               Integer(2, 100, name=\"min_data_in_leaf\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"076714586af5d50631fa216828b2eb45507a690c"},"cell_type":"code","source":"# @use_named_args(dimensions=dimensions) \n# def objective(**params):\n#     model = lgb.LGBMClassifier(boosting_type='gbdt',\n#                                objective='binary',\n#         #                        num_rounds = 15000,\n# #                                num_leaves= 6,\n# #                                max_bin= 63,\n# #                                min_data_in_leaf= 90,\n# #                                learning_rate= 0.01,\n# #                                min_sum_hessian_in_leaf= 0.000446,\n# #                                bagging_fraction= 0.55,\n#                                max_depth= 14,\n#                                save_binary= True,\n#                                seed= 31452,\n#                                drop_seed= 31452,\n#                                data_random_seed= 31452,\n#                                metric= 'auc',\n#                                is_unbalance= True,\n#                                boost_from_average= False,\n#                                n_jobs=-1,\n#                                verbose=0)\n        \n#     model.set_params(**params)\n#     return -np.mean(cross_val_score(model, \n#                                     X, Y, \n#                                     cv=skf, \n#                                     n_jobs=-1,\n#                                     scoring=auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"515640ec7d6acebd74903a0afeaf085f383ad72d"},"cell_type":"code","source":"# gp_round = gp_minimize(func=objective,\n#                        dimensions=dimensions,\n#                        acq_func='gp_hedge',\n#                        n_calls=100,\n#                        callback=[onstep])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54be4968edef8c93aaf5a0ab905250ceb769f34f"},"cell_type":"code","source":"# plot_convergence(gp_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041c4b10c3e861998dcdde5af01e99497eb1bd0b"},"cell_type":"code","source":"# Stacker class from https://www.kaggle.com/yekenot/simple-stacker-lb-0-284\n\n# class Ensemble(object):\n#     def __init__(self, n_splits, stacker, base_models):\n#         self.n_splits = n_splits\n#         self.stacker = stacker\n#         self.base_models = base_models\n\n#     def fit_predict(self, X, y, T):\n#         X = np.array(X)\n#         y = np.array(y)\n#         T = np.array(T)\n\n#         folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True).split(X, y))\n\n#         S_train = np.zeros((X.shape[0], len(self.base_models)))\n#         S_test = np.zeros((T.shape[0], len(self.base_models)))\n#         for i, clf in enumerate(self.base_models):\n\n#             S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n#             for j, (train_idx, test_idx) in enumerate(folds):\n#                 X_train = X[train_idx]\n#                 y_train = y[train_idx]\n#                 X_holdout = X[test_idx]\n#                 y_holdout = y[test_idx]\n\n#                 print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n#                 clf.fit(X_train, y_train)\n# #                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n# #                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n#                 y_pred = clf.predict_proba(X_holdout)[:,1]                \n\n#                 S_train[test_idx, i] = y_pred\n#                 S_test_i[:, j] = clf.predict_proba(T)[:,1]\n#             S_test[:, i] = S_test_i.mean(axis=1)\n\n#         results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n#         print(\"Stacker score: %.5f\" % (results.mean()))\n\n#         self.stacker.fit(S_train, y)\n#         res = self.stacker.predict_proba(S_test)[:,1]\n#         return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98ed29e337a3924dba22a80701b13ea3768fe889"},"cell_type":"code","source":"# lgb_params1 = {}\n# lgb_params1['boosting_type']= 'gbdt'\n# lgb_params1['objective'] = 'binary'\n# lgb_params1['bagging_fraction']= 0.55\n# lgb_params1['max_depth'] = 14\n# lgb_params1['save_binary'] = True\n# lgb_params1['metric'] = 'auc'\n# lgb_params1['is_unbalance'] = True\n# lgb_params1['boost_from_average'] = False\n# lgb_params1['n_jobs'] =-1\n# lgb_params1['verbose']=0\n# lgb_params1['learning_rate'] = 0.24574985454914924\n# lgb_params1['num_leaves'] = 2\n# lgb_params1['max_bin'] = 300\n# lgb_params1['min_sum_hessian_in_leaf'] = 1e-6\n# lgb_params1['n_estimators'] = 1370\n# lgb_params1['subsample'] = 1.0   \n# lgb_params1['min_samples_split'] = 100\n# lgb_params1['min_samples_leaf'] = 1\n\n# lgb_params2 = {}\n# lgb_params2['boosting_type']= 'gbdt'\n# lgb_params2['objective'] = 'binary'\n# lgb_params2['bagging_fraction']= 0.55\n# lgb_params2['max_depth'] = 14\n# lgb_params2['save_binary'] = True\n# lgb_params2['metric'] = 'auc'\n# lgb_params2['is_unbalance'] = True\n# lgb_params2['boost_from_average'] = False\n# lgb_params2['n_jobs'] =-1\n# lgb_params2['verbose']=0\n# lgb_params2['learning_rate'] = 0.24574985454914924\n# lgb_params2['num_leaves'] = 2\n# lgb_params2['max_bin'] = 300\n# lgb_params2['min_sum_hessian_in_leaf'] = 1e-6\n# lgb_params2['n_estimators'] = 1500\n# lgb_params2['subsample'] = 1.0   \n# lgb_params2['min_samples_split'] = 2\n# lgb_params2['min_samples_leaf'] = 1\n\n# lgb_params3 = {}\n# lgb_params3['boosting_type']= 'gbdt'\n# lgb_params3['objective'] = 'binary'\n# lgb_params3['bagging_fraction']= 0.55\n# lgb_params3['max_depth'] = 14\n# lgb_params3['save_binary'] = True\n# lgb_params3['metric'] = 'auc'\n# lgb_params3['is_unbalance'] = True\n# lgb_params3['boost_from_average'] = False\n# lgb_params3['n_jobs'] =-1\n# lgb_params3['verbose']=0\n# lgb_params3['learning_rate'] = 0.20384544323121773\n# lgb_params3['num_leaves'] = 2\n# lgb_params3['max_bin'] = 300\n# lgb_params3['min_sum_hessian_in_leaf'] = 0.049760518879130244\n# lgb_params3['n_estimators'] = 1286\n# lgb_params3['subsample'] = 0.7166484127307354   \n# lgb_params3['min_samples_split'] = 79\n# lgb_params3['min_samples_leaf'] = 92\n\n# lgb_params4 = {}\n# lgb_params4['boosting_type']= 'gbdt'\n# lgb_params4['objective'] = 'binary'\n# lgb_params4['bagging_fraction']= 0.55\n# lgb_params4['max_depth'] = 14\n# lgb_params4['save_binary'] = True\n# lgb_params4['metric'] = 'auc'\n# lgb_params4['is_unbalance'] = True\n# lgb_params4['boost_from_average'] = False\n# lgb_params4['n_jobs'] =-1\n# lgb_params4['verbose']= 0\n# lgb_params4['learning_rate'] = 0.01\n# lgb_params4['num_leaves'] = 6\n# lgb_params4['max_bin'] = 63\n# lgb_params4['min_sum_hessian_in_leaf'] = 0.000446\n# lgb_params4['n_estimators'] = 1286\n# lgb_params4['subsample'] = 0.7166484127307354   \n# lgb_params4['min_samples_split'] = 79\n# lgb_params4['min_samples_leaf'] = 90\n\n# lgb_model = lgb.LGBMClassifier(**lgb_params1)\n# lgb_model2 = lgb.LGBMClassifier(**lgb_params2)\n# lgb_model3 = lgb.LGBMClassifier(**lgb_params3)\n# lgb_model4 = lgb.LGBMClassifier(**lgb_params4)\n# log_model = LogisticRegression(solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a3d9693a0f854ad8604a3a30b94252afd63af5f"},"cell_type":"code","source":"# stack = Ensemble(n_splits=10,\n#         stacker = log_model,\n#         base_models = (lgb_model, lgb_model2, lgb_model3, lgb_model4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"639f152a376a9eb054d67a58b09037449b06f364"},"cell_type":"code","source":"# test_df = pd.read_csv(TEST_DF)\n# test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d9185ce6e159083e924cfa1def58b120c84c91"},"cell_type":"code","source":"# X_test = test_df.drop(['ID_code'], axis=1)\n# scaler.fit_transform(X_test.values)\n# predictions = lgb_gbm.predict(X_test, num_iteration=lgb_gbm.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6568537dcf371cc444cad8a7ae53b3087714cfa4"},"cell_type":"code","source":"# predictions = stack.fit_predict(X, Y, X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b911566d72ea1ae05d93c581b6d975e4f29d926"},"cell_type":"code","source":"# submission_df = pd.read_csv(SUB_DF)\n# submission_df.target = predictions\n# submission_df.to_csv('submission.csv', index=False)\n# submission_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}