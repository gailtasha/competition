{"cells":[{"metadata":{},"cell_type":"markdown","source":"## In this Kernel, I explore preprocessing the data by splitting it into clusters. \nThe approach is to split the data into K clusters and train a LGB model on each. While testing we first assign the test data point to the nearest cluster, then predict based on the model of the cluster. This kernel depicts the basic pipeline. We employ k-means clustering. Advanced clustering techniques  (like kernel k-means) can be tried to get a higher score."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\npath = \"../input/\"\nprint(os.listdir(path))","execution_count":7,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans","execution_count":19,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"%time\nTrain_df = pd.read_csv(path + 'train.csv')\nTrain_df.head()","execution_count":20,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 5.48 Âµs\n","name":"stdout"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nTest_df = pd.read_csv(path + 'test.csv')\nTest_df.head()","execution_count":58,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 5.72 Âµs\n","name":"stdout"},{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"  ID_code    var_0    var_1   ...     var_197  var_198  var_199\n0  test_0  11.0656   7.7798   ...     10.7200  15.4722  -8.7197\n1  test_1   8.5304   1.2543   ...      9.8714  19.1293 -20.9760\n2  test_2   5.4827 -10.3581   ...      7.0618  19.8956 -23.1794\n3  test_3   8.5374  -1.3222   ...      9.2295  13.0168  -4.2108\n4  test_4  11.7058  -0.1327   ...      7.2882  13.9260  -9.1846\n\n[5 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>11.0656</td>\n      <td>7.7798</td>\n      <td>12.9536</td>\n      <td>9.4292</td>\n      <td>11.4327</td>\n      <td>-2.3805</td>\n      <td>5.8493</td>\n      <td>18.2675</td>\n      <td>2.1337</td>\n      <td>8.8100</td>\n      <td>-2.0248</td>\n      <td>-4.3554</td>\n      <td>13.9696</td>\n      <td>0.3458</td>\n      <td>7.5408</td>\n      <td>14.5001</td>\n      <td>7.7028</td>\n      <td>-19.0919</td>\n      <td>15.5806</td>\n      <td>16.1763</td>\n      <td>3.7088</td>\n      <td>18.8064</td>\n      <td>1.5899</td>\n      <td>3.0654</td>\n      <td>6.4509</td>\n      <td>14.1192</td>\n      <td>-9.4902</td>\n      <td>-2.1917</td>\n      <td>5.7107</td>\n      <td>3.7864</td>\n      <td>-1.7981</td>\n      <td>9.2645</td>\n      <td>2.0657</td>\n      <td>12.7753</td>\n      <td>11.3334</td>\n      <td>8.1462</td>\n      <td>-0.0610</td>\n      <td>3.5331</td>\n      <td>9.7804</td>\n      <td>...</td>\n      <td>5.9232</td>\n      <td>5.4113</td>\n      <td>3.8302</td>\n      <td>5.7380</td>\n      <td>-8.6105</td>\n      <td>22.9530</td>\n      <td>2.5531</td>\n      <td>-0.2836</td>\n      <td>4.3416</td>\n      <td>5.1855</td>\n      <td>4.2603</td>\n      <td>1.6779</td>\n      <td>29.0849</td>\n      <td>8.4685</td>\n      <td>18.1317</td>\n      <td>12.2818</td>\n      <td>-0.6912</td>\n      <td>10.2226</td>\n      <td>-5.5579</td>\n      <td>2.2926</td>\n      <td>-4.5358</td>\n      <td>10.3903</td>\n      <td>-15.4937</td>\n      <td>3.9697</td>\n      <td>31.3521</td>\n      <td>-1.1651</td>\n      <td>9.2874</td>\n      <td>-23.5705</td>\n      <td>13.2643</td>\n      <td>1.6591</td>\n      <td>-2.1556</td>\n      <td>11.8495</td>\n      <td>-1.4300</td>\n      <td>2.4508</td>\n      <td>13.7112</td>\n      <td>2.4669</td>\n      <td>4.3654</td>\n      <td>10.7200</td>\n      <td>15.4722</td>\n      <td>-8.7197</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>8.5304</td>\n      <td>1.2543</td>\n      <td>11.3047</td>\n      <td>5.1858</td>\n      <td>9.1974</td>\n      <td>-4.0117</td>\n      <td>6.0196</td>\n      <td>18.6316</td>\n      <td>-4.4131</td>\n      <td>5.9739</td>\n      <td>-1.3809</td>\n      <td>-0.3310</td>\n      <td>14.1129</td>\n      <td>2.5667</td>\n      <td>5.4988</td>\n      <td>14.1853</td>\n      <td>7.0196</td>\n      <td>4.6564</td>\n      <td>29.1609</td>\n      <td>0.0910</td>\n      <td>12.1469</td>\n      <td>3.1389</td>\n      <td>5.2578</td>\n      <td>2.4228</td>\n      <td>16.2064</td>\n      <td>13.5023</td>\n      <td>-5.2341</td>\n      <td>-3.6648</td>\n      <td>5.7080</td>\n      <td>2.9965</td>\n      <td>-10.4720</td>\n      <td>11.4938</td>\n      <td>-0.9660</td>\n      <td>15.3445</td>\n      <td>10.6361</td>\n      <td>0.8966</td>\n      <td>6.7428</td>\n      <td>2.3421</td>\n      <td>12.8678</td>\n      <td>...</td>\n      <td>30.9641</td>\n      <td>5.6723</td>\n      <td>3.6873</td>\n      <td>13.0429</td>\n      <td>-10.6572</td>\n      <td>15.5134</td>\n      <td>3.2185</td>\n      <td>9.0535</td>\n      <td>7.0535</td>\n      <td>5.3924</td>\n      <td>-0.7720</td>\n      <td>-8.1783</td>\n      <td>29.9227</td>\n      <td>-5.6274</td>\n      <td>10.5018</td>\n      <td>9.6083</td>\n      <td>-0.4935</td>\n      <td>8.1696</td>\n      <td>-4.3605</td>\n      <td>5.2110</td>\n      <td>0.4087</td>\n      <td>12.0030</td>\n      <td>-10.3812</td>\n      <td>5.8496</td>\n      <td>25.1958</td>\n      <td>-8.8468</td>\n      <td>11.8263</td>\n      <td>-8.7112</td>\n      <td>15.9072</td>\n      <td>0.9812</td>\n      <td>10.6165</td>\n      <td>8.8349</td>\n      <td>0.9403</td>\n      <td>10.1282</td>\n      <td>15.5765</td>\n      <td>0.4773</td>\n      <td>-1.4852</td>\n      <td>9.8714</td>\n      <td>19.1293</td>\n      <td>-20.9760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>5.4827</td>\n      <td>-10.3581</td>\n      <td>10.1407</td>\n      <td>7.0479</td>\n      <td>10.2628</td>\n      <td>9.8052</td>\n      <td>4.8950</td>\n      <td>20.2537</td>\n      <td>1.5233</td>\n      <td>8.3442</td>\n      <td>-4.7057</td>\n      <td>-3.0422</td>\n      <td>13.6751</td>\n      <td>3.8183</td>\n      <td>10.8535</td>\n      <td>14.2126</td>\n      <td>9.8837</td>\n      <td>2.6541</td>\n      <td>21.2181</td>\n      <td>20.8163</td>\n      <td>12.4666</td>\n      <td>12.3696</td>\n      <td>4.7473</td>\n      <td>2.7936</td>\n      <td>5.2189</td>\n      <td>13.5670</td>\n      <td>-15.4246</td>\n      <td>-0.1655</td>\n      <td>7.2633</td>\n      <td>3.4310</td>\n      <td>-9.1508</td>\n      <td>9.7320</td>\n      <td>3.1062</td>\n      <td>22.3076</td>\n      <td>11.9593</td>\n      <td>9.9255</td>\n      <td>4.0702</td>\n      <td>4.9934</td>\n      <td>8.0667</td>\n      <td>...</td>\n      <td>39.3654</td>\n      <td>5.5228</td>\n      <td>3.3159</td>\n      <td>4.3324</td>\n      <td>-0.5382</td>\n      <td>13.3009</td>\n      <td>3.1243</td>\n      <td>-4.1731</td>\n      <td>1.2330</td>\n      <td>6.1513</td>\n      <td>-0.0391</td>\n      <td>1.4950</td>\n      <td>16.8874</td>\n      <td>-2.9787</td>\n      <td>27.4035</td>\n      <td>15.8819</td>\n      <td>-10.9660</td>\n      <td>15.6415</td>\n      <td>-9.4056</td>\n      <td>4.4611</td>\n      <td>-3.0835</td>\n      <td>8.5549</td>\n      <td>-2.8517</td>\n      <td>13.4770</td>\n      <td>24.4721</td>\n      <td>-3.4824</td>\n      <td>4.9178</td>\n      <td>-2.0720</td>\n      <td>11.5390</td>\n      <td>1.1821</td>\n      <td>-0.7484</td>\n      <td>10.9935</td>\n      <td>1.9803</td>\n      <td>2.1800</td>\n      <td>12.9813</td>\n      <td>2.1281</td>\n      <td>-7.1086</td>\n      <td>7.0618</td>\n      <td>19.8956</td>\n      <td>-23.1794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>8.5374</td>\n      <td>-1.3222</td>\n      <td>12.0220</td>\n      <td>6.5749</td>\n      <td>8.8458</td>\n      <td>3.1744</td>\n      <td>4.9397</td>\n      <td>20.5660</td>\n      <td>3.3755</td>\n      <td>7.4578</td>\n      <td>0.0095</td>\n      <td>-5.0659</td>\n      <td>14.0526</td>\n      <td>13.5010</td>\n      <td>8.7660</td>\n      <td>14.7352</td>\n      <td>10.0383</td>\n      <td>-15.3508</td>\n      <td>2.1273</td>\n      <td>21.4797</td>\n      <td>14.5372</td>\n      <td>12.5527</td>\n      <td>2.9707</td>\n      <td>4.2398</td>\n      <td>13.7796</td>\n      <td>14.1408</td>\n      <td>1.0061</td>\n      <td>-1.3479</td>\n      <td>5.2570</td>\n      <td>6.5911</td>\n      <td>6.2161</td>\n      <td>9.5540</td>\n      <td>2.3628</td>\n      <td>10.2124</td>\n      <td>10.8047</td>\n      <td>-2.5588</td>\n      <td>6.0720</td>\n      <td>3.2613</td>\n      <td>16.5632</td>\n      <td>...</td>\n      <td>19.7251</td>\n      <td>5.3882</td>\n      <td>3.6775</td>\n      <td>7.4753</td>\n      <td>-11.0780</td>\n      <td>24.8712</td>\n      <td>2.6415</td>\n      <td>2.2673</td>\n      <td>7.2788</td>\n      <td>5.6406</td>\n      <td>7.2048</td>\n      <td>3.4504</td>\n      <td>2.4130</td>\n      <td>11.1674</td>\n      <td>14.5499</td>\n      <td>10.6151</td>\n      <td>-5.7922</td>\n      <td>13.9407</td>\n      <td>7.1078</td>\n      <td>1.1019</td>\n      <td>9.4590</td>\n      <td>9.8243</td>\n      <td>5.9917</td>\n      <td>5.1634</td>\n      <td>8.1154</td>\n      <td>3.6638</td>\n      <td>3.3102</td>\n      <td>-19.7819</td>\n      <td>13.4499</td>\n      <td>1.3104</td>\n      <td>9.5702</td>\n      <td>9.0766</td>\n      <td>1.6580</td>\n      <td>3.5813</td>\n      <td>15.1874</td>\n      <td>3.1656</td>\n      <td>3.9567</td>\n      <td>9.2295</td>\n      <td>13.0168</td>\n      <td>-4.2108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>11.7058</td>\n      <td>-0.1327</td>\n      <td>14.1295</td>\n      <td>7.7506</td>\n      <td>9.1035</td>\n      <td>-8.5848</td>\n      <td>6.8595</td>\n      <td>10.6048</td>\n      <td>2.9890</td>\n      <td>7.1437</td>\n      <td>5.1025</td>\n      <td>-3.2827</td>\n      <td>14.1013</td>\n      <td>8.9672</td>\n      <td>4.7276</td>\n      <td>14.5811</td>\n      <td>11.8615</td>\n      <td>3.1480</td>\n      <td>18.0126</td>\n      <td>13.8006</td>\n      <td>1.6026</td>\n      <td>16.3059</td>\n      <td>6.7954</td>\n      <td>3.6015</td>\n      <td>13.6569</td>\n      <td>13.8807</td>\n      <td>8.6228</td>\n      <td>-2.2654</td>\n      <td>5.2255</td>\n      <td>7.0165</td>\n      <td>-15.6961</td>\n      <td>10.6239</td>\n      <td>-4.7674</td>\n      <td>17.5447</td>\n      <td>11.8668</td>\n      <td>3.0154</td>\n      <td>4.2546</td>\n      <td>6.7601</td>\n      <td>5.9613</td>\n      <td>...</td>\n      <td>22.8700</td>\n      <td>5.6688</td>\n      <td>6.1159</td>\n      <td>13.2433</td>\n      <td>-11.9785</td>\n      <td>26.2040</td>\n      <td>3.2348</td>\n      <td>-5.5775</td>\n      <td>5.7036</td>\n      <td>6.1717</td>\n      <td>-1.6039</td>\n      <td>-2.4866</td>\n      <td>17.2728</td>\n      <td>2.3640</td>\n      <td>14.0037</td>\n      <td>12.9165</td>\n      <td>-12.0311</td>\n      <td>10.1161</td>\n      <td>-8.7562</td>\n      <td>6.0889</td>\n      <td>-1.3620</td>\n      <td>10.3559</td>\n      <td>-7.4915</td>\n      <td>9.4588</td>\n      <td>3.9829</td>\n      <td>5.8580</td>\n      <td>8.3635</td>\n      <td>-24.8254</td>\n      <td>11.4928</td>\n      <td>1.6321</td>\n      <td>4.2259</td>\n      <td>9.1723</td>\n      <td>1.2835</td>\n      <td>3.3778</td>\n      <td>19.5542</td>\n      <td>-0.2860</td>\n      <td>-5.1612</td>\n      <td>7.2882</td>\n      <td>13.9260</td>\n      <td>-9.1846</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 5\ntest_size = 0.3","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting train_df into train and test sets (X_train, X_test, Y_train, Y_test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df = Train_df['target']\nTrain_df.drop(columns=['target'], inplace=True)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Train_df, y_df, test_size=test_size, random_state=40)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train), len(y_train)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"(140000, 140000)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [i for i in X_train.columns if i not in ['ID_code']]","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We augment the dataset to oversample the positive examples to deal with class imbalance.\n\nhttps://www.kaggle.com/jesucristo/santander-magic-lgb-0-901\n\nThe difference here is we augnent each cluster rather than the entire dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t//2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kmeans clustering of the data into n_clusters. Data is augmented with a column called 'clusters'.\n\nTo get cluster of a new data point x, use cluster = kmeans.predict(x)."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=n_clusters, max_iter=1000).fit(X_train[columns])\nX_train[\"clusters\"] = kmeans.labels_","execution_count":27,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## cluster_idxs is a list containing indices of data belonging to a cluster.\n\nFor eg: for getting data of cluster i use X_train[cluster_idxs[i]]"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_idxs = [X_train[\"clusters\"] == i for i in range(5)]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.1,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 4,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1,\n    'max_bin': 50,\n}","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Predictors[i] contains model for cluster i"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = []\n\nfor i in range(n_clusters):\n    \n    print(\"TRAINING MODEL FOR CLUSTER: {}\".format(i))\n    x_i = X_train[cluster_idxs[i]]\n    y_i = y_train[cluster_idxs[i]]\n\n    num_folds = 3\n    features = [c for c in x_i.columns if c not in ['ID_code', 'clusters', 'target']]\n    folds = KFold(n_splits=num_folds, random_state=44000)\n\n    x_i = x_i[features]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(x_i.values, y_i.values)):\n\n        X_trai, y_trai = x_i.iloc[trn_idx][features], y_i.iloc[trn_idx]\n        X_val, y_val = x_i.iloc[val_idx][features], y_i.iloc[val_idx]\n\n        X_trai, y_trai = augment(X_trai.values, y_trai.values)\n        X_trai = pd.DataFrame(X_trai)\n\n        print(\"Fold idx:{}\".format(fold_ + 1))\n        trn_data = lgb.Dataset(X_trai, label=y_trai)\n        val_data = lgb.Dataset(X_val, label=y_val)\n\n        clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 3000)\n        predictors.append(clf)","execution_count":31,"outputs":[{"output_type":"stream","text":"TRAINING MODEL FOR CLUSTER: 0\nFold idx:1\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.932588\tvalid_1's auc: 0.886164\n[10000]\ttraining's auc: 0.947049\tvalid_1's auc: 0.888172\nEarly stopping, best iteration is:\n[11686]\ttraining's auc: 0.951009\tvalid_1's auc: 0.888696\nFold idx:2\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933191\tvalid_1's auc: 0.886442\nEarly stopping, best iteration is:\n[5028]\ttraining's auc: 0.933276\tvalid_1's auc: 0.886708\nFold idx:3\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.931987\tvalid_1's auc: 0.88769\n[10000]\ttraining's auc: 0.946822\tvalid_1's auc: 0.886888\nEarly stopping, best iteration is:\n[7189]\ttraining's auc: 0.939179\tvalid_1's auc: 0.888161\nTRAINING MODEL FOR CLUSTER: 1\nFold idx:1\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.932206\tvalid_1's auc: 0.886743\nEarly stopping, best iteration is:\n[5646]\ttraining's auc: 0.934178\tvalid_1's auc: 0.887363\nFold idx:2\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.93107\tvalid_1's auc: 0.88847\nEarly stopping, best iteration is:\n[6456]\ttraining's auc: 0.936339\tvalid_1's auc: 0.889679\nFold idx:3\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.932623\tvalid_1's auc: 0.8868\n[10000]\ttraining's auc: 0.946674\tvalid_1's auc: 0.887662\nEarly stopping, best iteration is:\n[7528]\ttraining's auc: 0.940559\tvalid_1's auc: 0.888413\nTRAINING MODEL FOR CLUSTER: 2\nFold idx:1\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.928249\tvalid_1's auc: 0.88018\nEarly stopping, best iteration is:\n[5735]\ttraining's auc: 0.930388\tvalid_1's auc: 0.880805\nFold idx:2\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.924051\tvalid_1's auc: 0.887419\n[10000]\ttraining's auc: 0.937173\tvalid_1's auc: 0.888374\nEarly stopping, best iteration is:\n[8178]\ttraining's auc: 0.933163\tvalid_1's auc: 0.888947\nFold idx:3\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.921025\tvalid_1's auc: 0.895387\nEarly stopping, best iteration is:\n[6016]\ttraining's auc: 0.924388\tvalid_1's auc: 0.896329\nTRAINING MODEL FOR CLUSTER: 3\nFold idx:1\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.925025\tvalid_1's auc: 0.881551\nEarly stopping, best iteration is:\n[4512]\ttraining's auc: 0.923\tvalid_1's auc: 0.882151\nFold idx:2\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.926963\tvalid_1's auc: 0.880244\nEarly stopping, best iteration is:\n[6730]\ttraining's auc: 0.932868\tvalid_1's auc: 0.881527\nFold idx:3\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.927808\tvalid_1's auc: 0.884253\n[10000]\ttraining's auc: 0.941216\tvalid_1's auc: 0.885802\nEarly stopping, best iteration is:\n[9898]\ttraining's auc: 0.941035\tvalid_1's auc: 0.885994\nTRAINING MODEL FOR CLUSTER: 4\nFold idx:1\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.929795\tvalid_1's auc: 0.885716\nEarly stopping, best iteration is:\n[5383]\ttraining's auc: 0.931266\tvalid_1's auc: 0.885921\nFold idx:2\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.930639\tvalid_1's auc: 0.882112\n[10000]\ttraining's auc: 0.945853\tvalid_1's auc: 0.883086\nEarly stopping, best iteration is:\n[8714]\ttraining's auc: 0.942566\tvalid_1's auc: 0.883845\nFold idx:3\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.931376\tvalid_1's auc: 0.881382\nEarly stopping, best iteration is:\n[3853]\ttraining's auc: 0.926944\tvalid_1's auc: 0.882303\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Get the predictions on the Test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions(kmeans, X_test, n_clusters, get_score=True):\n    features = [c for c in X_test.columns if c not in ['ID_code', 'target']]\n    X_test['cluster'] = kmeans.predict(X_test[features])\n    test_idxs = [X_test['cluster'] == i for i in range(n_clusters)]\n    X_test.drop(columns=['cluster'], inplace=True)\n    preds = []\n    true = []\n\n    for i in range(n_clusters):\n        x_te = X_test[test_idxs[i]]\n        pred = predictors[i].predict(x_te[features])\n        preds.append(pred)\n        if get_score:\n            y_te = y_test[test_idxs[i]]\n            true.append(y_te.values)\n    x = []\n    y = []\n    for i in preds:\n        x = x + list(i)\n    if get_score:\n        for i in true:\n            y = y + list(i)\n        print(roc_auc_score( np.array(y), np.array(x)))\n    \n    return np.array(x)\n    ","execution_count":66,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AUC score on our sampled test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_predictions(kmeans, X_test, n_clusters = 5, get_score=True)","execution_count":68,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  errors=errors)\n","name":"stderr"},{"output_type":"stream","text":"0.8827581634479668\n","name":"stdout"},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"array([0.00713637, 0.03206016, 0.00710551, ..., 0.06524568, 0.38505604,\n       0.8001934 ])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## To enhance the pipeline:\n- Train the model on the entire dataset, skipping the test set\n- Use different clustering algorithms, (eg: kernel k-means)\n- Try different numeber of clusters\n- Try other models for each cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}