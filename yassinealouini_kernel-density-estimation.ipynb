{"cells":[{"metadata":{"_uuid":"1142d4fd5f1a019cccc62a4baea2fcaa0f367e86"},"cell_type":"markdown","source":"In this short notebook, I will explore a [nonparametric statistics](https://en.wikipedia.org/wiki/Nonparametric_statistics) method \nfor fitting densities: [**kernel density estimation**](https://en.wikipedia.org/wiki/Kernel_density_estimation). \nNotice that this notebook is (mainly) inspired from this great [post](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/. ). \n\nLet's dive in!"},{"metadata":{"trusted":true,"_uuid":"356d1dcd4970935084a274029515cc64322b52ed"},"cell_type":"code","source":"# Some imports, as usual.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KernelDensity\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a52a76e04952ad73ac511fa1357524384ff1b6d7"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49eacf6424510698d0c0614952b6c3a8636accf8"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 8))\ntrain_df['var_81'].plot(kind='hist', bins=100, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"947be3db93a5a635d5051139d9824f75af0246ae"},"cell_type":"code","source":"var_81_a = train_df['var_81'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"742cc6e7def37f2fead478dea86285e07040cfd9"},"cell_type":"markdown","source":"# Fitting"},{"metadata":{"_uuid":"7465887685662b7deb797dd646abbd1af5253ab5"},"cell_type":"markdown","source":"Let's try this approach on the `var_81` feature."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b0ca4fc3c8d47f268981d9f612b8495537357ff4"},"cell_type":"code","source":"kde = KernelDensity()\nkde.fit(var_81_a[:, None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c2533f541958fe56b67c09d8e39c82cab7e904"},"cell_type":"code","source":"x_grid = np.linspace(var_81_a.min(), var_81_a.max(), 1000)\npdf = np.exp(kde.score_samples(x_grid[:, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f1201bcb52764223ddba8134dc368c3d24c03e"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 8))\nax.plot(x_grid, pdf, linewidth=3, alpha=0.5, label='bw=%.2f' % kde.bandwidth)\nax.hist(var_81_a, 100, fc='gray', histtype='stepfilled', alpha=0.3, density=True)\nax.legend(loc='upper left')\nax.set_xlim(var_81_a.min(), var_81_a.max());","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0799a1f000bd96963a0be599e1f68956e37d5cb6"},"cell_type":"markdown","source":"=> The fitted curve doesn't seem to follow the histogram. Could we do better? \n\nYes, we can. There is a hyperparameter that one can vary: the kernel bandwidth. \nLet's see how the fitted kde changes when this bandwidth changes. "},{"metadata":{"_uuid":"a396ca49a647075588033652ef4fcf18d4af7702"},"cell_type":"markdown","source":"# Different bandwidths"},{"metadata":{"trusted":true,"_uuid":"7c1ba80a5d6a4c90442c99e00d659e67ae75996f"},"cell_type":"code","source":"def fit_kde_and_plot(bandwidth):\n    \"\"\" Fit a kernel density estimator and plot the resulting graph. \"\"\"\n    kde = KernelDensity(bandwidth=bandwidth)\n    kde.fit(var_81_a[:, None])\n    x_grid = np.linspace(var_81_a.min(), var_81_a.max(), 1000)\n    pdf = np.exp(kde.score_samples(x_grid[:, None]))\n\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    ax.plot(x_grid, pdf, linewidth=3, alpha=0.5, label='bw=%.2f' % kde.bandwidth)\n    ax.hist(var_81_a, 100, fc='gray', histtype='stepfilled', alpha=0.3, density=True)\n    ax.legend(loc='upper left')\n    ax.set_xlim(var_81_a.min(), var_81_a.max())\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"609a954c98ed97d5b946640ccc540eb4c5d05aaf"},"cell_type":"code","source":"fit_kde_and_plot(0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a466456d09e6d0c3e1b0a4245aedf763ed90892c"},"cell_type":"code","source":"fit_kde_and_plot(0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"596d70887c0a38888892ef334d55251a4fa0f273"},"cell_type":"code","source":"fit_kde_and_plot(0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada6a0bae9ab7aef5fb2528f54bab54e03c0c829"},"cell_type":"markdown","source":"=> A smaller bandwidth results in a more varying estimator and \na bigger value results in a smoother one. \n\nHave you spotted the [**bias-variance**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) trade-off already? ;)\n\nLet's try [**cross-validation**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) to find the best one."},{"metadata":{"_uuid":"8ad0db33d2e21f50a2658d5e2908b074abab33b1"},"cell_type":"markdown","source":"# Grid searching the best bandwidth"},{"metadata":{"_uuid":"3fae570033a3a9c62dd237c6fb59fd349877d502"},"cell_type":"markdown","source":"=> WIP: The grid search runs for a very long time, maybe I am doing something wrong? Any help is appreciated!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# TODO: Investigate why this takse so much time to run...\n# grid = GridSearchCV(KernelDensity(),\n#                     {'bandwidth': [0.1, 0.2, 0.3, 0.4, 1.0]},\n#                     cv=2, n_jobs=-1)\n# grid.fit(var_81_a[:, None])\n# print(grid.best_params_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}