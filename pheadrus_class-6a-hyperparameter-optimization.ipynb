{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"### imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ndf_test=test.drop(['ID_code'], axis=1)\ndf_test = df_test.values\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm(range(df_test.shape[1])):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\ndf_test_real = df_test[real_samples_indexes].copy()\ndf_test_real=pd.DataFrame(df_test_real)\ndf_test_real=df_test_real.add_prefix('var_')\ndf_test_real.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_value=train.drop(['ID_code', 'target'], axis=1)\ndf_combined=pd.concat([train_value, df_test_real])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(25):\n    var='var_'+str(i)\n    if i%25==0:\n        print (i)\n    dictionary=df_combined[var].value_counts().to_dict()\n    train['count_'+var]=train[var].map(dictionary)\n    train['test_'+var]=train[var]*np.log2(train['count_'+var]+1)\n    train['test1_'+var]=train[var]/np.log2(train['count_'+var]+1)\n    train['test2_'+var]=train[var]*(-np.log2(train['count_'+var]+1))\n    train.drop('count_'+var, inplace=True, axis=1)\n    dictionary1=df_test_real[var].value_counts().to_dict()\n    test['count_'+var]=test[var].map(dictionary)\n    test['test_'+var]=np.log2(test['count_'+var]+1)*test[var]\n    test['test1_'+var]=test[var]/np.log2(test['count_'+var]+1)\n    test['test2_'+var]=test[var]*(-np.log2(test['count_'+var]+1))\n    test.drop('count_'+var, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_code=test['ID_code']\nX_test = test.drop(['ID_code'],axis = 1)\n#X_test=X_test[X_test.columns[:200].append(X_test.columns[400:])]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['target']\nX = train.drop(['target', 'ID_code'], axis=1)\n#X=X[X.columns[:200].append(X.columns[400:])]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in X.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nauc_scorer = make_scorer(roc_auc_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=auc_scorer)\ngrid_obj = grid_obj.fit(X.iloc[:1000,:], y[:1000])\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X.iloc[:1000,:], y[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(X.iloc[:1000,:])\nprint(roc_auc_score(y[:1000], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exercise: Validate the above score using k-fold CV"},{"metadata":{},"cell_type":"markdown","source":"### RandomSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nauc_scorer = make_scorer(roc_auc_score)\n\n# Run the grid search\n\nrandom_search_obj = RandomizedSearchCV(clf, param_distributions=parameters,\n                                   n_iter=100, cv=5)\n\n#grid_obj = GridSearchCV(clf, parameters, scoring=auc_scorer)\nrandom_search_obj = random_search_obj.fit(X.iloc[:1000,:], y[:1000])\n\n# Set the clf to the best combination of parameters\nclf = random_search_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X.iloc[:1000,:], y[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(X.iloc[:1000,:])\nprint(roc_auc_score(y[:1000], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Observe how the randomsearchCV takes less time to execute but produces inferior results in this case.\n- Generally, it is okay to use randomsearchCV to save time and get approximately okayish results"},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimization\n\n- Link to original paper: https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\n### make a scorer fn\nauc_scorer = make_scorer(roc_auc_score)\n\nX = X.iloc[:1000,:]\ny = y[:1000]\n\n### define obj\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X, y, scoring=auc_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\n### define search space\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),  ### quniform defines how values will be sampled\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)            ### there are other parameteric distributions also available\n}\n\n### put together\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=3)   ### increase for best results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestParams = {'n_estimators': best['n_estimators'],\n              'max_depth': best['max_depth']\n             }\nbestModel = RandomForestClassifier(n_estimators = int(best['n_estimators']),max_depth= int(best['max_depth']))\n\nbestModel = bestModel.fit(X,y)\n\npredictions = bestModel.predict(X)\nprint(roc_auc_score(y, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### What do you conclude from this analysis? "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}