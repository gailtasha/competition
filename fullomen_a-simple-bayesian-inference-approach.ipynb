{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assested Exercise\n\n### Student: Damiano Franz√≤\n\nThe Notebook is divided in the following sections:\n- [0. Environment Setup](#0.-Environment-Setup)\n- [1. Dataset Setup](#1.-Dataset-Setup) \n- [2. Distribution Analysis](#2.-Distribution-Analysis)\n- [3. Bayesian Linear Regression](#3.-Bayesian-Linear-Regression)\n    - [3.a Bayesian linear regression Implementation](#3.a-Bayesian-linear-regression-Implementation)\n    - [3.b Gaussian Prior Parameters choice](#3.b-Gaussian-Prior-Parameters-choice)\n    - [3.c Nth order polynomial](#3.c-Nth-order-polynomial)\n    - [3.d Data Preprocessing](#3.d-Data-Preprocessing)\n    - [3.e Treat data as continuous](#3.e-Treat-data-as-continuous)\n    - [3.f Discretize Predictions](#3.f-Discretize-Predictions)\n    - [3.g Comparison vs Random Classifier](#3.g-Comparison-vs-Random-Classifier)\n- [4. Logistic Regression](#4.-Logistic-Regression)\n    - [4.a Bayesian Logistic Regression](#4.a-Bayesian-Logistic-Regression)\n    - [4.b Gradient Descent](#4.b Gradient Descent)\n    - [4.c Convexity of the problem](#4.c-Convexity-of-the-problem)\n    - [4.d Comparison with Bayesian Linear Regression Classifier](#4.d-Comparison-with-Bayesian-Linear-Regression-Classifier)\n    - [4.e Laplace Approximation](#4.e-Laplace-Approximation)\n- [5. Bonus Question](#5.-Bonus-Question)\n- [Addictional test with ADAM](#Addictional-test-with-ADAM)"},{"metadata":{},"cell_type":"markdown","source":"### 0. Environment Setup\nImport of used libraries, modules and functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nfrom copy import deepcopy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Dataset Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the csv dataset\ndf = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing some rows of the dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.set_index('ID_code')\n# it is the label indicating wheter the sample is a fraud or not\ny = df['target']\nX = df.drop(['target'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratify option assures that the proportion of 0 and 1 are mantained the same between test and train dataset\n# this option is extremely useful dealing imbalanced dataset\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Distribution Analysis\nBelow is displayed the distribution of labels within the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 3))\n\n# TR the count is computed automatically\ng = sns.countplot(x='target', data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is it possible to notice that the dataset is clearly imbalanced. Without the proper adjustements some of machine learning models could be biases toward the classification of *0* respect to *1*.  \nIn addiction to that, a proper consideration has to be done on the choice of evaluation metrics since the *accuracy* is not completely reliable. \nA dummy classifier can classify every sample with label *0* getting ~90% accuracy, but this does not imply it is a smart classifier. On the other hand, two interesting metrics can be *f1_score (macro)* and *f1_score (binary on the worst case)*."},{"metadata":{},"cell_type":"markdown","source":"### 3. Bayesian Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The dataframe is transformed into a numpy.matrix\nX_train, X_test = np.matrix(X_train_df.values), np.matrix(X_test_df.values)\ny_train, y_test = np.matrix(y_train_df).T, np.matrix(y_test_df).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.a Bayesian linear regression Implementation\nAbove there is a recap on Bayesian Linear regression related formulas.  \nThe method names are as much as possible coherent with their meaning in the context of the implementation of the formulas."},{"metadata":{},"cell_type":"markdown","source":"#### Recap of some formulas\n\n##### Prior\n$p(w) = \\mathcal{N}(0, S)$\n\n\n##### Parameters\n$ \\hat{w} = (X^tX)^{-1} X^Tt$  \n$ \\hat{\\sigma^2} = \\frac{1}{N}(t - Xw)^T(t - Xw) $  \n\n$\\Sigma = (\\frac{1}{\\sigma^2} X^{\\intercal}X + S^{-1})^{-1}$       \n$\\mu = \\frac{1}{\\sigma^2} \\Sigma X^{\\intercal}t$  \n##### Posterior\n$p(w| X,t,\\sigma^2) = \\mathcal{N}(\\mu,\\Sigma)$\n\n##### Prediction, density function\n$p(t_{new}| X, t, x_{new}, \\sigma^2) = \\mathcal{N}(x_{new}^T \\mu, \\sigma^2 +  x_{new}^T \\Sigma  x_{new})$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility functions used for the Bayesian Linear Regression\ndef get_w_hat(X, t):\n    xt_x_minus_one = np.linalg.inv(np.dot(X.T, X))\n    xt_t = np.dot(X.T, t)\n    w_hat = np.dot(xt_x_minus_one, xt_t)\n    return w_hat\n\ndef get_sigma_hat(w, X, t, N):\n    X_w = np.dot(X, w)\n    t_minus_X_w = (t - X_w)\n    sigma_hat = (1. / N) * np.dot(t_minus_X_w.T, t_minus_X_w)\n    return np.float(sigma_hat)\n\ndef get_Sigma_w(sigma_square, X, S):\n    return np.linalg.inv((1. / sigma_square) * np.dot(X.T, X) + np.linalg.inv(S))\n\ndef get_mean_w(sigma_square, Sigma, X, t):\n    return (1. / (sigma_square)) * np.dot(np.dot(Sigma, X.T), t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BayesianLinearRegressor:\n    def __init__(self, n=1, sigma_unif=1.):\n        self.n = n\n        self.sigma_unif = sigma_unif\n\n    def lrfit(self, X_, t):\n        X = get_X_powered(X_, self.n)\n        X = get_X_w_bias(X)\n\n        _, S = generate_prior(X.shape[1], sigma_unif=self.sigma_unif)\n\n        w_hat = get_w_hat(X, t)\n\n        sigma_square = get_sigma_hat(w_hat, X, t, X.shape[0])\n\n        Sigma = get_Sigma_w(sigma_square, X, S)\n\n        mean_w = get_mean_w(sigma_square, Sigma, X, t)\n\n        self.mean_w, self.sigma_square, self.Sigma = mean_w, sigma_square, Sigma\n    \n    def predict(self, X):\n        X_pow = get_X(X, self.n)\n\n        return np.dot(X_pow, self.mean_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.b Gaussian Prior Parameters choice\nIn the challenge web page it is written: *You are provided with an anonymized dataset containing numeric feature variables*  \nTherefore, actually you do not have **no prior belief** on the weights of the classifier. Usually, when instead you have prior beliefs on weights, you can use these information to **encode** them into the prior.  \n\nSince the prior is an distribution you state **before** looking into the data, it should not be possible(philosophically correct) to do cross validation on it. We can you do, it is just an usage of a *weakily infomative prior*, where you do not force weights to range across tight intervals.\n\nIn this case, a Gaussian Prior is a reasonable choice"},{"metadata":{},"cell_type":"markdown","source":"#### 3.c Nth order polynomial"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_X_powered(X, n):\n    \"\"\"\n    Return the X matrix concat with all the samples powered to 2..n\n    \n    Args:\n        X (numpy.matrix): Matrix (N, D)\n            - N : number of rows\n            - D : number of features\n        n (str): The power of the matrix.\n\n    Returns:\n        X_result: Matrix X with all the n powers\n    \"\"\"\n    if n <= 1:\n        return X\n    X_result = deepcopy(X)\n    X_powered = deepcopy(X)\n    for i in range(2, n + 1):\n        X_powered = np.multiply(X_powered, X)\n        X_result = np.concatenate((X_result, X_powered), axis=1)\n    return X_result\n\ndef get_X_w_bias(X):\n    \"\"\"\n    Return The X matrix with the bias term\n    \n    Args:\n        X (numpy.matrix): Matrix X (N, D)\n    \n    Returns:\n        X_result (numpy.matrix): Matrix X (N, D + 1) with bias term put on the right column\n    \"\"\"\n    X_result = np.concatenate((X, np.matrix(np.ones((X.shape[0]))).T), axis=1)\n    return X_result\n\ndef get_X(X, n=1):\n    \"\"\"\n    Put together the 2 functions above since they are often used together\n    \"\"\"\n    return get_X_w_bias(get_X_powered(X, n))\n\ndef generate_prior(features, sigma_unif=3.):\n    S = np.matrix(np.diag(sigma_unif * np.ones(features)))\n    return np.matrix(np.zeros(features)), S","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_err(y_pred, y, total=False):\n    \"\"\"\n    Display the error of the Regression predictions\n    \"\"\"\n    err = y - y_pred\n    tot_err = np.sum(np.multiply(err, err))\n    if total:\n        print(\"Total squared error: {}\".format(tot_err))\n    else:\n        print(\"Average squared error: {}\".format(tot_err / len(y)))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1./ (1. + np.exp(-x))\n\n@np.vectorize\ndef discretize(pred, threshold=0.5):\n    \"\"\"\n    The decorator vectorize allows to apply the function to each element\n    \"\"\"\n    return 1 if pred > threshold else 0 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.d Data Preprocessing\nAs discussed before, the dataset is anomized, so there are no way to encode domain knowledge into the data preprocessing of the dataset. Nevertheless, some data preprocessing techiques are still possible:\n- **Checking**: First of all a check on the ranges, missing values can be always done, you do not need domain knowledge on features to do that.  \n\n- **Normalize**: Most of the times there are no reason to **not** normalize the dataset, since experimentally it is a great method to dampen order of magnitude differences, and also in the first steps of some training models, it is a great way to avoid exploding gradient and vanishing gradient  \n\n- **Analyze distribution**: Even if features do not permit to explain domain knowledge, this does not exclude that some columns prenst some **statistical** property. Many times this can be done analyzing the distribution of some feature and from that you can state some hypotesis on preprocessing and test how the model behaves."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of Analysis distribution\nfig, ax = plt.subplots(ncols=3, figsize=(20, 4))\n\nsns.distplot(df['var_0'], ax=ax[0], color='red')\nsns.distplot(df['var_1'], ax=ax[1], color='green')\nsns.distplot(df['var_3'], ax=ax[2], color='blue')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions \"look like\" gaussian, so a possible preprocessing can be just the normalization of those features."},{"metadata":{},"cell_type":"markdown","source":"#### 3.e Treat data as continuous"},{"metadata":{"trusted":true},"cell_type":"code","source":"blr = BayesianLinearRegressor(n=3)\n\nblr.lrfit(X_train, y_train)\ny_test_und = blr.predict(X_test)\nprint_err(y_test_und, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Posterior Variance\nposterior_variance = blr.Sigma\nprint(\"Posterior variance matrix with shape: \", posterior_variance.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.f Discretize Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_metrics(y_test, y_pred):\n    \"\"\"\n    Utility function to display the most important metrics\n    \"\"\"\n    print(\"Weighted score: \", f1_score(y_test, y_pred, average='weighted'))\n    print(\"Macro score: \", f1_score(y_test, y_pred, average='macro'))\n    print(\"Binary 0 score: \", f1_score(y_test, y_pred, pos_label=0))\n    print(\"Binary 1 score: \", f1_score(y_test, y_pred, pos_label=1))\n    print(\"F1 classic score: \", f1_score(y_test, y_pred))\n    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In theory, a naive approach to discretize the prediction is to use 0.5 as threshold to separate 0 from 1. Nevertheless, we cannot exclude that other thresholds can produce different perfromance across different metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = discretize(y_test_und, 0.5)\n\nprint_metrics(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To test different approaches we cannot test the best threshold for the test prediction, since it can be lead to overfitting without we can even notice it. The approach adopted is Cross Validation on the threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def threshold_cross_validation_lg(X_train, y_train, metric_f, n=1, k=10):\n    treshoold_linspace = np.linspace(0, 1, 101)[1:-1] #no sense to take extremis\n    fold = list(range(k))\n\n    # Utility variables\n    fold = list(range(k)) # folds\n    bs = len(X_train) // 10 # batch size\n    tot_len = len(X_train) # total number of samples\n    best_thresholds = [] # best thresholds for each fold\n\n    for i in fold:\n        X_validation = X_train[i * bs : (i + 1) * bs]\n        X_train_fold = np.concatenate((X_train[0 : i * bs], X_train[(i + 1) * bs : tot_len]))\n        \n        y_validation = y_train[i * bs : (i + 1) * bs]\n        y_train_fold = np.concatenate((y_train[0 : i * bs], y_train[(i + 1) * bs : tot_len]))\n        \n        blr = BayesianLinearRegressor(n=n)\n\n        blr.lrfit(X_train_fold, y_train_fold)\n        \n        y_pred_undiscretized = blr.predict(X_validation)\n        \n        maximum_metric = 0.\n        best_thd = 0.\n\n        for thr in treshoold_linspace:\n            y_pred = discretize(y_pred_undiscretized, thr)\n\n            metric = metric_f(y_validation, y_pred)\n            if metric > maximum_metric:\n                maximum_metric = metric\n                best_thr = thr\n                \n        best_thresholds.append(best_thr)\n        print(\"best threshold fold {}: {}\".format(i, best_thr))\n    return np.median(best_thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code above allows to optimize *any* metric, just through the usage of a lambda function on the metric. We want to maximize *f1 score*. We take the median in order to avoid screwed result."},{"metadata":{"trusted":true},"cell_type":"code","source":"metric_f = lambda true, pred : f1_score(true, pred)\n\nbthr = threshold_cross_validation_lg(X_train, y_train, metric_f, n=3, k=10)\n\ny_pred = discretize(y_test_und, bthr)\n\nprint_metrics(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The f1 score obtained is 0.55, which is maximized respect to the threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_confusion_matrix(y_test, y_pred):\n    conf = np.array([[0, 0], [0, 0]])\n\n    conc = np.concatenate((y_test, y_pred), axis=1)\n\n    # we consider 0 as Positive for semplicity\n    for a in conc:\n        val_true, val_pred = a.item(0), a.item(1)\n        if val_true == val_pred:\n            # True Positive\n            if val_pred == 0:\n                conf[0][0] += 1\n            # True Negative\n            else:\n                conf[1][1] += 1\n        if val_true != val_pred:\n            # False positive\n            if val_pred == 0:\n                conf[0][1] += 1\n            # False negative\n            else:\n                conf[1][0] += 1\n    return conf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_test, y_pred):\n    conf = get_confusion_matrix(y_test, y_pred)\n    conf_normalized = 100. * conf / np.sum(conf, axis=0)\n\n    fig,ax = plt.subplots(ncols=2, figsize=(20, 4))\n\n    sns.heatmap(conf, ax=ax[0], annot=True)\n    sns.heatmap(conf_normalized, ax=ax[1], annot=True)\n\n\n    ax[0].set(xticklabels=[0, 1], yticklabels=[0, 1],\n       title='confusion matrix',\n       ylabel='True label',\n       xlabel='Predicted label')\n    ax[1].set(xticklabels=[0, 1], yticklabels=[0, 1],\n       title='Normalized confusion matrix',\n       ylabel='True label',\n       xlabel='Predicted label')\n    sns.set(font_scale=1.1)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"N.B. The accuracy is reported above on the print of the metrics"},{"metadata":{},"cell_type":"markdown","source":"#### 3.g Comparison vs Random Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = discretize(y_test_und, bthr)\n\nprint_metrics(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_classifier = np.matrix(np.random.rand(len(X_test), 1))\n\ny_pred_random_classifier = discretize(random_classifier)\n\nprint_metrics(y_pred_random_classifier, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance are not that bad considering that they have been done considering label to be continuos."},{"metadata":{},"cell_type":"markdown","source":"### 4. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"The formulas below are **semantically** equivalent:\n\n- $\\prod{t_n p(t_n=1|x_n,w) + (1 - t_n) p(t_n=0|x_n,w)}$\n\n- $\\prod{p(t_n=1|x_n,w)^{t_n}  p(t_n=0|x_n,w) ^{(1 - t_n)}}$\n\nSo in the loss functions it is utilized the first one just for comodity"},{"metadata":{},"cell_type":"markdown","source":"#### 4.a Bayesian Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BayesianLogisticRegression:\n    def __init__(self, num_parameters):\n        self.w = torch.zeros((num_parameters, 1), requires_grad=True, dtype=torch.float64)\n        self.num_parameters = num_parameters\n\n    def forward(self, X, cuda=False):\n        prod = torch.mm(X, self.w)\n        return torch.sigmoid(prod)\n\n    def predict(self, X):\n        return torch.sigmoid(torch.mm(X, self.w))\n\n    def update_model(self, lr):    \n        # Update parameters\n        gradients = self.w.grad.data\n        with torch.no_grad():\n            for idx in range(self.num_parameters):\n                self.w[idx].data -= lr * gradients[idx]\n\ndef loss_function(output, t, w):\n    t_float = t.type(torch.DoubleTensor)\n    likelihood = t_float * output + (1 - t_float) * (1 - output)\n    \n    const = float((1. / np.sqrt((2 * np.pi))))\n    prior =  torch.log(const * torch.exp(- w**2 / 2)).sum()\n    likel = torch.log(likelihood).sum()\n    return - (likel + prior)\n\ndef loss_function_weighted(output, t, weights):\n    t_float = t.type(torch.DoubleTensor)\n    likelihood = weights[1] * t_float * output + weights[0] * (1 - t_float) * (1 - output)\n    \n    const = float((1. / np.sqrt((2 * np.pi))))\n    prior =  torch.log(const * torch.exp(- w**2 / 2)).sum()\n    likel = torch.log(likelihood).sum()\n    return - (likel + prior)\n\ndef threshold_cross_validation(X_train, y_train, metric_f, iterations, lr=1e-8, n=1, k=3, verbose=False):\n    \"\"\"\n    Cross validation of the threeshold for logistic regression\n    \"\"\"\n    treshoold_linspace = np.linspace(0, 1, 21)[1:-1] #no sense to take extremis\n    \n    # Utility variables\n    fold = list(range(k))\n    # batch size\n    bs = len(X_train) // 10\n    tot_len = len(X_train)\n\n    maximum_metric = 0.\n    \n    \n    best_thresholds = []\n\n    # Split in k fold\n    for i in fold:\n        X_validation = X_train[i * bs : (i + 1) * bs]\n        X_validation_pow = get_X(X_validation, n)\n        X_train_fold = np.concatenate((X_train[0 : i * bs], X_train[(i + 1) * bs : tot_len]))\n        \n        y_validation = y_train[i * bs : (i + 1) * bs]\n        y_train_fold = np.concatenate((y_train[0 : i * bs], y_train[(i + 1) * bs : tot_len]))\n        \n        model, _ = train(iterations, n, X_train_fold, y_train_fold, X_validation, y_validation, lr=lr,verbose=verbose)\n    \n        X_validation_torch, y_validation_torch = torch.from_numpy(X_validation_pow), torch.from_numpy(y_validation)\n        y_pred_torch = model.predict(X_validation_torch)\n        y_pred_undiscretized = y_pred_torch.data.numpy()\n        \n        best_thr = 0.\n        maximum_metric = 0.\n\n        for thr in treshoold_linspace:\n            y_pred = discretize(y_pred_undiscretized, thr)\n\n            metric = metric_f(y_validation, y_pred)\n            if metric > maximum_metric:\n                maximum_metric = metric\n                best_thr = thr\n                \n        best_thresholds.append(best_thr)\n        print(\"best threshold fold {}: {}\".format(i, best_thr))\n    return np.median(best_thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.b Gradient Descent"},{"metadata":{},"cell_type":"markdown","source":"The learning rate suggested in the method, is the one best suited for dimension 1. In the code below there is no explicit cross validation or showing the effectiveness of this values. What I have experienced is that using an higher learning rate leads to explosion gradient wheter using a lower one leads to underfitting and slow loss decreasing."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(iterations, n, X_train, y_train, X_val, y_val, lr=1e-8, verbose=False):\n\n    X_train_pow = get_X(X_train, n)\n    X_test_pow = get_X(X_test, n)\n\n    X_train_torch, X_test_torch = torch.from_numpy(X_train_pow), torch.from_numpy(X_test_pow)\n    y_train_torch, y_test_torch = torch.from_numpy(y_train), torch.from_numpy(y_test)\n\n    losses = []\n    num_of_parameters = 200 * n + 1\n\n    model = BayesianLogisticRegression(num_of_parameters)\n\n    # X_train_torch, X_test_torch, y_train_torch, y_test_torch, w = X_train_torch.to('cuda'),X_test_torch.to('cuda'),y_train_torch.to('cuda'),y_test_torch.to('cuda'), w.to('cuda')\n    for i in range(iterations):\n\n        output = model.forward(X_train_torch, y_train_torch)\n\n        loss = loss_function(output, y_train_torch, model.w)\n\n        loss.backward()\n\n        model.update_model(lr)\n\n        losses.append(loss.item())\n\n        model.w.grad.data.zero_()\n        if verbose and iterations > 10 and i % (iterations // 10) == 0:\n            print(\"Loss after another 10% of total iterations\", loss.item())\n    print(\"--Training Completed--\")\n    return model, losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can train the model on the entire dataset\nmodel, losses = train(12, 1, X_train, y_train, X_test, y_test, lr=1e-8, verbose=False)\n# Testing with 0.5 as threshold\nX_test_torch = torch.from_numpy(get_X(X_test, 1))\n\ny_pred_torch = model.predict(X_test_torch)\ny_pred_undiscretized = y_pred_torch.data.numpy()\ny_pred = discretize(y_pred_undiscretized, 0.5)\n\nprint(\"Metrics for 0.5 as threshold\")\nprint_metrics(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting losses\nfig, ax = plt.subplots(ncols=2, figsize=(15, 3))\n\nax[0].plot(losses)\nax[0].set_title('All iterations')\n\nax[1].plot(losses[30:])\nax[1].set_title('Losses after 30 iterations')\n\nfor axis in ax:\n    axis.set_xlabel('Iterations')\n    axis.set_ylabel('loss values')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first iterations are the most sensitive regarding the gradient, since it explodes if the learning rates is too high"},{"metadata":{},"cell_type":"markdown","source":"#### 4.c Convexity of the problem\n- $ log(sigmoid(w^{T}x))$ it is concave\n- $ - log(sigmoid(w^{T}x))$, which can be used as the loss function is convex\n\nNot necessary to do multiple restarts."},{"metadata":{},"cell_type":"markdown","source":"#### 4.d Comparison with Bayesian Linear Regression Classifier\nFirst of all a cross validation on the Threshold is performed also for logistic regression. Basically, the output of a logistic regression classifier is a probability, se you can interpret the threshold as: *I consider a sample to be a fraud if the probability is above 30%*. Expecially in imbalanced dataset, it could lead to a better performance within certain metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximize the f1 score\nmetric_f = lambda true, pred : f1_score(true, pred)\n\nbthr = threshold_cross_validation(X_train, y_train, metric_f, 12, k=5, verbose=False) # lr=1e-8, n=1\n\nprint(\"Best threshold obtained: {}\".format(bthr))\n\ny_pred_torch = model.predict(X_test_torch)\ny_pred_undiscretized = y_pred_torch.data.numpy()\ny_pred = discretize(y_pred_undiscretized, bthr)\n\nprint_metrics(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This operation has been done since we want to maximize the *f1 score*. In addiction to that, the loss function **do not** encode this willing! An alternative could ne weighted loss function, but after several tests I did not obtained good results compared to this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(15, 3))\n\nax[0].plot(losses)\nax[1].plot(losses[20:])\n\nfor axis in ax:\n    axis.set_xlabel('Iterations')\n    axis.set_ylabel('loss values')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"N.B. Metrics of Logistic regression above\n\nWith those parameters, the Bayesian Linear regression discretize achieves better results. An hypotesis is that Bayesian Logistic regression with MAP requires more parameters tuning, and above is done just with N-th 1 power, which is not completely far to compar with a Bayesian Logistic regression with N-th 3 power.\n\nThe computation is consistently more expensive on Bayesian Logistic regression."},{"metadata":{},"cell_type":"markdown","source":"#### 4.e Laplace Approximation"},{"metadata":{},"cell_type":"markdown","source":"Derivative Calculation\n\nThe steps are the ones in the following list (directly from ASI slides):\n- find $ \\mu = \\hat{w} $ that maximises $g(w; X, t, \\sigma^2)$, so we take the one we obtained in the Bayesian Logisti linear regression\n- find:  $$\\Sigma^{-1} = - \\frac{\\partial^2 log(g(w;X,t,\\sigma^2))}{\\partial w \\partial w^T}\\biggr\\rvert_{\\hat{w}}$$\n\n    \n\nSo if we calculate those two elements, then it is easy to sample from $\\mathcal{N}(\\mu, \\Sigma)$. Therefore, you an approximate the mean throught Sampling.\n\n$$E_{\\mathcal{N}(\\mu, \\Sigma)}[P(T_{new} = 1 | x_{new}, w] \\approx \\frac{1}{S} \\sum_{s=1}^S{\\frac{1}{1 + exp(-w^T x_{new})}}$$\n\nRight now, we need to calculate $\\frac{\\partial^2 log(g(w;X,t,\\sigma^2))}{\\partial w \\partial w^T}$, to do it we proceed element-wise and then generalizing to the N-th case"},{"metadata":{},"cell_type":"markdown","source":"The considered likelihood is the one above (the prior term will be added afterwars):\n-  $\\prod{p(t_n=1|x_n,w)^{t_n}  p(t_n=0|x_n,w) ^{(1 - t_n)}}$\n\n\n- first derivative:\n$$\\frac{\\partial log(g(w;X,t,\\sigma^2))}{\\partial w_d} = \\frac{\\partial \\sum_n^N{t_n log(sigm(w^T x^{(n)})) + (1 - t_n) log(1 - sigm(w^T x^{(n)}))} }{\\partial w_d} -\\sum_i^D{\\frac{1}{2} \\frac{\\partial (\\frac{w_i^2}{\\sigma_i^2})}{\\partial w_i}} $$\n$$\\frac{\\partial log(g(w;X,t,\\sigma^2))}{\\partial w_d} = \\sum_n^N{x_d^N (t_n - sigm(w^T x^{(n)})} + \\frac{w_d}{\\sigma_d^2}$$\n\n- second derivative:\n\n$$\\frac{\\partial log(g(w;X,t,\\sigma^2))}{\\partial w_d \\partial w_e} = \\sum_n^N{x_d^{(n)} x_d^{(e)} sigm(w^T x^{(n)}) (sigm(w^T x^{(n)}) - 1)} + \\frac{\\delta_{d,e}}{\\sigma^2} I$$\n\n- prior term second derivative with $\\sigma_i=1$ for all: $- I$\n\nMatrix form (equivalent version manually reconstructed)\n$$ Sigma = -(X^T (X \\odot (sigm \\mathbb{1}) \\odot (sigm \\mathbb{1} - I) - I)^{-1}$$"},{"metadata":{},"cell_type":"markdown","source":"##### 5. Bonus Question"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can train the model on the entire dataset\nmodel, _ = train(12, 1, X_train, y_train, X_test, y_test, lr=1e-8, verbose=True)\n\n# Testing with 0.5 as threshold\n\ny_pred_torch = model.predict(X_test_torch)\ny_pred_undiscretized = y_pred_torch.data.numpy()\ny_pred = discretize(y_pred_undiscretized, 0.5)\n\nprint(\"Metrics for 0.5 as threshold\")\nprint_metrics(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = model.w.data.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Sigma_laplace(X, w):\n    return np.dot(X.T, np.multiply(X, np.dot(sigmoid(np.dot(X, w)) - 1, np.ones((1, len(w)))))) - X.shape[0] * np.eye(w.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_w_b = get_X(X_test) # X_test\n\nnum_samples = 10\n\ndistr = np.random.multivariate_normal(np.squeeze(np.asarray(mean_w)), cov, num_samples)\n\ny_storage_test = np.zeros((X_test_w_b.shape[0], 1))\n\nfor w in distr:\n    w = np.matrix(w).T\n    y_pred_und = sigmoid(np.dot(X_test_w_b, w))\n    \n    y_pred = discretize(y_pred_und, 0.2)\n    \n    y_storage_test += y_pred_und\n\ny_pred = discretize(y_storage_test / num_samples, 0.2)\n\nprint(\"Laplace metrics\")\nprint_metrics(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Performance are similar respect the ones with Bayesian Logistic Regression. The laplace approximation is not a method to provide the highest likelihood, but to provide some information respect to the variance."},{"metadata":{},"cell_type":"markdown","source":"##### Addictional test with ADAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_optimizer(iterations, n, X_train, y_train, X_val, y_val, lr=1e-8, verbose=False):\n\n    X_train_pow = get_X(X_train, n)\n    X_test_pow = get_X(X_test, n)\n\n    X_train_torch, X_test_torch = torch.from_numpy(X_train_pow), torch.from_numpy(X_test_pow)\n    y_train_torch, y_test_torch = torch.from_numpy(y_train), torch.from_numpy(y_test)\n\n    losses = []\n    num_of_parameters = 200 * n + 1\n\n    model = BayesianLogisticRegression(num_of_parameters)\n    \n    optimizer = torch.optim.Adam([model.w], lr=lr)\n\n    # X_train_torch, X_test_torch, y_train_torch, y_test_torch, w = X_train_torch.to('cuda'),X_test_torch.to('cuda'),y_train_torch.to('cuda'),y_test_torch.to('cuda'), w.to('cuda')\n    for i in range(iterations):\n        optimizer.zero_grad()\n    \n        output = model.forward(X_train_torch, y_train_torch)\n\n        loss = loss_function(output, y_train_torch, model.w)\n\n        loss.backward()\n\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        model.w.grad.data.zero_()\n        if verbose and iterations > 10 and i % (iterations // 10) == 0:\n            print(\"Loss after another 10% of total iterations\", loss.item())\n    print(\"--Training Completed--\")\n    return model, losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7000\nmodel, losses = train_optimizer(12, 2, X_train, y_train, X_test, y_test, lr=1e-4, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_torch = torch.from_numpy(get_X(X_test, 2))\ny_pred_und = model.predict(X_test_torch).detach().numpy()\n\ny_pred = discretize(y_pred_und, 0.2)\n\nprint_metrics(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using ADAM optimizer and the best threshold it possible to obtain a compared performance respect to the Bayesian Logistic regression"},{"metadata":{},"cell_type":"markdown","source":"#### Addictional materials"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert\nX_train_pow = get_X(X_train, N)\nX_test_pow = get_X(X_test, N)\n\nX_train_torch, X_test_torch = torch.from_numpy(X_train_pow), torch.from_numpy(X_test_pow)\ny_train_torch, y_test_torch = torch.from_numpy(y_train), torch.from_numpy(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest = test.set_index('ID_code')\n\nX_testd_matrix = np.matrix(test.values)\n\ny_testd_pred = blr.predict(X_testd_matrix)\n\ny_testd_pred_discretized = discretize(y_testd_pred, 0.28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test.index)\nsubmission['target'] = y_testd_pred_discretized\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}