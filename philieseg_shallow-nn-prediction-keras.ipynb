{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import Packages\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load data (Notebook set up in Kaggle) \n\ntrain = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract Features that have high correlations with the target\n# https://www.kaggle.com/wwu651/auto-feature-engineering-lgb\n\nfeatures=[]\ncor=[]\nfor feature in train.iloc[:,2:].columns:\n    if (train['target'].corr(train[feature])>0.05)|(train['target'].corr(train[feature])<-0.05):\n        features.append(feature)\n        cor.append(train['target'].corr(train[feature]))\n\ndf_corr=pd.DataFrame({'Features': features,'Correlations':cor}).sort_values(by='Correlations').set_index('Features')\n\ndf_corr.plot(kind='barh',figsize=(10,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Feature engineering with featuretools package\n# #not used since it did not increase AUC\n\n# import featuretools as ft\n\n# features.append('ID_code')\n\n# es = ft.EntitySet(id=\"santander\")\n\n# es = es.entity_from_dataframe(entity_id=\"santander\",\n#                               dataframe=train[features],\n#                                index=\"ID_code\")\n\n# feature_matrix, feature_defs = ft.dfs(entityset=es,\n#                                        target_entity=\"santander\",\n# #                                       agg_primitives=[\"skew\", \"std\"],\n#                                       trans_primitives=['multiply_numeric','add_numeric'],\n#                                        max_depth=1)\n\n# es_test = es.entity_from_dataframe(dataframe=test[features],\n#                                              entity_id='test',\n#                                              index='ID_code')\n\n# feature_matrix_test, feature_defs_test = ft.dfs(entityset=es_test, \n#                                                  target_entity='test',\n#                                                  trans_primitives=['multiply_numeric','add_numeric'],\n#                                                  max_depth=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Merge new features on datasets \n\n# feature_matrix = feature_matrix[feature_matrix.columns[~feature_matrix.columns.isin(train.columns)]].reset_index()\n# train = pd.merge(train, feature_matrix, on='ID_code', how='left')\n\n# feature_matrix_test = feature_matrix_test[feature_matrix_test.columns[~feature_matrix_test.columns.isin(test.columns)]].reset_index()\n# test = pd.merge(test, feature_matrix_test, on='ID_code', how='left')\n\n# #Clear up RAM\n# del feature_matrix, feature_defs, feature_matrix_test, feature_defs_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor col in train.columns[~train.columns.isin(['target', 'ID_code'])]:\n    train[col].hist(histtype=u'step')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standard scale all variables except ID_code and the target\n# Create Train and Test sets \n\n#Splitting train set in train and validation (validation = 0.2)\n\nfrom sklearn.model_selection import train_test_split\n\nX = train[train.columns[~train.columns.isin(['target', 'ID_code'])]]\ny = train[['target']]\n\nscale = StandardScaler().fit(X)\nX_scaled = pd.DataFrame(scale.transform(X))\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.2, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor col in X_scaled.columns:\n    X_scaled[col].hist(histtype=u'step')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Upsample the minority class\n# # SMOTE generates random noise in the added columns of the minority class\n\n# from imblearn.over_sampling import SMOTE\n# from imblearn.under_sampling import RandomUnderSampler\n# import plotly.graph_objects as go\n\n# labels = [\"Non Subscriber - 0\", \"Subscriber - 1\"]\n# values = y_train.target.value_counts()\n\n# print(X_train.shape, y_train.shape)\n# print(y_train.target.value_counts())\n# donut = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5, marker_colors=[\"rgb(153, 214, 255)\", \"rgb(0, 92, 153)\"])])\n# donut.show()\n\n# #Apply oversampling with random noise in upsampled observations\n# over = SMOTE(\"auto\")\n# X_train, y_train = over.fit_sample(X_train, y_train)\n\n# labels = [\"Non Subscriber - 0\", \"Subscriber - 1\"]\n# values = y_train.target.value_counts()\n\n# donut = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5, marker_colors=[\"rgb(153, 214, 255)\", \"rgb(0, 92, 153)\"])])\n# donut.show()\n# print(X_train.shape, y_train.shape)\n# print(y_train.target.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up Gaussian Naive Bayes Classifier\n\nfrom sklearn.metrics         import accuracy_score, auc, f1_score, precision_recall_curve, roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.naive_bayes     import GaussianNB, BernoulliNB\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.tree            import DecisionTreeClassifier\nfrom sklearn.ensemble        import RandomForestClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\n\nbayes        = GaussianNB()\nlogistic     = LogisticRegression()\nrandomForest = RandomForestClassifier()\ngboost       = GradientBoostingClassifier()\nbernoulli    = BernoulliNB()\ntree         = DecisionTreeClassifier()\n\nmodels = {\n#           \"logistic\"     :logistic,\n#           \"logistic_1\"   :logistic,\n#           \"ada\"          :ada,\n#           \"knn\"          :knn,\n#           \"lda\"          : lda,\n#           \"lda_svd\"      : lda,\n            \"bayes\"        :bayes\n#             \"bernoulli\": bernoulli\n#           \"randomForest\" :randomForest\n#           \"xgboost_tree\" :xgboost,\n#           \"xgboost_lin\" :xgboost,\n#           \"xgboost_dart\" :xgboost,\n#           #\"svc\"          : svc,\n#           \"gboost\"       :gboost,\n#           \"tree\"         :tree\n         }\n\n# Iniatiate Grid Values to loop through in grid search\ngrid_values = {\n    \"logistic\":{\"solver\":[\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"],'penalty': ['l2'],'C':[0.1, 1, 5, 50], \n                \"max_iter\":[300, 90, 100, 150], \"class_weight\":[None]},\n    \"logistic_1\":{\"solver\":[\"liblinear\"],'penalty': ['l1'],'C':[0.2,0, 1, 0.5,3], \"max_iter\":[300, 90, 100, 150], \n                  \"class_weight\":[None]},\n    \"ada\":{\"n_estimators\":[5,6]},\n    \"knn\":{\"n_neighbors\":[150, 200,100, 250,300,500], \"weights\":[\"uniform\"], \"leaf_size\":[50,100,300], \"metric\":[\"manhattan\", \"euclidean\", \"minkowski\"]},\n\n    \"randomForest\":{\"max_depth\":[5], \"n_estimators\":[300], \"class_weight\":[None], \n                    \"max_features\":[\"auto\"]},\n    \"lda\":{\"solver\":[\"lsqr\",\"eigen\"], \"shrinkage\":[None, \"auto\"]},\n    \"bayes\":{},\n    \"bernoulli\":{},\n    \"cat_nb\":{},\n    \"lda_svd\":{\"solver\":[\"svd\"]},\n    \"gboost\":{\"loss\":[\"deviance\", \"exponential\"], \"n_estimators\":[100,200,300], \"learning_rate\":[0.003, 0.001]},\n    \"xgboost_tree\":{\"booster\":[\"gbtree\"], \"eta\":[0.1,0.2], \"max_depth\":[2,3,4]},\n    \"xgboost_lin\":{\"booster\":[\"gblinear\"], \"lambda\":[0, 0.8,0.2,1], \"feature_selector\":[\"cyclic\",\"shuffle\"], \n                   \"top_k\":[8,10,15,20,7]},\n    \"xgboost_dart\":{\"booster\":[\"dart\"], \"skip_drop\":[0.05,0.03,0.01], \"sample_type\":[\"uniform\", \"weighted\"], \n                    \"rate_drop\":[0.3,0.4,0.5], \"normalize_type\":[\"tree\", \"forest\"]},\n    \"svc\":{'C':[0.001,0.01,1,5,2], \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \"degree\":[2,3,4,5]},\n    \"tree\": {\"max_depth\": [3, 5, 4], \"max_features\":[None, \"auto\"], \"class_weight\":[None]}\n              }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run GridSearch and print best parameters, Confusion Matrix and AUC\n\n#Initiate empty dataframe to store model results\noverview = pd.DataFrame()\n\n#loop through models in models dictionary\nfor model in models:\n    \n    search_grid = grid_values[model]\n    \n    #grid search parameters in grid_values\n    #scoring is based on roc_auc -> outcome of grid_clf_acc is best model from grid search\n    grid_clf_acc = GridSearchCV(models[model], param_grid = search_grid, scoring = 'roc_auc')\n    grid_clf_acc.fit(X_scaled, y)\n\n    #Predict values for validation set\n    y_pred = grid_clf_acc.predict(X_valid)\n    \n    proba_train   = pd.DataFrame(grid_clf_acc.predict_proba(X_train))[1]\n    auc_train     = roc_auc_score(np.array(y_train),np.array(proba_train))\n    \n    #compute evaluation metrics\n    probabilities = pd.DataFrame(grid_clf_acc.predict_proba(X_valid))[1]\n    auc           = roc_auc_score(np.array(y_valid),np.array(probabilities))\n    acc           = accuracy_score(y_valid,y_pred)\n    f1            = f1_score(y_valid,y_pred)\n    prec_recall   = precision_recall_curve(y_valid,y_pred)\n    \n    print(\"\\n\" , model, \"\\t\", \"AUC:\", auc, grid_clf_acc.best_params_)\n    \n    #add model metrics to result dataframe\n    overview[model] = [auc, auc_train, acc, f1, prec_recall, grid_clf_acc.best_params_]\n    \n    #print confusion matrix\n    cmtx = pd.DataFrame(\n    confusion_matrix(y_valid, y_pred), \n    index=['true:no', 'true:yes'], \n    columns=['pred:no', 'pred:yes'])\n\n    print(cmtx)\n    \n    #print histogram of predicted probabilities\n    probabilities.hist()\n    plt.show()\n\n#set index for result dataframe\noverview.index = [\"AUC\", \"AUC_Train\", \"Accuracy\", \"F1 Score\", \"Precision_Recall\", \"best params\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Neural Network \n\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\nimport tensorflow as tf\n\ncheckpoint = ModelCheckpoint(filepath = \"weights.hdf5\", verbose=1, save_best_only=True)\ncallback = EarlyStopping(monitor=\"val_auc\", patience=15, verbose=0, mode='max')\n\ndef get_model(activation = \"relu\"):\n    \n    model = Sequential()\n    model.add(Dense(512, activation='relu', input_dim = len(X_train.columns)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    model.add(Dense(512, activation='sigmoid'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation = \"sigmoid\"))\n    \n    loss = 'binary_crossentropy'\n    optimizer = tf.keras.optimizers.Adam(lr=0.00002)\n#     optimizer= 'adam'\n    metrics = [tf.keras.metrics.AUC()]\n    \n    \n    model.compile(loss=loss, optimizer= optimizer, metrics=metrics)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run Neural Network and predict on validation set (optimized towards AUC)\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics         import roc_auc_score\nfrom sklearn.metrics         import confusion_matrix\n\n#get new model from function\nnn           = KerasClassifier(build_fn=get_model)\n\n#fit model on scaled data (full) and use 20% as validation data\nhistory = nn.fit(X_scaled, y, batch_size = 128, validation_split=.2, epochs = 50, callbacks = [checkpoint, callback]) \n\n#calculate AUC and cufusion matrix\nproba = pd.DataFrame(nn.predict_proba(X_valid))[1]\ny_pred = nn.predict(X_valid)\n\nauc           = roc_auc_score(np.array(y_valid),np.array(proba))\n\nprint(auc)\n\ncmtx = pd.DataFrame(\nconfusion_matrix(np.array(y_valid), np.array(y_pred)), \nindex=['true:yes', 'true:no'], \ncolumns=['pred:yes', 'pred:no'])\n\nprint(cmtx, \"\\n\", auc)\n\n#Print training performance\nimport matplotlib.pyplot as plt\ntry:\n    # Plot training & validation accuracy values\n    plt.plot(history.history[list(history.history.keys())[1]])\n    plt.plot(history.history[list(history.history.keys())[3]])\n    plt.title('Model AUC')\n    plt.ylabel('AUC')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.show()\nexcept:\n    next","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()\n\nmodel.load_weights('/kaggle/working/weights.hdf5')\n\nmodel.fit(X_scaled, y, batch_size = 128, validation_split=.2, epochs = 10, callbacks = [checkpoint, callback]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new model and load weights (lowest loss in training)\nmodel = get_model()\n\nmodel.load_weights('/kaggle/working/weights.hdf5')\nmodel.evaluate(X_valid, y_valid)\n\nproba = pd.DataFrame(model.predict_proba(X_valid))[0]\n\ny_pred = model.predict(X_valid)\n\n#Check AUC for model with loaded weights and full model on validation set from bayes classifier \n#Note: this step only compares the two AUC scores, the data is likely to be included in the train data and therefore not used for model selection / validation\nauc           = roc_auc_score(np.array(y_valid),np.array(proba))\nprint('Loaded Weights:', auc)\n\nproba = pd.DataFrame(nn.predict_proba(X_valid))[1]\nauc           = roc_auc_score(np.array(y_valid),np.array(proba))\nprint('Full Model:', auc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale test data and predict test data\nX_test = test[test.columns[~test.columns.isin(['ID_code'])]]\n\nscale = StandardScaler().fit(X_test)\ntest_scaled = pd.DataFrame(scale.transform(X_test))\n\ntest_pred = pd.DataFrame(model.predict_proba(test_scaled))[0] #Neural Net AUC 0.87\ntest_pred1 = pd.DataFrame(grid_clf_acc.predict_proba(test_scaled))[1] #Bayes AUC 0.89\n\nsubmission = test[['ID_code']]\nsubmission['target'] = np.array(test_pred)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission1 = test[['ID_code']]\nsubmission1['target'] = np.array(test_pred1)\nsubmission1.to_csv(\"submission1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.target.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}