{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport datetime\nimport warnings\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing seaborn and matplotlabsib\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading the data - creating as dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts().plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Standardizing / normalizing the data using sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the sklearn package\n\nimport sklearn\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = train.drop(['ID_code', 'target'], axis=1)\ntarget = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing = test.drop('ID_code', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling the training data\ntraining_scaled = scaler.fit_transform(training)\n\n# scaling the testing data\ntesting_scaled = scaler.fit_transform(testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the training data: {}\".format(training_scaled.shape))\nprint(\"Shape of the testing data: {}\".format(testing_scaled.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling the garbage collector\nimport gc\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the Model using keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing packages\n\nimport keras\nfrom keras import layers\nfrom keras import utils\nfrom keras import models\n\nfrom keras.layers.core import (Dense, Activation, Flatten, Dropout)\nfrom keras.layers import BatchNormalization\nfrom keras.models import (Sequential, Model)\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the misc., properties\n\nBATCH_SIZE = 64\nNP_EPOCHS = 50\nNP_CLASSES = 1\nVERBOSE = 1\nVALIDATION_SPLIT = 0.2\n\n# taking the instance as RMSProp\noptimizer = optimizers.RMSprop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the generator the generator code has been taken from https://www.kaggle.com/mathormad/knowledge-distillation-with-nn-rankgauss\n\ndef mixup_data(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n\n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n    return mixed_x, mixed_y\n\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size / float(batch_size)))\n    return [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X, y, batch_size=128, shuffle=True, mixup=False):\n    y = np.array(y)\n    sample_size = X.shape[0]\n    indexed = np.arange(sample_size)\n\n    while True:\n        if shuffle:\n            np.random.shuffle(indexed)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = indexed[batch_start:batch_end]\n            x_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n\n            if mixup:\n                x_batch, y_batch = mixup_data(x_batch, y_batch, alpha=1.0)\n            yield x_batch, y_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the checkpoints\n\nfrom keras.callbacks import (ModelCheckpoint, ReduceLROnPlateau, EarlyStopping)\nfrom keras.layers import LeakyReLU\n\nNP_OUTPUT_FUNCTION = \"sigmoid\"\nDROPOUT_FIRST = 0.25\nDROPOUT_SECOND = 0.20\nNP_INPUT_SHAPE = training_scaled.shape[1]\n\nprint(\"Input shape has been taken as: {}\".format(NP_INPUT_SHAPE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building the model\n\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(NP_INPUT_SHAPE,)))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_FIRST))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_FIRST))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(64))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(32))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(16))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(8))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\nmodel.add(Activation(NP_OUTPUT_FUNCTION))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the summary of the model to see the trainable and non trainable parameters\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncurrent_dt_time = datetime.datetime.now()\nmodel_name = 'model_init' + '_' + str(current_dt_time).replace(' ', '').replace(':', '_') + '/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n    \nfile_path = model_name + \"model-{epoch:05d}-{loss:.5f}-{val_auc:.5f}-{val_loss:.5f}-{val_auc:.5f}.h5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lrt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating call back methods\n\n## Call back method to calculate ROC AUC - We can found optins from: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n## We can calculate the ROC AUC for mini batches - so we can calculate the ROC AUC score at the end of each epoch by using callbacks method\n     \nfrom keras.callbacks import Callback\nfrom sklearn import metrics\n\n\nclass findROC(Callback):\n    def __init__(self, etraining, evalidation):\n        # for training\n        self.x_train = etraining[0]\n        self.y_train = etraining[1]\n\n        # for validation\n        self.x_val = evalidation[0]\n        self.y_val = evalidation[1]\n\n    def on_train_begin(self, logs=None):\n        return\n\n    def on_batch_end(self, batch, logs=None):\n        return\n\n    def on_epoch_begin(self, epoch, logs=None):\n        return\n\n    def on_epoch_end(self, epoch, logs=None):\n        y_pred_training = self.model.predict(self.x_train)\n        roc_score_training = metrics.roc_auc_score(self.y_train, y_pred_training)\n\n        y_pred_validation = self.model.predict(self.x_val)\n        roc_score_validation = metrics.roc_auc_score(self.y_val, y_pred_validation)\n\n        print(\"Training RoC score found: {}, validation RoC score found: {}\".format(roc_score_training,\n                                                                                    roc_score_validation))\n        return\n    \n    def on_batch_begin(self, batch, logs=None):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating model checkpoint\ncheckpoint = ModelCheckpoint(filepath=file_path, \n                             monitor='val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             save_weights_only=False, \n                             mode='auto', \n                             period=1)\n\n# early stopping\nearly = EarlyStopping(monitor='val_loss',\n                      mode='auto',\n                      patience=5,\n                      verbose=1)\n\n\nLR = ReduceLROnPlateau(monitor=\"val_loss\",\n                       factor=0.2,\n                       patience=2,\n                       min_lr=0.000001,\n                       verbose=1,\n                       cooldown=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC & AUC metric to monitor\nimport tensorflow as tf\nimport keras.backend as B\n\ndef auc(y_true, y_pred):\n    score = tf.metrics.auc(y_true, y_pred)[1]\n    B.get_session().run(tf.local_variables_initializer())\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the optimizer\n\noptimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n# optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compiling the model\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', auc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data into train and test using sklearn library\nfrom sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(training_scaled, target, test_size=0.20, random_state=123456789)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X training data shape: {}\".format(X_train.shape))\nprint(\"y training data shape: {}\".format(y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X validation data shape: {}\".format(X_val.shape))\nprint(\"y validation data shape: {}\".format(y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the number of training and validation steps per epoch\n\nnum_training_seq = len(X_train)\nnum_validation_seq = len(X_val)\n\nprint(\"# training sequences: {}\".format(num_training_seq))\nprint(\"# validation sequences: {}\".format(num_validation_seq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if(num_training_seq % BATCH_SIZE) == 0:\n    training_steps_per_epoch = int(num_training_seq / BATCH_SIZE)\nelse:\n    training_steps_per_epoch = int(num_training_seq / BATCH_SIZE) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if(num_validation_seq % BATCH_SIZE) == 0:\n    validation_steps_per_epoch = int(num_validation_seq / BATCH_SIZE)\nelse:\n    validation_steps_per_epoch = int(num_validation_seq / BATCH_SIZE) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of training steps are required for epoch: {}\".format(training_steps_per_epoch))\nprint(\"Number of validation steps are requried for epoch: {}\".format(validation_steps_per_epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the model\n\nhistory = model.fit_generator(generator=batch_generator(X_train, y_train, BATCH_SIZE),\n                             validation_data=batch_generator(X_val, y_val, BATCH_SIZE),\n                             epochs=NP_EPOCHS,\n                             verbose=VERBOSE,\n                             steps_per_epoch=training_steps_per_epoch,\n                             validation_steps=validation_steps_per_epoch,\n                             class_weight=None,\n                             initial_epoch=0,\n                             # callbacks=[checkpoint, early, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])\n                             callbacks=[checkpoint, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}