{"cells":[{"metadata":{"id":"yo4n6mO1jkPD","colab_type":"text"},"cell_type":"markdown","source":"At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem."},{"metadata":{"id":"AcPyarVFjb1i","colab_type":"text"},"cell_type":"markdown","source":"**Source:**https://www.kaggle.com/c/santander-customer-transaction-prediction"},{"metadata":{"id":"vp_f5TZFj1cZ","colab_type":"text"},"cell_type":"markdown","source":"**Reference:**https://www.kaggle.com/gpreda/santander-eda-and-prediction \\\nhttps://www.kaggle.com/jiweiliu/lgb-2-leaves-augment \\\nhttps://www.kaggle.com/sicongfang/eda-feature-engineering"},{"metadata":{"id":"JaZhN8ZjN59L","colab_type":"text"},"cell_type":"markdown","source":"**metric used = auc(since data is not balanced)**"},{"metadata":{"id":"MEEekvltXPKA","colab_type":"text"},"cell_type":"markdown","source":"# Note: Here i am working only on train data dropping the labels for test as aaic till naive bayes for better visualization"},{"metadata":{"id":"PQZey_DJs2fg","colab_type":"text"},"cell_type":"markdown","source":"**importing the necessary libraries**"},{"metadata":{"id":"GxjhxbfLhnA8","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns            #For plots\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"0iNpbdEphvHf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"n92S9F3GiJbD","colab_type":"code","outputId":"22a8b4b9-10e4-4372-fd42-54b87f9cd3f2","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"V44_06bWtBT4","colab_type":"text"},"cell_type":"markdown","source":"**As we can see from above that train and test, both contains  0.2 million rows.**"},{"metadata":{"id":"x-Cu2ABkh_Uw","colab_type":"code","outputId":"3f58ace2-8ae9-4b49-94fe-9876fc261671","colab":{"base_uri":"https://localhost:8080/","height":253},"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gp51_uIYiEHh","colab_type":"code","outputId":"1776ae87-d46c-4635-ae1c-3eaa417e8fab","colab":{"base_uri":"https://localhost:8080/","height":253},"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"JdvSX3cliTW4","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#target = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"mvCYJGO4iqOc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#train = train.drop([\"target\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"7Yap7bp4j-WF","colab_type":"text"},"cell_type":"markdown","source":"**Checking missing values if any**"},{"metadata":{"id":"aoWX6J1CjIpd","colab_type":"code","outputId":"4be9e7bb-7a99-4b12-fe6b-42c26dcfc880","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"jPVNeG5Ok63H","colab_type":"text"},"cell_type":"markdown","source":"**we can see that there is no null value in train**"},{"metadata":{"id":"cs8YRHQ4kr--","colab_type":"code","outputId":"61857ab8-d6e0-453c-b0f2-7fd3df1579e9","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"f4HAdhy_k021","colab_type":"text"},"cell_type":"markdown","source":"**We can see that there is no null values in test data**"},{"metadata":{"id":"YaXuf3V0lQeO","colab_type":"text"},"cell_type":"markdown","source":"# Lets  describe train"},{"metadata":{"id":"qt3vYqoHlAz-","colab_type":"code","outputId":"ec08fc05-99b7-483d-a086-8bf55db24ccd","colab":{"base_uri":"https://localhost:8080/","height":346},"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"bzKCrbrblhjf","colab_type":"text"},"cell_type":"markdown","source":"# Lets describe test"},{"metadata":{"id":"jnoYYHLllPIP","colab_type":"code","outputId":"2211653b-2b9d-4726-e59d-39123f66d083","colab":{"base_uri":"https://localhost:8080/","height":346},"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"u6TWMw2ptlE9","colab_type":"text"},"cell_type":"markdown","source":"**from this train_vis we are going to visualize  11 features as pair plot because visualizing all at one takes a lot of time**"},{"metadata":{"id":"eOz7EuB5rFih","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_vis = train.iloc[:, 1:13]","execution_count":null,"outputs":[]},{"metadata":{"id":"N47L7QcHuD8x","colab_type":"code","outputId":"f3dc3ed6-0f6d-4da4-fcdd-7a80875d6149","colab":{"base_uri":"https://localhost:8080/","height":111},"trusted":true},"cell_type":"code","source":"train_vis.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"fHTb25ftmRvl","colab_type":"text"},"cell_type":"markdown","source":"# Data Analysis on train data"},{"metadata":{"id":"QhNhqRvyl7PO","colab_type":"code","outputId":"ad61d694-23ef-42bd-bac1-dcc9e3597edc","colab":{"base_uri":"https://localhost:8080/","height":411},"trusted":true},"cell_type":"code","source":"# PROVIDE CITATIONS TO YOUR CODE IF YOU TAKE IT FROM ANOTHER WEBSITE.\n# https://matplotlib.org/gallery/pie_and_polar_charts/pie_and_donut_labels.html#sphx-glr-gallery-pie-and-polar-charts-pie-and-donut-labels-py\n\n\ny_value_counts = train['target'].value_counts()\nprint(\"Number of people transacted the money in future \", y_value_counts[1], \", (\", (y_value_counts[1]/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of people not transacted the money in future  \", y_value_counts[0], \", (\", (y_value_counts[0]/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n#above codes will give the%age of approved and not approved project\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"transacted\", \"not transacted\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Number of people transacted money or not\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4HHaauITnWGv","colab_type":"text"},"cell_type":"markdown","source":"**So from the above plot we can observe that the number of people transected the money os about 10% of the total data only.**\\\n*this is a purely imbalanced data.*"},{"metadata":{"id":"aUfkkSFqtUeB","colab_type":"text"},"cell_type":"markdown","source":"# Visualising with all the features is quite difficult so I am choosing 10 var columns to visualise as pair plot"},{"metadata":{"id":"wNEc_SxLoIdV","colab_type":"text"},"cell_type":"markdown","source":"# PAIR PLOT only for first 10 var_0 to var_10\n**we can visualize relationship between two varioables with this**"},{"metadata":{"id":"xmJzNbMqqwE9","colab_type":"code","outputId":"4be7f4b2-ce27-4fd2-d7e1-bda92ed4f8d3","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.pairplot.html\nplt.close()  #Closing all open window\nsns.set_style(\"whitegrid\");\nsns.pairplot(train_vis, hue=\"target\", height=3);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"_CPM4bgjwffB","colab_type":"text"},"cell_type":"markdown","source":"**from this few features only we can see that both traget is easily seperable using any of the two features.**\\\nalthough the data is imbalanced but easily seperable"},{"metadata":{"id":"Kl5Dq7nQyhsl","colab_type":"text"},"cell_type":"markdown","source":"# Pdf for all features\nhttps://www.kaggle.com/gpreda/santander-eda-and-prediction"},{"metadata":{"id":"kM1_cKOqw4eB","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"id":"Ad3wauDlzDHq","colab_type":"text"},"cell_type":"markdown","source":"# first 100"},{"metadata":{"id":"dy1jsKbG3Gwm","colab_type":"text"},"cell_type":"markdown","source":"**here I am distributing dataset label wise**"},{"metadata":{"id":"9HK5qtiUb8L_","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"JtyKA6cDyliA","colab_type":"code","outputId":"ddcbdc84-46c5-45d0-c4b8-de5c7a6a4a6e","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"features = train.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"id":"9TFKSI7uz962","colab_type":"text"},"cell_type":"markdown","source":"# from features 100 -200"},{"metadata":{"id":"tWIFPxN1z5l6","colab_type":"code","outputId":"0f3193e5-84e2-4baf-8cb0-f34fd7fce477","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"features = train.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"id":"XScDeiHL3Xcf","colab_type":"text"},"cell_type":"markdown","source":"**As we can see from above pdf that there is a lot of different distribution**\\\n**and for most of the data where label=1 and label=0 follows same distribution**\\\n**var_10 ,var_11, var_8,  var_65, var_84  ect. follows same distribution like gaussian**\\\n**var_70, var_60, var_85 ect follows similar distribution**\\\n**var_80, var_86 etc follows similar distribution.**\\\nsimilarly we can see from feature 102 to 202."},{"metadata":{"id":"o6Bte5No0z9Q","colab_type":"text"},"cell_type":"markdown","source":"# Visualising by tsne"},{"metadata":{"id":"pgWdGgzi04DC","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_5000 = train.head(5000)\ny =train_5000[\"target\"]\nx = train_5000.iloc[:,2:202].values","execution_count":null,"outputs":[]},{"metadata":{"id":"T_aUTvKx02tK","colab_type":"code","outputId":"125a078a-da56-4c71-8e5b-7846c7822bca","colab":{"base_uri":"https://localhost:8080/","height":454},"trusted":true},"cell_type":"code","source":"# https://github.com/pavlin-policar/fastTSNE you can try this also, this version is little faster than sklearn \n#reference: aaic tsne\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn import datasets\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n\nX_embedding = tsne.fit_transform(x)\n# if x is a sparse matrix you need to pass it as X_embedding = tsne.fit_transform(x.toarray()) , .toarray() will convert the sparse matrix into dense matrix\n\nfor_tsne = np.vstack((X_embedding.T, y)).T#y.reshape(-1,1)\nfor_tsne_df = pd.DataFrame(data=for_tsne, columns=['Dim_1','Dim_2','label'])\n# Ploting the result of tsne\nsns.FacetGrid(for_tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"Visualise tsne \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"kQgsfgD_27XZ","colab_type":"text"},"cell_type":"markdown","source":"**from the above tsne plot we can see that label 1 is not much seperable when we visualise it in 2d plot**"},{"metadata":{"id":"Al3Zp-A-jOJA","colab_type":"text"},"cell_type":"markdown","source":"# Visualizing mean, median, std, kurtosis, skew, add, min, max, moving average of train and simultaneously doing feature engineering"},{"metadata":{"id":"GMoJp35Q26eS","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"features_train = train.columns.values[2:202]\nfeatures_test = test.columns.values[1:201]\nrow_mean_train = train[features_train].mean(axis=1)\ntrain[\"row_mean\"] =row_mean_train\nrow_mean_test = test[features_test].mean(axis=1)\ntest[\"row_mean\"] = row_mean_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"W8nidTs2f6TN","colab_type":"text"},"cell_type":"markdown","source":"**Pdf gives the probabily of points lying in a certain range**"},{"metadata":{"id":"iqtvCtf3dmnc","colab_type":"code","outputId":"c9df658b-6fb8-416a-f2cf-ba99a2d4495c","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_mean\")\\\n             .add_legend()\nplt.title(\"Histogram of mean\")\nplt.ylabel(\"Density of mean\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"jdNL32yFHiBe","colab_type":"text"},"cell_type":"markdown","source":"**from the above pdf we can say that when mean>6.2 and mean<7 then it is clear that probability of target=1 is high.**"},{"metadata":{"id":"1UE5qSLxsL0m","colab_type":"text"},"cell_type":"markdown","source":"**adding median**"},{"metadata":{"id":"odfslKVEXEtx","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#reference : aaic haberman","execution_count":null,"outputs":[]},{"metadata":{"id":"7KtdXUWUsMDq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_median_train = train[features_train].median(axis=1)\ntrain[\"row_median\"] =row_median_train\nrow_median_test = test[features_test].median(axis=1)\ntest[\"row_median\"] = row_median_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"SJMWLyVns_F4","colab_type":"code","outputId":"1ae199ee-c1b7-414d-e2d3-3ee4773fd312","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_median\")\\\n             .add_legend()\nplt.title(\"Histogram of median\")\nplt.ylabel(\"Density of median\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"ikAhj6NWITAV","colab_type":"text"},"cell_type":"markdown","source":"**from the above pdf we can see that when median>6 and median<7 , the probability of target==1 is high**"},{"metadata":{"id":"RhnW5kSHtKzz","colab_type":"text"},"cell_type":"markdown","source":"**std**"},{"metadata":{"id":"xcK1FAMAtJKX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_std_train = train[features_train].std(axis=1)\ntrain[\"row_std\"] =row_std_train\nrow_std_test = test[features_test].std(axis=1)\ntest[\"row_std\"] = row_std_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HUKaHNZ_tdm2","colab_type":"code","outputId":"6d849570-84b5-4beb-a090-b9b4ee44f1d8","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_std\")\\\n             .add_legend()\nplt.title(\"Histogram of std\")\nplt.ylabel(\"Density of std\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"H7NSHV7kIjzH","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when std>9.2 and std<10.2 probability of target==1 is high.**"},{"metadata":{"id":"p0ToZyuW7oNR","colab_type":"text"},"cell_type":"markdown","source":"**min**"},{"metadata":{"id":"aGUKwymn7tkZ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_min_train = train[features_train].min(axis=1)\ntrain[\"row_min\"] =row_min_train\nrow_min_test = test[features_test].min(axis=1)\ntest[\"row_min\"] = row_min_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"s12QuO7Y7t46","colab_type":"code","outputId":"d5af1b83-5d3d-457a-e2a5-523e3945aac5","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_min\")\\\n             .add_legend()\nplt.title(\"Histogram of min\")\nplt.ylabel(\"Density of min\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"nd8F-9x3I1xm","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when  min<-20 and min>-50 probability of target==1 is high**"},{"metadata":{"id":"pn56PaWn8I8U","colab_type":"text"},"cell_type":"markdown","source":"**max**"},{"metadata":{"id":"9qwdwZWj8JFk","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_max_train = train[features_train].max(axis=1)\ntrain[\"row_max\"] =row_max_train\nrow_max_test = test[features_test].max(axis=1)\ntest[\"row_max\"] = row_max_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"o5OGLg0d8JId","colab_type":"code","outputId":"f6eb04c7-6b41-4cbf-c0cd-31fd7791c875","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_max\")\\\n             .add_legend()\nplt.title(\"Histogram of max\")\nplt.ylabel(\"Density of max\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"DFZFFdPQJLQj","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when  max>35 and min<45 probability of target==1 is high**"},{"metadata":{"id":"E7t7ozof9vab","colab_type":"text"},"cell_type":"markdown","source":"**Skew**"},{"metadata":{"id":"LhqJhciu9vlK","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_skew_train = train[features_train].skew(axis=1)\ntrain[\"row_skew\"] =row_skew_train\nrow_skew_test = test[features_test].skew(axis=1)\ntest[\"row_skew\"] = row_skew_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Bg2h0gqL9voP","colab_type":"code","outputId":"cace41b7-9e1d-4f5e-b6a3-a15b322fbed9","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_skew\")\\\n             .add_legend()\nplt.title(\"Histogram of skew\")\nplt.ylabel(\"Density of skew\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"wvaxmI7e-Jd6","colab_type":"text"},"cell_type":"markdown","source":"**kurtosis**"},{"metadata":{"id":"bAjN2qqV-Jo6","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_kurt_train = train[features_train].kurtosis(axis=1)\ntrain[\"row_kurt\"] =row_kurt_train\nrow_kurt_test = test[features_test].kurtosis(axis=1)\ntest[\"row_kurt\"] = row_kurt_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"k6Sn1q36-Jrz","colab_type":"code","outputId":"3b4b482c-9c51-4c08-bda8-cb2d9552761b","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_kurt\")\\\n             .add_legend()\nplt.title(\"Histogram of kurt\")\nplt.ylabel(\"Density of kurt\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"SSsUTQMTJjC-","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when  kurt>2 and kurt<4 probability of target==1 is high**"},{"metadata":{"id":"bPk-REGH-9MU","colab_type":"text"},"cell_type":"markdown","source":"**sum**"},{"metadata":{"id":"55_agg3_-9Vs","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"row_sum_train = train[features_train].sum(axis=1)\ntrain[\"row_sum\"] =row_sum_train\nrow_sum_test = test[features_test].sum(axis=1)\ntest[\"row_sum\"] = row_sum_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ncN8gMH_-9Y-","colab_type":"code","outputId":"8b3522e5-4846-41a1-d1da-c7b44b0bbbf0","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_sum\")\\\n             .add_legend()\nplt.title(\"Histogram of sum\")\nplt.ylabel(\"Density of sum\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"MV5iVRhaLELD","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when  sum>1250 and sum<1450 probability of target==1 is high**"},{"metadata":{"id":"MEmrGLbOGf0b","colab_type":"text"},"cell_type":"markdown","source":"**moving sum mean**"},{"metadata":{"id":"SZQ6SeZ3GykE","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/hjd810/keras-lgbm-aug-feature-eng-sampling-prediction\nrow_ma_train = train[features_train].apply(lambda x: np.ma.average(x), axis=1)\ntrain[\"ma\"] = row_ma_train\nrow_ma_test = test[features_test].apply(lambda x: np.ma.average(x), axis=1)\ntest[\"ma\"] = row_ma_test\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CEIe27tbGyh0","colab_type":"code","outputId":"02cbf75a-23cf-4490-e689-79613c55df3a","colab":{"base_uri":"https://localhost:8080/","height":399},"trusted":true},"cell_type":"code","source":"#https://seaborn.pydata.org/generated/seaborn.distplot.html\n#https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.average.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"ma\")\\\n             .add_legend()\nplt.title(\"Histogram of ma\")\nplt.ylabel(\"Density of ma\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"wg4XryUKLUB_","colab_type":"text"},"cell_type":"markdown","source":"**it is clear from the above pdf that when  ma>6.2 and ma<7 probability of target==1 is high**"},{"metadata":{"id":"GXBKg34xsKZu","colab_type":"text"},"cell_type":"markdown","source":"**Pdf and cdf using kde(Kernal Distribution Estimation)**"},{"metadata":{"id":"fzsZuA7HrUH_","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"PSdHvyC7rZ-s","colab_type":"code","outputId":"bd25a141-1673-425e-ff2f-197f422b3abc","colab":{"base_uri":"https://localhost:8080/","height":465},"trusted":true},"cell_type":"code","source":"#reference: aaic haberman\ncounts, bin_edges=np.histogram(t0[\"row_mean\"], bins=10, density=True)\npdf=counts/(sum(counts))\nprint(pdf);    #this will return 10 values\nprint(bin_edges);  #this will return 11 values\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label=\"mean0pdf\");\nplt.plot(bin_edges[1:], cdf, label=\"mean0pdf\");\n\ncounts, bin_edges=np.histogram(t1[\"row_mean\"], bins=10, density=True)\npdf=counts/(sum(counts))\nprint(pdf);    #this will return 10 values\nprint(bin_edges);  #this will return 11 values\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label=\"mean1pdf\");\nplt.plot(bin_edges[1:], cdf, label=\"mean1pdf\");\nplt.legend()\nplt.title(\"Pdf & Cdf of year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"percentage\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Utyv3H8hLk9t","colab_type":"text"},"cell_type":"markdown","source":"**from the above pdf and cdf we can say that 90 % of data lie below 7.5**"},{"metadata":{"id":"EX2HsKDstyHf","colab_type":"text"},"cell_type":"markdown","source":"# Box_plot"},{"metadata":{"id":"Q61cDkknuBCz","colab_type":"code","outputId":"f95f726b-c423-4ef0-ebd2-b7e7377588cb","colab":{"base_uri":"https://localhost:8080/","height":312},"trusted":true},"cell_type":"code","source":"#reference aaic haberman\nsns.boxplot(x=\"target\", y=\"row_sum\", data=train)\nplt.title(\"Boxplot for row_sum\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"l7rNkXVmL0m-","colab_type":"text"},"cell_type":"markdown","source":"**distribution according to box plot is also same.**"},{"metadata":{"id":"k-NSthG5t0K6","colab_type":"code","outputId":"5a2844e8-6eb5-48de-ea89-b7d9b9cf6b0f","colab":{"base_uri":"https://localhost:8080/","height":312},"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"target\", y=\"row_mean\", data=train)\nplt.title(\"Boxplot for mean\")\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"F3IjzKpoQL1t","colab_type":"text"},"cell_type":"markdown","source":"# Visualize var_13 to var_17"},{"metadata":{"id":"JwduBbEwQOAj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#create a function which makes the plot:\n#https://www.kaggle.com/sicongfang/eda-feature-engineering\nfrom matplotlib.ticker import FormatStrFormatter\ndef visualize_numeric(ax1, ax2, ax3, df, col, target):\n    #plot histogram:\n    df.hist(column=col,ax=ax1,bins=200)\n    ax1.set_xlabel('Histogram')\n    \n    #plot box-whiskers:\n    df.boxplot(column=col,by=target,ax=ax2)\n    ax2.set_xlabel('Transactions')\n    \n    #plot top 10 counts:\n    cnt = df[col].value_counts().sort_values(ascending=False)\n    cnt.head(10).plot(kind='barh',ax=ax3)\n    ax3.invert_yaxis()  # labels read top-to-bottom\n#     ax3.yaxis.set_major_formatter(FormatStrFormatter('%.2f')) #somehow not working \n    ax3.set_xlabel('Count')","execution_count":null,"outputs":[]},{"metadata":{"id":"ToJi7_3mQOEW","colab_type":"code","outputId":"c8b1cd62-4457-43b6-ec78-696a18c72503","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"##https://www.kaggle.com/sicongfang/eda-feature-engineering\nfor col in list(train.columns[10:20]):\n    fig, axes = plt.subplots(1, 3,figsize=(10,3))\n    ax11 = plt.subplot(1, 3, 1)\n    ax21 = plt.subplot(1, 3, 2)\n    ax31 = plt.subplot(1, 3, 3)\n    fig.suptitle('Feature: %s'%col,fontsize=5)\n    visualize_numeric(ax11,ax21,ax31,train,col,'target')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"h3oQTj8oMDz9","colab_type":"text"},"cell_type":"markdown","source":"**->from the above we can conclude that data follows different distribution**\\\n**->from the boxplot we can assume that for var_11 50% of its values lies with -8 to 0. and for for var_10 50% of its value lie within -5 to 5 and like wise for others we can conclude from boxplot**\\\n**->from the above count plot we can see that maximum number of count of some particular value is variable in nature. **"},{"metadata":{"id":"hejj9a96_ki2","colab_type":"code","outputId":"20f4ac72-b0ef-47d6-977c-02648fe1e9b7","colab":{"base_uri":"https://localhost:8080/","height":160},"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"PruBZJ79NrfH","colab_type":"text"},"cell_type":"markdown","source":"# Now saving all the feature engineered data to train_santander.csv"},{"metadata":{"id":"yKO5eHhnT9C8","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train.to_csv(\"../train_sant.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"1pdWWKKoUi_D","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test.to_csv(\"../test_sant.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"lB_Sp5-MSg-c","colab_type":"text"},"cell_type":"markdown","source":"**importing necessary libraries**"},{"metadata":{"id":"zX5dRQNmw1GG","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KnyuGGHQSmSz","colab_type":"text"},"cell_type":"markdown","source":"**working only on train datasets just to see how well my model is doing.**"},{"metadata":{"id":"ISjGPFZQw1Iv","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../train_sant.csv\")\n#test = pd.read_csv(\"/content/drive/My Drive/test_santander.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"11oG-O0qxbCE","colab_type":"code","outputId":"60fc6dfb-6e82-47b2-826f-36891a46d784","colab":{"base_uri":"https://localhost:8080/","height":173},"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"md02LatmSyAS","colab_type":"text"},"cell_type":"markdown","source":"**as we can see from above that I have successfully added feature engineered features in train data**"},{"metadata":{"id":"rtUkHYTc1p9E","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#target values\ntarget = train[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"id":"yfx5SUav1Xrf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#imp features from column 3 to 212\ntrain = train.iloc[:,3:212]","execution_count":null,"outputs":[]},{"metadata":{"id":"SYBIEL3_KWx_","colab_type":"code","outputId":"6526bdb1-77c5-4c8b-bf36-0851e43fbd20","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"b0YjrrDdLBUs","colab_type":"text"},"cell_type":"markdown","source":"**Dividing train into train and test**"},{"metadata":{"id":"0-G9jSEIItMy","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\ntrain, test, y_train, y_test = train_test_split(train, target, test_size=0.4)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"BoLYGZqRKKCF","colab_type":"code","outputId":"8674d4fd-020b-4d76-fab6-a93702a12929","colab":{"base_uri":"https://localhost:8080/","height":156},"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"20HT4DYjxfj7","colab_type":"code","outputId":"8051c733-200f-43db-a683-53678809b511","colab":{"base_uri":"https://localhost:8080/","height":156},"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"id":"eEpuMZJzZYsL","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() \nscaler = scaler.fit(train) \ntrain = scaler.transform(train)\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"xJ_idodVLQ6a","colab_type":"text"},"cell_type":"markdown","source":"# Now applying different ML algorithms"},{"metadata":{"id":"hJU0h5c8yFyQ","colab_type":"text"},"cell_type":"markdown","source":"# Logistic"},{"metadata":{"id":"Ro7PC7XALX6N","colab_type":"text"},"cell_type":"markdown","source":"**Defining necessary functions.**"},{"metadata":{"id":"D9KbDGA75J9u","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your cr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","execution_count":null,"outputs":[]},{"metadata":{"id":"AYwO-lBi9LsU","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#From facebook recommendation applied this code is taken and modified according to use\nfrom sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    TN = C[0,0]       \n    FP = C[0,1]  \n    FN = C[1,0]\n    TP = C[1,1]\n    print(\"True Positive\",TP)\n    print(\"False Negative\",FN)\n    print(\"False Positive\",FP)\n    print(\"True Negative\",TN)\n    \n    \n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    \n    B =(C/C.sum(axis=0))\n    plt.figure(figsize=(30,6))\n    \n    labels = [0,1]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"Navy\", as_cmap=True)#https://stackoverflow.com/questions/37902459/seaborn-color-palette-as-matplotlib-colormap\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NXJlJXiHJ0Ly","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef predict(proba, threshould, fpr, tpr):\n    \n    t = threshould[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    \n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"-7ts_OeFXOqO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#As mentioned in logistic regression assignment I am changing alpha to log to plot a goog graph\nimport numpy as np\ndef log_alpha(al):\n    alpha=[]\n    for i in al:\n        a=np.log(i)\n        alpha.append(a)\n    return alpha    ","execution_count":null,"outputs":[]},{"metadata":{"id":"4YVLyjAX-mLa","colab_type":"text"},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"id":"A6E9tJo2HBBu","colab_type":"code","outputId":"e9146acb-666f-4801-eab1-7e69866949bc","colab":{"base_uri":"https://localhost:8080/","height":436},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\nlg = SGDClassifier(loss='log', class_weight='balanced', penalty=\"l2\")\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(lg, parameters, cv=3, scoring='roc_auc', n_jobs=-1, return_train_score=True,)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\nalpha = log_alpha(alpha)\n\n\nbest_alpha = clf.best_estimator_.alpha\n#best_split = clf.best_estimator_.min_samples_split\n\nprint(best_alpha)\n#print(best_split)\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"UkbUeVBgLiem","colab_type":"text"},"cell_type":"markdown","source":"**From the above plot it is clearly visible that when alpha=0.0001 we have maximum auc.**"},{"metadata":{"id":"S_Tvf2M7_cFQ","colab_type":"text"},"cell_type":"markdown","source":"# Making final models with best alpha and penalty"},{"metadata":{"id":"QfHKh7inUbMG","colab_type":"code","outputId":"2d63eeb5-3382-437a-da9b-c56df7945997","colab":{"base_uri":"https://localhost:8080/","height":294},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV\n\nlg = SGDClassifier(loss='log', alpha=best_alpha, penalty=\"l2\", class_weight=\"balanced\")\n#lg.fit(train_1, project_data_y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\nsig_clf = CalibratedClassifierCV(lg, method=\"isotonic\")\nlg = sig_clf.fit(train, y_train)\n\n\ny_train_pred = lg.predict_proba(train)[:,1]   \ny_test_pred = lg.predict_proba(test)[:,1] \n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"aaX36LdRL1JU","colab_type":"text"},"cell_type":"markdown","source":"**So the maximum auc here is 0.862**"},{"metadata":{"id":"wteRz6S5A4D5","colab_type":"text"},"cell_type":"markdown","source":"# Confusion Matrix with using map"},{"metadata":{"id":"9cQoIwDRKjt8","colab_type":"code","outputId":"07332908-965b-435a-8822-d22a06d55ce6","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6rWRW2l4QDWy","colab_type":"code","outputId":"0cea4ea8-eb13-45de-ba36-fc76b00a7baf","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"l43w1MVfXxra"},"cell_type":"markdown","source":"# SVM"},{"metadata":{"colab_type":"code","outputId":"72614622-bb0a-40f7-8d6d-4a880d2fd1cb","id":"yQW_pr-EXxrc","colab":{"base_uri":"https://localhost:8080/","height":436},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\nsvm = SGDClassifier(loss='hinge', class_weight='balanced', penalty=\"l2\")\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(svm, parameters, cv=3, scoring='roc_auc', n_jobs=-1, return_train_score=True,)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\nalpha = log_alpha(alpha)\n\n\nbest_alpha = clf.best_estimator_.alpha\n#best_split = clf.best_estimator_.min_samples_split\n\nprint(best_alpha)\n#print(best_split)\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"tcyhdYcSL_M2","colab_type":"text"},"cell_type":"markdown","source":"**from the above auc plot it is clearly visible that when auc alpha = 0.0001 , we have maximum auc**"},{"metadata":{"colab_type":"text","id":"PIqeGvsQXxri"},"cell_type":"markdown","source":"# Making final models with best alpha and penalty"},{"metadata":{"colab_type":"code","outputId":"47e524fa-753b-442f-ccc7-b4955e972c93","id":"kmy4h0NPXxrj","colab":{"base_uri":"https://localhost:8080/","height":294},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV\n\nsvm = SGDClassifier(loss='hinge', alpha=best_alpha, penalty=\"l2\", class_weight=\"balanced\")\n#svm.fit(train_1, project_data_y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\nsig_clf = CalibratedClassifierCV(svm, method=\"isotonic\")\nsvm = sig_clf.fit(train, y_train)\n\n\ny_train_pred = svm.predict_proba(train)[:,1]   \ny_test_pred = svm.predict_proba(test)[:,1] \n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"phv8P8CqMSC-","colab_type":"text"},"cell_type":"markdown","source":"**so from the above plot we can see tha  test auc is 0.863**"},{"metadata":{"colab_type":"text","id":"t6DcNTN3YPPB"},"cell_type":"markdown","source":"# Confusion Matrix with using map"},{"metadata":{"colab_type":"code","outputId":"350e3a1e-1c14-411c-8b24-754e7123b7a8","id":"j8GML2ujYPPC","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"d75072c3-d7a2-43df-dc8c-4fe91794db7b","id":"rv0Qdg_aYPPG","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"0-mpZemLmlAA"},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"colab_type":"code","id":"b3q-6FaS5J9y","outputId":"3804e9bc-68c2-4072-ea2f-f89b76f42515","colab":{"base_uri":"https://localhost:8080/","height":330},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n#https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nnaive = MultinomialNB(fit_prior=False)\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(naive, parameters, cv=3, scoring='roc_auc', return_train_score=True)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\ntrain_auc= list(clf.cv_results_['mean_train_score'])\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = list(clf.cv_results_['mean_test_score']) \ncv_auc_std= clf.cv_results_['std_test_score']\n\nbest_alpha=clf.best_estimator_.alpha\n\nalpha = log_alpha(alpha)\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","id":"7sGqBiq95J-E","outputId":"3823108b-042d-4da7-ff91-0f67275a50f2","colab":{"base_uri":"https://localhost:8080/","height":294},"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nnaive = MultinomialNB(alpha=best_alpha, fit_prior=False)\nnaive.fit(train, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = naive.predict_proba(train)[:,1]    \ny_test_pred = naive.predict_proba(test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"DQrigScpMmbu","colab_type":"text"},"cell_type":"markdown","source":"**from the above plot we can say that test auc is 0.854**"},{"metadata":{"id":"0ohXGDPcqkVn","colab_type":"text"},"cell_type":"markdown","source":"# Confusion Matrix using heat map"},{"metadata":{"colab_type":"code","id":"Fgh9On9h5J-N","outputId":"5858e141-91a8-4fb7-a17b-91b57fdbd09a","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","id":"b6MPTsbO5J-P","outputId":"b4fa2abf-afe2-4784-8723-82a8da1d73d8","colab":{"base_uri":"https://localhost:8080/","height":510},"trusted":true},"cell_type":"code","source":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","execution_count":null,"outputs":[]},{"metadata":{"id":"yCMLX_o8fWgh","colab_type":"text"},"cell_type":"markdown","source":"# Light GbM\nhttps://www.kaggle.com/gpreda/santander-eda-and-prediction"},{"metadata":{"id":"zT4jqRswgucf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../train_sant.csv\")\ntest = pd.read_csv(\"../test_sant.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"-OJjdTAfg9YB","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#train, test, y_train, y_test = train_test_split(train, target, test_size=0.4)","execution_count":null,"outputs":[]},{"metadata":{"id":"denqszrWfXnD","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#taking all the columns except idcode, target, unnamed0\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target',\"Unnamed:0\"]]\ntarget = train['target']","execution_count":null,"outputs":[]},{"metadata":{"id":"Ic7e-hceUkHe","colab_type":"text"},"cell_type":"markdown","source":"**importing necessary libraries**"},{"metadata":{"id":"0hSFYhGmhgAa","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"QXzWWjKpfpOn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#setting parameters\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Hbc3vyvkfp_A","colab_type":"code","outputId":"db70306f-6ff6-49e1-a6dd-4a438db48d40","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"#making 10 folds\nfolds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"id":"zfDVUuBfNXc6","colab_type":"text"},"cell_type":"markdown","source":"**from the above we can say taht lightgbm has performed well than all the other models. auc reaching to 0.90**"},{"metadata":{"id":"zqqX5uh3NeOs","colab_type":"text"},"cell_type":"markdown","source":"# Important features in decending order."},{"metadata":{"id":"skVJmtKlfqCR","colab_type":"code","outputId":"1e22f86a-5a44-45ee-b978-01f7b47ae37b","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\n#plotting a bar plot where y represents features and x represents its importance.\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"4dr2lHsqOMUw","colab_type":"text"},"cell_type":"markdown","source":"# PrettyTable"},{"metadata":{"id":"VlEbKX3gOOyb","colab_type":"code","outputId":"5ffb3fca-3723-4435-8438-0fdc738c3d7a","colab":{"base_uri":"https://localhost:8080/","height":158},"trusted":true},"cell_type":"code","source":"#http://zetcode.com/python/prettytable/\nfrom prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names =[\"Models\",\"Test auc\"]\nx.add_row([\"Logistic \",0.862])\nx.add_row([\"SVM \",0.863])\nx.add_row([\"Naive \",0.854])\nx.add_row([\"LightGbm\",0.90])\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"T1tXflSWOyPD","colab_type":"text"},"cell_type":"markdown","source":"# Conclusion:\n**Lightgbm is giving best results than any other models.**"},{"metadata":{"id":"O4I_h1FrO-7C","colab_type":"text"},"cell_type":"markdown","source":"# Steps Done:\n1. Importing the necessary libraries.\n2. Visualizing the train and test data.\n3. Checking for null values in train and test data if any.\n4. Describing the data\n5.Since pairplot for all the data was not possible so I did it for random 10 data\n6. Analysis of train data where we find out that data is purely unbaanced.\n7. Visualizing the pair plots.\n8. Pdf for all the features from 2 to 202(here we find out that there is some corelations between some of the data.)\n9. Visualising by tsne.\n10. Visualizing mean.\n11. visualising median\n12. visualising dtd\n13. visualising min\n14. visualising max\n15. visualising kurtosis\n16. visualizing skew\n17. visualising moving average.\n18. Visualizing by kde\n19. Visualizing by boxplot\n20. puting all the features to to dataframe.\n21. importing necessary libraries\n22. importing the new train data.\n23. Splitting data into train and test.\n24. Applying different models like naive bayes, logistic regression, svm, lightgbm\n25. Feature importance\n"}],"metadata":{"colab":{"name":"navneetkumar384@gmail.comSantander.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}