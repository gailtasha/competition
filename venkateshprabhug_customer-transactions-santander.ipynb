{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-media/competitions/santander/atm_image.png)\n\n## Introduction\n\nAt Santander their mission is to help people and businesses prosper. Santander is always looking for ways to help customers understand their financial health and identify which products and services might help them achieve their monetary goals."},{"metadata":{},"cell_type":"markdown","source":"## Objective\nIn this challenge, we have to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for the competition has the same structure as the real data Santander have available to solve this problem."},{"metadata":{},"cell_type":"markdown","source":"## Data\n\nWe are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set.\n\n**File descriptions**\n****\n- train.csv - the training set.\n- test.csv - the test set. The test set contains some rows which are not included in scoring.\n- sample_submission.csv - a sample submission file in the correct format."},{"metadata":{},"cell_type":"markdown","source":"## Approach\n\nFirst we will start by acquiring our data. We also need to understand our data better before creating models. We will be using Pandas, Scikit-Learn and other necessary libraries to create a classifier which can classify whether a customer will make a specific transaction in future."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.precision = 2\npd.options.display.max_columns = 250\npd.set_option('float_format', '{:2f}'.format)\nseed = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/santander-customer-transaction-prediction'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = pd.read_csv(f'{path}/train.csv')\nts = pd.read_csv(f'{path}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"After acquiring we can analyze the data. This is a crucial and the most import part in a Data Science process. The better understanding we have about our data means more meaningful features we can generate and thereby increasing the accuracy of our models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train shape: {tr.shape}')\nprint(f'Test shape: {ts.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for missing values\n\nWe can see that we have lots of data! It's time to check if there are any missing values somewhere out there."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Are there any missing values in train? {tr.isnull().sum().any()}')\nprint(f'Are there any missing values in test? {ts.isnull().sum().any()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in our dataset. This is good thing else we need to handle the missing values."},{"metadata":{},"cell_type":"markdown","source":"#### Understanding the target variable\n\nWe know that this is a binary classification problem. But to know how many data samples we have in each of the classes we can use value_counts() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(tr['target'])\nplt.title(f'Positive class: {round(tr[\"target\"].value_counts()[1]/len(tr) * 100, 2)}%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is highly imbalanced. We either need to apply under/over sampling or play with class weights."},{"metadata":{},"cell_type":"markdown","source":"#### Checking for correlation\n\nAs you may know, the features are anonymized and because of that we will not be able to know what features add importance to our target variable. We can find the correlation among the features and only select columns that are highly correlated with the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr = tr.corr()['target'][1:].abs()\ncorrelations = pd.DataFrame({'column': corr.index, 'correlation': corr}).sort_values('correlation', ascending=False).reset_index(drop=True)\ncorrelations.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.plot(correlations['column'][:20], correlations['correlation'][:20])\nplt.xticks(correlations['column'][:20], correlations['column'][:20], rotation='45')\nplt.title('Feature Correlations')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the distributions of highly correlated columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,10))\nax = fig.gca()\ncols = correlations['column'][:10].values\ntr[cols].hist(ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base Model\n\nUsing correlation to select features might be tricky for tree models. It is safe to create a base model by passing all the features and considering only the features that are important."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = RandomForestClassifier(random_state=seed, class_weight={0:1, 1:9}, n_estimators=20, verbose=0)\n%time base_model.fit(tr.drop(['ID_code', 'target'], 1), tr['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame({'feature': tr.drop(['ID_code', 'target'], 1).columns, 'importance': base_model.feature_importances_}).sort_values('importance', ascending=False).reset_index(drop=True)\nimportances[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can confirm that the top 9 features overlaps with features we selected using correlation method."},{"metadata":{"trusted":true},"cell_type":"code","source":"top = 100\nselected_features = importances['feature'][:top].values\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can proceed with the model creation part."},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"Before proceeding let us shuffle our data. This is to avoid any bias while splitting the data into train and validation sets. We can easily sample our data through panda's sample() function. The parameter frac denotes the % of data to be selected. Here we need all the data so we choose 1. In other cases we can pass float values between 0 and 1 to select the appropriate % of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = tr.sample(random_state=seed, frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = tr[selected_features]\ntarget = tr['target']\n\n# Payload here represents the actual test data for which we are trying to predict in this challenge\npayload = ts[selected_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape, payload.shape, target.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dimensionality Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# # from MulticoreTSNE import MulticoreTSNE as TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decomposed = PCA(n_components=100).fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=seed, stratify=target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier(random_state=seed, \n                           scale_pos_weight=10, \n                           silent=True, \n                           max_depth=None, \n                           learning_rate=0.2, \n                           loss_function='Logloss', \n                           n_estimators=2000)\n%time model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(x_train)\ny_val_pred = model.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, roc_auc_score\n\npd.DataFrame({'Train Set': roc_auc_score(y_train, y_train_pred)*100, 'Validation Set': roc_auc_score(y_val, y_val_pred)*100}, index=['ROC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 3))\na, b = np.bincount(y_val)\n# sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='g')\n# plt.show()\nsns.heatmap(np.stack([(confusion_matrix(y_val, y_val_pred)[0]/a)*100, (confusion_matrix(y_val, y_val_pred)[1]/b)*100], 0), annot=True, fmt='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(payload).flatten()\n\nsubmission = pd.DataFrame({'ID_code': ts['ID_code'], 'target': predictions})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}