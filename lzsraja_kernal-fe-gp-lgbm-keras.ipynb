{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom gplearn.genetic import SymbolicTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Data reading"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('../input/train.csv')\ndata_test = pd.read_csv('../input/test.csv')\nprint('Training data shape:',data_train.shape)\nprint('Testing data shape:',data_test.shape)\ndata_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_notebook_mode(connected=True)\nlabels = [str(x) for x in list(data_train['target'].unique())]\nvalues = [(len(data_train[data_train['target'] == x])/len(data_train))*100 for x in list(data_train['target'].unique())]    \ntrace=go.Pie(labels=labels,values=values)\niplot([trace],filename = 'Target Percentages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"fe_train = data_train.copy()\nfe_test = data_test.copy()\nsd = True #standardize\nst = True #statistics values\nsp = False #sampling\ngp = True #gplearning\nif st:\n    idx = features = fe_train.columns.values[2:202]\n    for df in [fe_train, fe_test]:\n        df['sum'] = df[idx].sum(axis=1)  \n        df['min'] = df[idx].min(axis=1)\n        df['max'] = df[idx].max(axis=1)\n        df['mean'] = df[idx].mean(axis=1)\n        df['std'] = df[idx].std(axis=1)\n        df['skew'] = df[idx].skew(axis=1)\n        df['kurt'] = df[idx].kurtosis(axis=1)\n        df['med'] = df[idx].median(axis=1)\nif sd:\n    scaler = StandardScaler()\n    fe_train.iloc[:,2:] = scaler.fit_transform(fe_train.iloc[:,2:])\n    fe_test.iloc[:,1:] = scaler.fit_transform(fe_test.iloc[:,1:])\nif gp:\n    function_set = ['add', 'sub', 'mul', 'div', 'log', 'sqrt', 'abs', 'neg', 'max', 'min']\n    generations = 80\n    population_size = 2000\n    gp = SymbolicTransformer(generations=generations, population_size=population_size,\n                         hall_of_fame=100, n_components=10,\n                         function_set=function_set,\n                         parsimony_coefficient=0.0005,\n                         max_samples=0.9, verbose=1,\n                         random_state=0, n_jobs=3)\n    gp.fit(fe_train.iloc[:,2:],fe_train['target'])\n    gp_feature_name = ['NV'+str(i) for i in range(1, 11)]\n    train_gp_features = pd.DataFrame(gp.transform(fe_train.iloc[:,2:]), columns=gp_feature_name)\n    test_gp_features = pd.DataFrame(gp.transform(fe_test.iloc[:,1:]), columns=gp_feature_name)\n    if sd:\n        train_gp_features = pd.DataFrame(scaler.fit_transform(train_gp_features))\n        test_gp_features = pd.DataFrame(scaler.fit_transform(test_gp_features))\n    fe_train = pd.concat([fe_train, train_gp_features], axis=1)\n    fe_test = pd.concat([fe_test, test_gp_features], axis=1)\nif sp:\n    fe_zeros = fe_train[fe_train['target'] == 0].sample(frac=0.25)\n    fe_ones = fe_train[fe_train['target'] > 0]\n    fe_train = pd.concat([fe_ones, fe_zeros]).sample(frac=1)\nX = fe_train.iloc[:,2:]\nY = fe_train['target']\nX_target = fe_test.iloc[:,1:]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=113)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LGBM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = True\nparam = {\n    'bagging_freq': 8, #handling overfitting\n    'bagging_fraction': 0.3,#handling overfitting - adding some noise\n     #'boost': 'dart', \n    #'boost': 'goss',\n     'boost_from_average':False,\n     'boost': 'gbdt',   \n    'feature_fraction': 0.2, #handling overfitting\n    'learning_rate': 0.008, #the changes between one auc and a better one gets really small thus a small learning rate performs better\n    'max_depth':2, \n    'metric':'auc',\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'xentropy', \n    'verbosity':1,\n    \"bagging_seed\" : 122,\n    \"seed\": 20,\n    }\nnum_boost_round = 200000\nverbose_eval=5000\nif not cv:\n    lgbm_train = lgb.Dataset(X_train,label=y_train)\n    lgbm_test = lgb.Dataset(X_test,label=y_test)\n    lgbm_valid = (lgbm_test,lgbm_train)\n    valid_names = ['valid','train']\n    lgbm_model = lgb.train(param,lgbm_train,num_boost_round=num_boost_round,valid_sets=lgbm_valid,valid_names=valid_names,\n                          verbose_eval=verbose_eval,\n                           early_stopping_rounds=5000)\n    lgbm_predict = lgbm_model.predict(X_target)\nelse:\n    lgbm_cv_valid_score = []\n    lgbm_cv_predict = []\n    kf = StratifiedKFold(n_splits=5,shuffle = False, random_state=311)\n    for _fold, (trn_idx, val_idx) in enumerate(kf.split(X.values, Y.values)):\n        print('Fold{}:'.format(_fold+1))\n        X_cv_train, y_cv_train = X.iloc[trn_idx], Y.iloc[trn_idx]\n        X_cv_test, y_cv_test = X.iloc[val_idx], Y.iloc[val_idx]\n        lgbm_cv_train = lgb.Dataset(X_cv_train,label=y_cv_train)\n        lgbm_cv_test = lgb.Dataset(X_cv_test,label=y_cv_test)\n        lgbm_cv_valid = (lgbm_cv_test,lgbm_cv_train)\n        valid_names = ['valid','train']\n        lgbm_model = lgb.train(param,lgbm_cv_train,num_boost_round=num_boost_round,valid_sets=lgbm_cv_valid,valid_names=valid_names,\n                          verbose_eval=verbose_eval,\n                           early_stopping_rounds=5000)\n        lgbm_cv_valid_score.append(roc_auc_score(y_cv_test,lgbm_model.predict(X_cv_test)))\n        print('\\tCVscore:{}'.format(lgbm_cv_valid_score[_fold]))\n        print('-'*50)\n        lgbm_cv_predict.append(lgbm_model.predict(X_target))\n    print('---Mean CV score:{}---'.format(np.mean(lgbm_cv_valid_score)))\n    lgbm_predict = np.mean(lgbm_cv_predict,axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keras NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\ninput_dim  = X_train.shape[1]\nepochs = 100\nmodel = Sequential()\nmodel.add(Dense(128,input_dim=input_dim, kernel_initializer='normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nadam = optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=adam,loss='binary_crossentropy',metrics = [auc])\nnn_model = model\nnn_history = nn_model.fit(X_train, y_train, validation_data = (X_test ,y_test),\n                          epochs=epochs,batch_size=64,verbose=1)\nnn_predict = nn_model.predict(X_target)[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ensemble & output"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_predict3 = 0.1*nn_predict+0.9*lgbm_predict\nensemble_predict3[ensemble_predict3>1]=1\nensemble_predict3[ensemble_predict3<0]=0\nsubmit = pd.DataFrame({'ID_code':fe_test['ID_code'],'target':ensemble_predict3})\nsubmit.to_csv('nnlgbm_submission1.csv', index=False)\nensemble_predict4 = 0.3*nn_predict+0.7*lgbm_predict\nensemble_predict4[ensemble_predict4>1]=1\nensemble_predict4[ensemble_predict4<0]=0\nsubmit = pd.DataFrame({'ID_code':fe_test['ID_code'],'target':ensemble_predict4})\nsubmit.to_csv('nnlgbm_submission2.csv', index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}