{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nfrom scipy import stats\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 222)\n\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\ndf_train = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')\ndf_join = pd.concat([df_train, df_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Overview of the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Rows: %s\\nColumns: %s\" % (df_train.shape[0], df_train.shape[1]))\nprint(\"*\" * 30)\nprint(df_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_types = df_train.columns.to_series().groupby(df_train.dtypes)\nprint(group_types.count())\nprint(\"*\" * 30)\nprint(group_types.groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the feature columns are of type **Float** the prediction target is of type **Int** and the ID columns is of type **Object**."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_vals = df_join.isna().sum()\nmiss_vals_percent = 100 * miss_vals / len(df_join)\nnull_df = pd.concat([miss_vals, miss_vals_percent], keys=['Missing Values', 'Missing %'], axis=1)\nnull_df.sort_values(by='Missing %', inplace=True, ascending=False)\nprint(null_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are no null or empty values the target values that are missing are a part of the test data which is as expected of course."},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Distrubtion Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats = df_train.select_dtypes(include=['float64']) # Grab all the usable features since they are only floats\nnormality_results = df_train_feats.apply(lambda x: stats.normaltest(x)[1], axis=0)\n\n# The Probability threshold that the feature is normally distributed\nalpha = 0.05\nnormals = normality_results[normality_results > alpha]\nprint(\"Number of normally distributed features: %s\\nPercentage of Features that are normally distributed: %s\\nFeatures that are normally distrubted: %s\" % (len(normals), len(normals)/df_train_feats.shape[1], normals.index.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I find it strange that  none of the features are normally distributed given the normality test which uses **Dâ€™Agostino's K^2 Normality Test** with a given alpha value of *0.05*. My hunch is that there's something wrong where the normality test are failing or maybe (highly unlikely) that none of the features follow a Guassian distribution. Let's take a visual look. "},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 25\ncol = 8\nfig, ax = plt.subplots(row, col, figsize=(col * 7, row * 5))\n\nidx = 0\nfor r in range(0, row):\n    for c in range(0, col):\n        x = df_train_feats.iloc[:, idx] # grab column\n        sns.distplot(x, axlabel=x.name, ax=ax[r][c])\n        idx += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually it seems that all the features indicate a Gaussian Distribution so it looks like the **normality test** we tried above failed due to the sample size being too large. See this [article](https://medium.com/data-design/large-amount-of-observations-statistical-test-not-so-statistical-3d8ed0e94be) detailing more. We can perform another visual representation to be sure that are assumption is correct such as a Q-Q plot but I think this will suffice. A Gaussian distribution indicates 2 things that 1) we can use a parameteric modeling method to predict here and 2) most models perform/behave better when data follows a Guassian Distribution which we of course want. "},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\nLuckily the data is well prepared and will have to do little to now preprocessing."},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.8\ncorr = df_train_feats.corr(method='pearson') \nFEATURES_TO_REMOVE = []\n\ni = 0\nfor j in range(0, len(corr)):\n    if i != j:\n        if corr.iloc[i,j] >= THRESHOLD:\n            FEATURES_TO_REMOVE.append(corr.iloc[:, j].name)\n\nprint(\"Features to Remove\\ncount: %s feats: %s\" % (len(FEATURES_TO_REMOVE), FEATURES_TO_REMOVE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it doesn't look like any of the features are correlated to each other so no need to remove any features. "},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_y = df_train[['target']]\ndf_train_X = df_train.drop(['target', 'ID_code'], axis=1) # only the feature columns are of type float64\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(df_train_X, df_train_y, test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n               'LogisticRegression',\n               'GaussianNB'\n              ]\n\nscaler = StandardScaler()\n\nresults = {\n    'classifiers': classifiers,\n    'auc': [],\n    'acc': [],\n    'f1': []\n}\n\nfor classifier in classifiers:\n    pipe = make_pipeline(scaler, eval(classifier)())\n    pipe.fit(train_X, train_y)\n    pred_y = pipe.predict(test_X)\n    \n    results['auc'].append(roc_auc_score(test_y, pred_y))\n    results['acc'].append(accuracy_score(test_y, pred_y))\n    results['f1'].append(f1_score(test_y, pred_y))\n    \nresults_df = pd.DataFrame(data=results)\nresults_df.sort_values(by='auc', ascending=False, inplace=True)\nprint(results_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameter Tuning\nThe **GaussianNB** and **LogisticRegressionCV** classifiers perform the best. Let's see if we can improve the scores by parameter tuning. Using the top 2 classifiers, let's run them through the **GridSearch** hyperparameter optimization to see if we can find the optimal parameters to get the best results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_classifiers = ['LogisticRegression','GaussianNB']\nparams = {\n    'LogisticRegression': {\n        'logisticregression__penalty': ['l2'],\n        'logisticregression__class_weight': [None, 'balanced'],\n        'logisticregression__solver': ['sag', 'saga'],\n        'logisticregression__max_iter': [25, 100, 125],\n        'logisticregression__tol': [1e-2, 1e-6]\n    },\n    'GaussianNB': {\n        'gaussiannb__var_smoothing': [1e-06, 1e-10, 1e-13]\n    }\n}\ngrid_results = {\n    \"classifiers\": selected_classifiers,\n    \"predict_auc\": [],\n    \"grid_auc\": []\n}\nfinal_models = {}\nscaler = StandardScaler()\n\nfor classifier in selected_classifiers:\n    pipe = make_pipeline(scaler, eval(classifier)())\n    search = GridSearchCV(pipe, params[classifier], scoring = 'roc_auc', n_jobs=2)\n    search.fit(train_X, train_y)\n    \n    grid_results['grid_auc'].append(search.best_score_)\n    pred_y = search.predict(test_X)\n    grid_results['predict_auc'].append(roc_auc_score(test_y, pred_y))\n    \n    final_models[classifier] = search.best_estimator_\n\ndf_search_results = pd.DataFrame(data=grid_results)\ndf_search_results.sort_values(by='predict_auc', ascending=False, inplace=True)\nprint(df_search_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the winning model with optimized parameters is the **GaussianNB** estimator. "},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"id_series = df_test['ID_code']\ndf_test_X = df_test.drop(['ID_code'], axis=1)\n\nbest_estimator = final_models['GaussianNB']\npipe = make_pipeline(StandardScaler(), best_estimator)\npipe.fit(df_train_X, df_train_y)\n\npredict_y = pipe.predict(df_test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.concat([id_series, pd.DataFrame(predict_y, columns=['Target'])], axis=1)\nprint(submission_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('Transaction_Prediction_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}