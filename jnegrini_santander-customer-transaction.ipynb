{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this challenge, the goal is to help Santander to predict if their clients will make a specific transaction.  The dataset provided by the bank is anonymised, and it is a representation of the real customer data. It is a binary classification problem, with 200 features and 200.000 samples.\n\nThis Notebook presents an overview of how the data was explored, analysed and prepared to be used on an ML model. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate as CV\nimport scipy.stats as stats\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\nrandom_state = 42\nnp.random.seed(random_state)\nfrom sklearn.model_selection import cross_validate\n# visualization\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def data_load(filename):\n    return pd.read_csv(filename)\n\ndef basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d\" %(size[0],size[1], sum_duplicates, sum_null))\n\n#Plot Bar graph with all classes and percentages - Return number of Classes and Samples per class\ndef bar_plot(df, target):\n    unique, counts = np.unique(target, return_counts = True)\n    label = np.zeros(len(unique))\n    for i in range(len(unique)):\n        label[i] = (counts[i]/df.shape[0])*100\n        plt.bar(unique,counts, color = ['burlywood', 'green'], edgecolor='black')\n        plt.text(x = unique[i]-0.15, y = counts[i]+0.01*df.shape[0], s = str(\"%.2f%%\" % label[i]), size = 15)\n    plt.ylim(0, df.shape[0])\n    plt.xticks(unique)\n    plt.xlabel(\"Target\")\n    plt.ylabel(\"Count\")\n    plt.show()\n    return unique, counts\n\n#Plots Heatmap and top 10 and bottom correlated features\ndef feat_corr_analysis(corrmat):\n    f, ax = plt.subplots(figsize =(9, 8)) \n    #1 Heatmap\n    sns.heatmap(corrmat, vmin=0, vmax=0.2, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n    plt.title(\"Heatmap - Correlation between data variables\")\n    #2 Correlation Values and Features\n    correlations = corrmat.abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n    correlations = correlations[correlations['level_0'] != correlations['level_1']]\n    #Top 10 correlated features\n    correlations.tail(10)\n    #Bottom 10 correlated features\n    correlations.head(10)\n    return correlations.tail(10)\n\ndef feat_corr_distr(train,test):\n    #Plot distribution of Feature Correlation\n    train_corr_distr = train.values.flatten()\n    train_corr_distr = train_corr_distr[train_corr_distr != 1]\n    test_corr_distr = test.values.flatten()\n    test_corr_distr = test_corr_distr[test_corr_distr != 1]\n    plt.figure(figsize=(20,5))\n    sns.distplot(train_corr_distr, color=\"Red\", label=\"Train\")\n    sns.distplot(test_corr_distr, color=\"black\", label=\"Test\")\n    plt.xlabel(\"Correlation values\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Feature Correlation\"); \n    plt.legend();\n    \ndef prediction(x_train,y_train):\n    classifier.fit(x_train,y_train)\n    y_proba = classifier.predict_proba(x_train)\n    y_pred = classifier.predict(x_train)\n    y_proba = classifier.predict_proba(x_train)\n    score = roc_auc_score(y_train, y_pred)\n    return y_proba, score\n\ndef probability_class(y_proba, true_label):\n    plt.figure(figsize=(20,5))\n    sns.distplot(y_proba[true_label==1,1], label=\"True Class 1\")\n    sns.distplot(y_proba[true_label==0,1], label=\"True Class 0\")\n    plt.xticks(np.arange(0,1, 0.1))\n    plt.xlabel(\"Predicted probability values of class 1\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Predicted probability values of class 1 against the true Target\"); \n    plt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1 - EDA"},{"metadata":{},"cell_type":"markdown","source":"Importing the Training and Test set CSV files."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Load Data\ntrain = data_load('../input/santander-customer-transaction-prediction/train.csv')\ntest = data_load('../input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Overview\nFor an overview of the data, it is analysed the number of samples, features, duplicated and null values:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"***Train EDA***\")\ntrain_EDA = basic_EDA(train)\nprint(\"***Test EDA***\")\ntest_EDA  = basic_EDA(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a large dataset, with 200.000 samples and over 200 features. No null values or duplicated entries were found, eliminating the need for data cleanse at this stage. It is also fundamental to understand the type of variables present in this dataset:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.info()\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"target\" column contains the true labels. The \"ID_CODE\" is the object dtype mentioned in the info(). Other than this, all values are numeric for train and test sets. Therefore there is no need for enconding."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test.info()\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, it is possible to remove the ID_code column and replace it with a numerical index that can be easily handled by the ML models. The new column is called Id."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#ID_Code is the only object dtype. It can be replaced by index values\ntrain[\"Id\"] = train.index.values\ntest[\"Id\"] = test.index.values\ninit_train_ID = train.ID_code.values\ninit_test_ID = test.ID_code.values\ntrain.drop(\"ID_code\", axis=1, inplace=True)\ntest.drop(\"ID_code\", axis=1, inplace=True)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Samples per Class\nBy analysing the samples per class ratio it is possible to see the class imbalance issue. Almost 90% of the samples are clients that did not performed the transaction. \n\n* An initial attempt was made to use SMOTE and ADASYN as oversampling methods. Preliminary test have shown that this did not helped the model to generalise to the test set."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"##Visualise Class Imbalance - Training Set\nnum_classes, feat_per_class = bar_plot(train, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Correlation\nSince this dataset has a significant amount of features, this analysis helps to develop an understanding of how the variable relate to each other. This can indicate starting points for feature engineering and the most relevant variables for the model. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_corrmat = train.drop([\"target\"], axis=1).corr()\nfeat_corr_train = feat_corr_analysis(train_corrmat)\ntest_corrmat = test.corr()\nfeat_corr_test = feat_corr_analysis(test_corrmat)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is none or very little correlation between the features for training and test sets. The plot below shows that the correlation values distribution is similar for the train and test set."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feat_corr_distr(train_corrmat, test_corrmat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the EDA, it was possible to conclude:\n* No major data cleanse method was required, as the data was mostly numerical and there were no missing or duplicated entries;\n* There is little or no variance between the variables, which renders difficult to define the feature engineering path;\n* The data imbalance issue needs to be addressed."},{"metadata":{},"cell_type":"markdown","source":"# Part 2 - Model Baseline\nRandom Forest (RF) algorithm is used as a starting point to allow a better understanding of this dataset. RF is quick, and it does not have that many parameters to tune and usually provide reasonable results. By understanding what works and what does not works with RF, it is possible to improve the model quicker and then try other algorithms. This first step is mainly to understand the top essential features,  trial some feature engineering and sampling techniques."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Prepare DF\nX_train = train.drop(\"target\", axis=1).values\ny_train = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the RF classifier. Grid Search supported the selection of the initial parameters:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Random Forest Baseline Model\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators = 10, \n                       criterion = 'gini',\n                       max_depth = 15, \n                       max_features = 'auto', \n                       min_samples_leaf = 1, \n                       random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next block builds the RF model with Cross Validation and outputs and test set scores."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def CV(clf, X, y, metric, cv):\n    scores = cross_validate(clf,X,y, scoring = metric, cv = cv)\n    return scores\n\nmetric = make_scorer(roc_auc_score)\nCV_result = CV(classifier, X_train, y_train, metric, 3)\nprint(\"Cross Validation Results: \\n\", (CV_result['test_score']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below creates the RF model and outputs the AUC result for the training set. It also outputs the probabilities of each sample belonging to class 0 or 1."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"RF_y_proba, RF_model_score = prediction(X_train,y_train)\nprint(\"Baseline RF: %.2f \"%(RF_model_score))\npd.DataFrame(RF_y_proba).describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RF_y_proba contains the probability of class 0 and 1 for every sample. Most values for class 1 are small, and the majority does not reach 0.1, as shown by the table above. One hypothesis can be that the usual threshold of 0.5 is not ideal for this model to assign class 1 or 0. One way to visualise this is to plot the \"Class 1\" probability distribution for all samples, according to their True label. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"probability_class(RF_y_proba, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above it is possible to extract the following insights:\n* Most of the True Class 1 samples were predicted by our model with a probability lower than 0.5\n* Most of the True Class 0 samples are concentrated within the values of 0 and 0.15\n* For samples with Class 1 probability values higher than approximately 0.15, it is almost safe to say they are True Class 1. Altough there is the orange bump around 0.17\n* It is possible to perform a quick test and see if this helps the model accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the graph, used a threshold of 0.15\nthreshold = 0.15\ny_pred = np.zeros(RF_y_proba.shape[0])\ny_pred[RF_y_proba[:,1] >= threshold] = 1\nRFT_model_score = roc_auc_score(y_train, y_pred)\nprint(\"Baseline RF: %.2f \\nThreshold RF: %.2f\"%(RF_model_score, RFT_model_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A 30% increase in accuracy was achieved by this simple but effective strategy. This is something to keep in mind in the future and check if other models also present this behaviour."},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances\nThis analysis can helps reduce the number of features while mantaining the minimum impact on model accuracy. Using the RF classifier, the following features are more effective to our model:\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"importances = classifier.feature_importances_\nindices = np.argsort(importances)[::-1]\ncolors = plt.cm.Reds(importances)\n# Plot the feature importances\nplt.figure(1, figsize=(40,20))\nplt.title(\"Feature importances\")\nsns.barplot(x=indices, y=importances[indices], order = indices,palette=\"Blues_d\")\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"To improve visualisation, narrowing down:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#From the Feature Importances, we can see that \nn_top = 100\nidx = np.argsort(importances)[::-1][0:n_top]\nfeature_names = train.drop(\"target\", axis=1).columns.values\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx],palette=\"Blues_d\");\nplt.title(\"Top important features to start\");\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.DataFrame(X_train[:,idx]).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next model is built using the top features. Additional aggreagation features were created performing the following operations per row: "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def concat_feat_eng(df,df_feat):\n    sum_feat= df_feat.sum(axis=1)\n    min_feat = df_feat.min(axis=1)\n    max_feat = df_feat.max(axis=1)\n    mean_feat = df_feat.mean(axis=1)\n    std_feat = df_feat.std(axis=1)\n    var_feat = df_feat.var(axis=1)\n    per25_feat = np.percentile(df_feat, 25, axis = 1)\n    per50_feat = np.percentile(df_feat, 50, axis = 1)\n    per75_feat = np.percentile(df_feat, 75, axis = 1)\n    iqr_feat = per75_feat - per25_feat\n    skw_feat = stats.skew(df_feat, axis = 1)\n    kur_feat = stats.kurtosis(df_feat, axis = 1)\n    df = np.concatenate((df_feat,                        \n                        sum_feat[:,None],\n                        min_feat[:,None],\n                        max_feat[:,None],\n                        mean_feat[:,None],\n                        std_feat[:,None],\n                        var_feat[:,None],\n                        per25_feat[:,None],\n                        per50_feat[:,None],\n                        per75_feat[:,None],\n                        iqr_feat[:,None],\n                        skw_feat[:,None],\n                        kur_feat[:,None]),\n                        axis = 1)\n    return df\n\nX_train_top = X_train[:,0:n_top]\nX_train_top = concat_feat_eng(X_train_top,X_train_top)\n\nRF1_y_proba, RF1_model_score = prediction(X_train_top, y_train)\nprobability_class(RF1_y_proba,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the new features, is necessary to revaluate and assign a new threshold "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"RF1_y_pred = np.zeros(RF1_y_proba.shape[0])\nRF1_y_pred[RF1_y_proba[:,1] >= 0.12] = 1\nRF1_model_score = roc_auc_score(y_train, y_pred)\nprint(\"Baseline RF %.2f\\nThreshold RF %.2f\\nThreshold RF with only top features %.2f\"%(RF_model_score, RFT_model_score, RF1_model_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Submission\nBy reducing the number of features to a quarter of the original model, the same accuracy was mantained. \n\nAt this point is interesting to check our score in the test set and submit:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_test_top = X_test[:,idx]\nX_test_top = X_test[:,0:n_top]\nX_test_top = concat_feat_eng(X_test_top,X_test_top)\n\n#####################################################\ny_proba = classifier.predict_proba(X_test_top)\ny_pred = np.zeros(y_proba.shape[0])\ny_pred[y_proba[:,1] >= 0.12] = 1\n#submission = pd.concat([pd.DataFrame(init_test_ID),pd.DataFrame(y_pred)],axis = 1)\n#submission.columns = ['ID_code', 'Target']\n#submission.to_csv(\"submission_RForest.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Conclusion\n\nThe result on the test set points to overfitting, since it only achieved **0.59** on the test set while **0.85** on the training set.\n\nWith the learned lessons from the baseline model, it is now possible to devote attention into applying other models."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Part 3 - Models"},{"metadata":{},"cell_type":"markdown","source":"## Light GBM \nLightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. Light GBM usually demonstrates a good performance for large datasets, if compared to other algorithms (XGBoost, Gradient Boosting and AdaBoost, for example).\n\nThe code below defines my Light GBM model. Grid Search was performed to tune the main parameters, this was commented out to reduce the time to run the full notebook. The \"is_unbalance\" parameter has proven to be very helpful for this model, it is responsible for a 5% increase on the final auc score. All the dataset features are used, as it did not significantly increased the training time."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Adds the aggregation features to the original Train and Test set\nX_train = concat_feat_eng(X_train, X_train)\nX_test = concat_feat_eng(X_test, X_test)\n\n#Creates a Validation Set\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\ntrain_data = lightgbm.Dataset(X_train, label=y_train)\nval_data = lightgbm.Dataset(X_val, label=y_val)\n\n#Grid Search\n#param_grid = {\n#    'num_leaves': [16,32,64],\n#    'bagging_fraction' :[ 0.5,0.8],\n#    'bagging_freq' : [10, 20, 50, 100],\n#    'max_depth' : [10,50,100]\n#    }\n\n#LGB = lightgbm.LGBMClassifier(boosting = 'gbdt',\n#                            objective = 'binary',\n#                              is_unbalance = 'true',\n#                              learning_rate = 0.005,\n#                              metric = 'auc',\n#                              num_iterations = 1000,\n#                              bagging_fraction = 0.8,\n#                              bagging_freq = 20,\n#                              feature_fraction = 0.5,\n#                              )\n#\n#LGB_Grid = GridSearchCV(estimator=LGB, param_grid=param_grid, cv= 2, scoring=make_scorer(roc_auc_score), verbose=2)\n#LGB_Grid.fit(X_train, y_train)\n#print(LGB_Grid.best_params_)\n#print(LGB_Grid.best_score_)\n\n#LGB Final Parameters\nparams = {\n    'bagging_fraction': 0.5,\n    'bagging_freq': 10,\n    'boosting': 'gbdt', \n    'feature_fraction': 0.5, \n    'is_unbalance': 'true',    \n    'max_depth': 10,\n    'metric': 'auc',    \n    'num_leaves': 32,    \n    'learning_rate': 0.005,\n    'objective': 'binary',    \n    'verbose': 0\n}\n\nLGB = lightgbm.train(params, \n                   train_data,\n                   valid_sets=val_data,\n                   num_boost_round=50000,\n                   early_stopping_rounds=50)\n\nLGB_y_val = LGB.predict(X_val)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Light GBM Validation set AUC: %.2f\" %(roc_auc_score(y_val, LGB_y_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is an improvement over the previous RF model. The analysis of the probability distribution also shows that the 0.5 threshold is not the minimum point where the 0 and 1 distributions meet:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.distplot(LGB_y_val[y_val==1], label=\"True Class 1\")\nsns.distplot(LGB_y_val[y_val==0], label=\"True Class 0\")\nplt.xlabel(\"Predicted probability values\")\nplt.ylabel(\"Density\")\nplt.title(\"Predicted probability values against the true Target for Validation set\"); \nplt.legend();\n\nLGB_y_pred = LGB.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(init_test_ID),pd.DataFrame(LGB_y_pred)],axis = 1)\nsubmission.columns = ['ID_code', 'Target']\nsubmission.to_csv(\"submission_LGB.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above it is possible to visualise the probability distribtuion between the two classes. Differently from the RF model, the distribution is more balanced and most of the class 1 samples are concentrated on the right corner, where p > 0.5. For this reason, it is not necessary to use the strategy described earlier. The model results can be directly sent for submission.\n\nThe LGB achieved a **0.89** AUC on both training and test sets. This is a good indication that our model is not overfitted to the training data as the RF. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nLight GBM has shown to be an interesting choice for this dataset. The final model auc is near to 90% for both train and test sets. \n\nPreliminary tests with Logistic Regression, SVM and XGBoost have not reached auc values above 65%. As future work, an ensemble model would be an interesting approach as well as applying ANN."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}