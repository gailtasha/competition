{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Background Information ##"},{"metadata":{},"cell_type":"markdown","source":"Financial institutions, like Santander, help people and businesses prosper by providing tools and services to assess their personal financial health and to identify additional ways to help customers reach their monetary goals. In the United States, it is estimated that 40% of Americans cannot cover a $400 emergency expense1. As a result, it is imperative that financial institutions learn consumer habits to adopt new technologies to better serve their financial needs. "},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement ##\nSantander, a financial institution, is trying to predict the next transaction a given customer is trying to complete based on historical banking information. This is a binary classification problem where the input data contains 299 unnamed normally-distributed feature variables. The solution to this problem will be evaluated on a provided test data set by Santander. "},{"metadata":{},"cell_type":"markdown","source":"## Solution Statement ##"},{"metadata":{},"cell_type":"markdown","source":"The provided train.csv file contains 200,000 unique rows corresponding to customer data. Given the large dataset, and the need to complete binary classification, there are many solutions to this problem: mine will involve using a deep neural network, after preprocessing the inputs by normalizing and scaling features, to classify the two target variables. After the model is trained and validated on a subset of the data from the train.csv file, I will run my trained model on the provided test set from Santander and measure the accuracy of each prediction. "},{"metadata":{},"cell_type":"markdown","source":"## Load Data ##"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Santander dataset\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\nsubmission_data = pd.read_csv('../input/sample_submission.csv')\n","execution_count":59,"outputs":[{"output_type":"stream","text":"['sample_submission.csv', 'test.csv', 'train.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration & Visualizations ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Size of training data\ntrain_data.shape","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"(200000, 202)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['target'].head(5)\n","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"0    0\n1    0\n2    0\n3    0\n4    0\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Dataset Statistics ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"              target          var_0      ...              var_198        var_199\ncount  200000.000000  200000.000000      ...        200000.000000  200000.000000\nmean        0.100490      10.679914      ...            15.870720      -3.326537\nstd         0.300653       3.040051      ...             3.010945      10.438015\nmin         0.000000       0.408400      ...             6.299300     -38.852800\n25%         0.000000       8.453850      ...            13.829700     -11.208475\n50%         0.000000      10.524750      ...            15.934050      -2.819550\n75%         0.000000      12.758200      ...            18.064725       4.836800\nmax         1.000000      20.315000      ...            26.079100      28.500700\n\n[8 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.100490</td>\n      <td>10.679914</td>\n      <td>-1.627622</td>\n      <td>10.715192</td>\n      <td>6.796529</td>\n      <td>11.078333</td>\n      <td>-5.065317</td>\n      <td>5.408949</td>\n      <td>16.545850</td>\n      <td>0.284162</td>\n      <td>7.567236</td>\n      <td>0.394340</td>\n      <td>-3.245596</td>\n      <td>14.023978</td>\n      <td>8.530232</td>\n      <td>7.537606</td>\n      <td>14.573126</td>\n      <td>9.333264</td>\n      <td>-5.696731</td>\n      <td>15.244013</td>\n      <td>12.438567</td>\n      <td>13.290894</td>\n      <td>17.257883</td>\n      <td>4.305430</td>\n      <td>3.019540</td>\n      <td>10.584400</td>\n      <td>13.667496</td>\n      <td>-4.055133</td>\n      <td>-1.137908</td>\n      <td>5.532980</td>\n      <td>5.053874</td>\n      <td>-7.687740</td>\n      <td>10.393046</td>\n      <td>-0.512886</td>\n      <td>14.774147</td>\n      <td>11.434250</td>\n      <td>3.842499</td>\n      <td>2.187230</td>\n      <td>5.868899</td>\n      <td>10.642131</td>\n      <td>...</td>\n      <td>24.259300</td>\n      <td>5.633293</td>\n      <td>5.362896</td>\n      <td>11.002170</td>\n      <td>-2.871906</td>\n      <td>19.315753</td>\n      <td>2.963335</td>\n      <td>-4.151155</td>\n      <td>4.937124</td>\n      <td>5.636008</td>\n      <td>-0.004962</td>\n      <td>-0.831777</td>\n      <td>19.817094</td>\n      <td>-0.677967</td>\n      <td>20.210677</td>\n      <td>11.640613</td>\n      <td>-2.799585</td>\n      <td>11.882933</td>\n      <td>-1.014064</td>\n      <td>2.591444</td>\n      <td>-2.741666</td>\n      <td>10.085518</td>\n      <td>0.719109</td>\n      <td>8.769088</td>\n      <td>12.756676</td>\n      <td>-3.983261</td>\n      <td>8.970274</td>\n      <td>-10.335043</td>\n      <td>15.377174</td>\n      <td>0.746072</td>\n      <td>3.234440</td>\n      <td>7.438408</td>\n      <td>1.927839</td>\n      <td>3.331774</td>\n      <td>17.993784</td>\n      <td>-0.142088</td>\n      <td>2.303335</td>\n      <td>8.908158</td>\n      <td>15.870720</td>\n      <td>-3.326537</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.300653</td>\n      <td>3.040051</td>\n      <td>4.050044</td>\n      <td>2.640894</td>\n      <td>2.043319</td>\n      <td>1.623150</td>\n      <td>7.863267</td>\n      <td>0.866607</td>\n      <td>3.418076</td>\n      <td>3.332634</td>\n      <td>1.235070</td>\n      <td>5.500793</td>\n      <td>5.970253</td>\n      <td>0.190059</td>\n      <td>4.639536</td>\n      <td>2.247908</td>\n      <td>0.411711</td>\n      <td>2.557421</td>\n      <td>6.712612</td>\n      <td>7.851370</td>\n      <td>7.996694</td>\n      <td>5.876254</td>\n      <td>8.196564</td>\n      <td>2.847958</td>\n      <td>0.526893</td>\n      <td>3.777245</td>\n      <td>0.285535</td>\n      <td>5.922210</td>\n      <td>1.523714</td>\n      <td>0.783367</td>\n      <td>2.615942</td>\n      <td>7.965198</td>\n      <td>2.159891</td>\n      <td>2.587830</td>\n      <td>4.322325</td>\n      <td>0.541614</td>\n      <td>5.179559</td>\n      <td>3.119978</td>\n      <td>2.249730</td>\n      <td>4.278903</td>\n      <td>...</td>\n      <td>10.880263</td>\n      <td>0.217938</td>\n      <td>1.419612</td>\n      <td>5.262056</td>\n      <td>5.457784</td>\n      <td>5.024182</td>\n      <td>0.369684</td>\n      <td>7.798020</td>\n      <td>3.105986</td>\n      <td>0.369437</td>\n      <td>4.424621</td>\n      <td>5.378008</td>\n      <td>8.674171</td>\n      <td>5.966674</td>\n      <td>7.136427</td>\n      <td>2.892167</td>\n      <td>7.513939</td>\n      <td>2.628895</td>\n      <td>8.579810</td>\n      <td>2.798956</td>\n      <td>5.261243</td>\n      <td>1.371862</td>\n      <td>8.963434</td>\n      <td>4.474924</td>\n      <td>9.318280</td>\n      <td>4.725167</td>\n      <td>3.189759</td>\n      <td>11.574708</td>\n      <td>3.944604</td>\n      <td>0.976348</td>\n      <td>4.559922</td>\n      <td>3.023272</td>\n      <td>1.478423</td>\n      <td>3.992030</td>\n      <td>3.135162</td>\n      <td>1.429372</td>\n      <td>5.454369</td>\n      <td>0.921625</td>\n      <td>3.010945</td>\n      <td>10.438015</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.408400</td>\n      <td>-15.043400</td>\n      <td>2.117100</td>\n      <td>-0.040200</td>\n      <td>5.074800</td>\n      <td>-32.562600</td>\n      <td>2.347300</td>\n      <td>5.349700</td>\n      <td>-10.505500</td>\n      <td>3.970500</td>\n      <td>-20.731300</td>\n      <td>-26.095000</td>\n      <td>13.434600</td>\n      <td>-6.011100</td>\n      <td>1.013300</td>\n      <td>13.076900</td>\n      <td>0.635100</td>\n      <td>-33.380200</td>\n      <td>-10.664200</td>\n      <td>-12.402500</td>\n      <td>-5.432200</td>\n      <td>-10.089000</td>\n      <td>-5.322500</td>\n      <td>1.209800</td>\n      <td>-0.678400</td>\n      <td>12.720000</td>\n      <td>-24.243100</td>\n      <td>-6.166800</td>\n      <td>2.089600</td>\n      <td>-4.787200</td>\n      <td>-34.798400</td>\n      <td>2.140600</td>\n      <td>-8.986100</td>\n      <td>1.508500</td>\n      <td>9.816900</td>\n      <td>-16.513600</td>\n      <td>-8.095100</td>\n      <td>-1.183400</td>\n      <td>-6.337100</td>\n      <td>...</td>\n      <td>-7.452200</td>\n      <td>4.852600</td>\n      <td>0.623100</td>\n      <td>-6.531700</td>\n      <td>-19.997700</td>\n      <td>3.816700</td>\n      <td>1.851200</td>\n      <td>-35.969500</td>\n      <td>-5.250200</td>\n      <td>4.258800</td>\n      <td>-14.506000</td>\n      <td>-22.479300</td>\n      <td>-11.453300</td>\n      <td>-22.748700</td>\n      <td>-2.995300</td>\n      <td>3.241500</td>\n      <td>-29.116500</td>\n      <td>4.952100</td>\n      <td>-29.273400</td>\n      <td>-7.856100</td>\n      <td>-22.037400</td>\n      <td>5.416500</td>\n      <td>-26.001100</td>\n      <td>-4.808200</td>\n      <td>-18.489700</td>\n      <td>-22.583300</td>\n      <td>-3.022300</td>\n      <td>-47.753600</td>\n      <td>4.412300</td>\n      <td>-2.554300</td>\n      <td>-14.093300</td>\n      <td>-2.691700</td>\n      <td>-3.814500</td>\n      <td>-11.783400</td>\n      <td>8.694400</td>\n      <td>-5.261000</td>\n      <td>-14.209600</td>\n      <td>5.960600</td>\n      <td>6.299300</td>\n      <td>-38.852800</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>8.453850</td>\n      <td>-4.740025</td>\n      <td>8.722475</td>\n      <td>5.254075</td>\n      <td>9.883175</td>\n      <td>-11.200350</td>\n      <td>4.767700</td>\n      <td>13.943800</td>\n      <td>-2.317800</td>\n      <td>6.618800</td>\n      <td>-3.594950</td>\n      <td>-7.510600</td>\n      <td>13.894000</td>\n      <td>5.072800</td>\n      <td>5.781875</td>\n      <td>14.262800</td>\n      <td>7.452275</td>\n      <td>-10.476225</td>\n      <td>9.177950</td>\n      <td>6.276475</td>\n      <td>8.627800</td>\n      <td>11.551000</td>\n      <td>2.182400</td>\n      <td>2.634100</td>\n      <td>7.613000</td>\n      <td>13.456400</td>\n      <td>-8.321725</td>\n      <td>-2.307900</td>\n      <td>4.992100</td>\n      <td>3.171700</td>\n      <td>-13.766175</td>\n      <td>8.870000</td>\n      <td>-2.500875</td>\n      <td>11.456300</td>\n      <td>11.032300</td>\n      <td>0.116975</td>\n      <td>-0.007125</td>\n      <td>4.125475</td>\n      <td>7.591050</td>\n      <td>...</td>\n      <td>15.696125</td>\n      <td>5.470500</td>\n      <td>4.326100</td>\n      <td>7.029600</td>\n      <td>-7.094025</td>\n      <td>15.744550</td>\n      <td>2.699000</td>\n      <td>-9.643100</td>\n      <td>2.703200</td>\n      <td>5.374600</td>\n      <td>-3.258500</td>\n      <td>-4.720350</td>\n      <td>13.731775</td>\n      <td>-5.009525</td>\n      <td>15.064600</td>\n      <td>9.371600</td>\n      <td>-8.386500</td>\n      <td>9.808675</td>\n      <td>-7.395700</td>\n      <td>0.625575</td>\n      <td>-6.673900</td>\n      <td>9.084700</td>\n      <td>-6.064425</td>\n      <td>5.423100</td>\n      <td>5.663300</td>\n      <td>-7.360000</td>\n      <td>6.715200</td>\n      <td>-19.205125</td>\n      <td>12.501550</td>\n      <td>0.014900</td>\n      <td>-0.058825</td>\n      <td>5.157400</td>\n      <td>0.889775</td>\n      <td>0.584600</td>\n      <td>15.629800</td>\n      <td>-1.170700</td>\n      <td>-1.946925</td>\n      <td>8.252800</td>\n      <td>13.829700</td>\n      <td>-11.208475</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>10.524750</td>\n      <td>-1.608050</td>\n      <td>10.580000</td>\n      <td>6.825000</td>\n      <td>11.108250</td>\n      <td>-4.833150</td>\n      <td>5.385100</td>\n      <td>16.456800</td>\n      <td>0.393700</td>\n      <td>7.629600</td>\n      <td>0.487300</td>\n      <td>-3.286950</td>\n      <td>14.025500</td>\n      <td>8.604250</td>\n      <td>7.520300</td>\n      <td>14.574100</td>\n      <td>9.232050</td>\n      <td>-5.666350</td>\n      <td>15.196250</td>\n      <td>12.453900</td>\n      <td>13.196800</td>\n      <td>17.234250</td>\n      <td>4.275150</td>\n      <td>3.008650</td>\n      <td>10.380350</td>\n      <td>13.662500</td>\n      <td>-4.196900</td>\n      <td>-1.132100</td>\n      <td>5.534850</td>\n      <td>4.950200</td>\n      <td>-7.411750</td>\n      <td>10.365650</td>\n      <td>-0.497650</td>\n      <td>14.576000</td>\n      <td>11.435200</td>\n      <td>3.917750</td>\n      <td>2.198000</td>\n      <td>5.900650</td>\n      <td>10.562700</td>\n      <td>...</td>\n      <td>23.864500</td>\n      <td>5.633500</td>\n      <td>5.359700</td>\n      <td>10.788700</td>\n      <td>-2.637800</td>\n      <td>19.270800</td>\n      <td>2.960200</td>\n      <td>-4.011600</td>\n      <td>4.761600</td>\n      <td>5.634300</td>\n      <td>0.002800</td>\n      <td>-0.807350</td>\n      <td>19.748000</td>\n      <td>-0.569750</td>\n      <td>20.206100</td>\n      <td>11.679800</td>\n      <td>-2.538450</td>\n      <td>11.737250</td>\n      <td>-0.942050</td>\n      <td>2.512300</td>\n      <td>-2.688800</td>\n      <td>10.036050</td>\n      <td>0.720200</td>\n      <td>8.600000</td>\n      <td>12.521000</td>\n      <td>-3.946950</td>\n      <td>8.902150</td>\n      <td>-10.209750</td>\n      <td>15.239450</td>\n      <td>0.742600</td>\n      <td>3.203600</td>\n      <td>7.347750</td>\n      <td>1.901300</td>\n      <td>3.396350</td>\n      <td>17.957950</td>\n      <td>-0.172700</td>\n      <td>2.408900</td>\n      <td>8.888200</td>\n      <td>15.934050</td>\n      <td>-2.819550</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>12.758200</td>\n      <td>1.358625</td>\n      <td>12.516700</td>\n      <td>8.324100</td>\n      <td>12.261125</td>\n      <td>0.924800</td>\n      <td>6.003000</td>\n      <td>19.102900</td>\n      <td>2.937900</td>\n      <td>8.584425</td>\n      <td>4.382925</td>\n      <td>0.852825</td>\n      <td>14.164200</td>\n      <td>12.274775</td>\n      <td>9.270425</td>\n      <td>14.874500</td>\n      <td>11.055900</td>\n      <td>-0.810775</td>\n      <td>21.013325</td>\n      <td>18.433300</td>\n      <td>17.879400</td>\n      <td>23.089050</td>\n      <td>6.293200</td>\n      <td>3.403800</td>\n      <td>13.479600</td>\n      <td>13.863700</td>\n      <td>-0.090200</td>\n      <td>0.015625</td>\n      <td>6.093700</td>\n      <td>6.798925</td>\n      <td>-1.443450</td>\n      <td>11.885000</td>\n      <td>1.469100</td>\n      <td>18.097125</td>\n      <td>11.844400</td>\n      <td>7.487725</td>\n      <td>4.460400</td>\n      <td>7.542400</td>\n      <td>13.598925</td>\n      <td>...</td>\n      <td>32.622850</td>\n      <td>5.792000</td>\n      <td>6.371200</td>\n      <td>14.623900</td>\n      <td>1.323600</td>\n      <td>23.024025</td>\n      <td>3.241500</td>\n      <td>1.318725</td>\n      <td>7.020025</td>\n      <td>5.905400</td>\n      <td>3.096400</td>\n      <td>2.956800</td>\n      <td>25.907725</td>\n      <td>3.619900</td>\n      <td>25.641225</td>\n      <td>13.745500</td>\n      <td>2.704400</td>\n      <td>13.931300</td>\n      <td>5.338750</td>\n      <td>4.391125</td>\n      <td>0.996200</td>\n      <td>11.011300</td>\n      <td>7.499175</td>\n      <td>12.127425</td>\n      <td>19.456150</td>\n      <td>-0.590650</td>\n      <td>11.193800</td>\n      <td>-1.466000</td>\n      <td>18.345225</td>\n      <td>1.482900</td>\n      <td>6.406200</td>\n      <td>9.512525</td>\n      <td>2.949500</td>\n      <td>6.205800</td>\n      <td>20.396525</td>\n      <td>0.829600</td>\n      <td>6.556725</td>\n      <td>9.593300</td>\n      <td>18.064725</td>\n      <td>4.836800</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>20.315000</td>\n      <td>10.376800</td>\n      <td>19.353000</td>\n      <td>13.188300</td>\n      <td>16.671400</td>\n      <td>17.251600</td>\n      <td>8.447700</td>\n      <td>27.691800</td>\n      <td>10.151300</td>\n      <td>11.150600</td>\n      <td>18.670200</td>\n      <td>17.188700</td>\n      <td>14.654500</td>\n      <td>22.331500</td>\n      <td>14.937700</td>\n      <td>15.863300</td>\n      <td>17.950600</td>\n      <td>19.025900</td>\n      <td>41.748000</td>\n      <td>35.183000</td>\n      <td>31.285900</td>\n      <td>49.044300</td>\n      <td>14.594500</td>\n      <td>4.875200</td>\n      <td>25.446000</td>\n      <td>14.654600</td>\n      <td>15.675100</td>\n      <td>3.243100</td>\n      <td>8.787400</td>\n      <td>13.143100</td>\n      <td>15.651500</td>\n      <td>20.171900</td>\n      <td>6.787100</td>\n      <td>29.546600</td>\n      <td>13.287800</td>\n      <td>21.528900</td>\n      <td>14.245600</td>\n      <td>11.863800</td>\n      <td>29.823500</td>\n      <td>...</td>\n      <td>58.394200</td>\n      <td>6.309900</td>\n      <td>10.134400</td>\n      <td>27.564800</td>\n      <td>12.119300</td>\n      <td>38.332200</td>\n      <td>4.220400</td>\n      <td>21.276600</td>\n      <td>14.886100</td>\n      <td>7.089000</td>\n      <td>16.731900</td>\n      <td>17.917300</td>\n      <td>53.591900</td>\n      <td>18.855400</td>\n      <td>43.546800</td>\n      <td>20.854800</td>\n      <td>20.245200</td>\n      <td>20.596500</td>\n      <td>29.841300</td>\n      <td>13.448700</td>\n      <td>12.750500</td>\n      <td>14.393900</td>\n      <td>29.248700</td>\n      <td>23.704900</td>\n      <td>44.363400</td>\n      <td>12.997500</td>\n      <td>21.739200</td>\n      <td>22.786100</td>\n      <td>29.330300</td>\n      <td>4.034100</td>\n      <td>18.440900</td>\n      <td>16.716500</td>\n      <td>8.402400</td>\n      <td>18.281800</td>\n      <td>27.928800</td>\n      <td>4.272900</td>\n      <td>18.321500</td>\n      <td>12.000400</td>\n      <td>26.079100</td>\n      <td>28.500700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Immediate Key Takeaways: The mean, standard deviation, and maximum values of the features vary widely; if we choose to implement a black-box algorithm, like neural networks, a key step will be data preprocessing, which will involve feature scaling, potentially outlier-detection and removal, and definitely normalization of these variables."},{"metadata":{},"cell_type":"markdown","source":"### Separate Training and Validation Datasets ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(feature_train_data, train_data['target'], test_size = 0.20, random_state = 25)","execution_count":63,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the dataset statistics, we can see that the mean and standard deviatation of each var feature significantly varies. If we apply supervised learning algorithms without any feature scaling or preprocessing, the algorithm will bias to certain features over others when learning the relationship between the input features and the target classification of the customer.\n"},{"metadata":{},"cell_type":"markdown","source":"## Implement LGBM Algorithm ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\n\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    #\"lambda_l1\" : 5,\n    #\"lambda_l2\" : 5,\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=random_state)\nskf.get_n_splits(X_train, y_train)","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"5"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nX_test = df_test[features].values\nfeature_importance_df = pd.DataFrame()\npredictions = df_test[['ID_code']]\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    print(\"FOLD: \", fold, \"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n    \n    N = 5\n    p_valid = 0\n    yp = 0\n    \n    for i in range(N):\n        \n        trn_data = lgb.Dataset(X_train, label = y_train)\n        val_data = lgb.Dataset(X_valid, label = y_valid)\n        \n        lgb_clf = lgb.train(lgb_params,\n                   trn_data,\n                   100000,\n                   valid_sets = [trn_data, val_data],\n                    verbose_eval = 5000,\n                    early_stopping_rounds = 3000)\n        \n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n    \n    \n    #Get importance of the fold when predicting test set\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    predictions['fold{}'.format(fold+1)] = yp/N\n","execution_count":null,"outputs":[{"output_type":"stream","text":"FOLD:  0 TRAIN: [     1      2      3 ... 199996 199997 199999] TEST: [     0     11     12 ... 199988 199992 199998]\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933214\tvalid_1's auc: 0.898465\n[10000]\ttraining's auc: 0.954304\tvalid_1's auc: 0.899339\nEarly stopping, best iteration is:\n[9666]\ttraining's auc: 0.953074\tvalid_1's auc: 0.899417\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933214\tvalid_1's auc: 0.898465\n[10000]\ttraining's auc: 0.954304\tvalid_1's auc: 0.899339\nEarly stopping, best iteration is:\n[9666]\ttraining's auc: 0.953074\tvalid_1's auc: 0.899417\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933214\tvalid_1's auc: 0.898465\n[10000]\ttraining's auc: 0.954304\tvalid_1's auc: 0.899339\nEarly stopping, best iteration is:\n[9666]\ttraining's auc: 0.953074\tvalid_1's auc: 0.899417\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933214\tvalid_1's auc: 0.898465\n[10000]\ttraining's auc: 0.954304\tvalid_1's auc: 0.899339\nEarly stopping, best iteration is:\n[9666]\ttraining's auc: 0.953074\tvalid_1's auc: 0.899417\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933214\tvalid_1's auc: 0.898465\n[10000]\ttraining's auc: 0.954304\tvalid_1's auc: 0.899339\nEarly stopping, best iteration is:\n[9666]\ttraining's auc: 0.953074\tvalid_1's auc: 0.899417\nFOLD:  1 TRAIN: [     1      2      3 ... 199996 199997 199999] TEST: [     0     11     12 ... 199988 199992 199998]\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933155\tvalid_1's auc: 0.898876\n[10000]\ttraining's auc: 0.954058\tvalid_1's auc: 0.899928\nEarly stopping, best iteration is:\n[8359]\ttraining's auc: 0.947866\tvalid_1's auc: 0.900082\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933155\tvalid_1's auc: 0.898876\n[10000]\ttraining's auc: 0.954058\tvalid_1's auc: 0.899928\nEarly stopping, best iteration is:\n[8359]\ttraining's auc: 0.947866\tvalid_1's auc: 0.900082\nTraining until validation scores don't improve for 3000 rounds.\n[5000]\ttraining's auc: 0.933155\tvalid_1's auc: 0.898876\n[10000]\ttraining's auc: 0.954058\tvalid_1's auc: 0.899928\nEarly stopping, best iteration is:\n[8359]\ttraining's auc: 0.947866\tvalid_1's auc: 0.900082\nTraining until validation scores don't improve for 3000 rounds.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stratified KFold is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. Especially for this problem, where we have an unbalanced binary classification issue, we need to keep this in mind."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\npredictions.to_csv('lgb_all_predictions.csv', index=None)\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\nsub_df.to_csv(\"lgb_submission.csv\", index=False)\noof.to_csv('lgb_oof.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfa = pd.DataFrame(np.random.randn(5, 4),\n                    columns=list('ABCD'),\n                   index=pd.date_range('20130101', periods=5))","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfa","execution_count":91,"outputs":[{"output_type":"execute_result","execution_count":91,"data":{"text/plain":"                   A         B         C         D\n2013-01-01 -1.013877  1.161512 -1.660159  1.169879\n2013-01-02 -0.590156 -0.749123 -0.638018  0.308878\n2013-01-03  0.654513 -2.166617  1.202527  1.031492\n2013-01-04 -0.358395 -0.620226 -0.041413 -0.467817\n2013-01-05  0.360603  0.271051  0.364370  1.162930","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-01</th>\n      <td>-1.013877</td>\n      <td>1.161512</td>\n      <td>-1.660159</td>\n      <td>1.169879</td>\n    </tr>\n    <tr>\n      <th>2013-01-02</th>\n      <td>-0.590156</td>\n      <td>-0.749123</td>\n      <td>-0.638018</td>\n      <td>0.308878</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>0.654513</td>\n      <td>-2.166617</td>\n      <td>1.202527</td>\n      <td>1.031492</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>-0.358395</td>\n      <td>-0.620226</td>\n      <td>-0.041413</td>\n      <td>-0.467817</td>\n    </tr>\n    <tr>\n      <th>2013-01-05</th>\n      <td>0.360603</td>\n      <td>0.271051</td>\n      <td>0.364370</td>\n      <td>1.162930</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfa.columns","execution_count":96,"outputs":[{"output_type":"execute_result","execution_count":96,"data":{"text/plain":"Index(['A', 'B', 'C', 'D'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfa.iloc[0:2][[col for col in dfa.columns if col not in ['B', 'C']] ]","execution_count":104,"outputs":[{"output_type":"execute_result","execution_count":104,"data":{"text/plain":"                   A         D\n2013-01-01 -1.013877  1.169879\n2013-01-02 -0.590156  0.308878","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-01</th>\n      <td>-1.013877</td>\n      <td>1.169879</td>\n    </tr>\n    <tr>\n      <th>2013-01-02</th>\n      <td>-0.590156</td>\n      <td>0.308878</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"col for col in df_train.columns if col not in ['target', 'ID_code']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}