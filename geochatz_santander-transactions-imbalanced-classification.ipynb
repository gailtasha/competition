{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=center><font size = 4>Santander Customer Transaction Prediction</font></h1>\n<h1 align=center><font size = 5>Model binary classifier on imbalanced data</font></h1>","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n* [Introduction/Business Problem](#introduction)\n* [Setup](#setup)\n* [Data processing and exploration](#prep)\n* [Model training and evaluation](#modeling)\n* [Submission](#predictions)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"introduction\"></a>\n# Introduction/Business Problem","metadata":{}},{"cell_type":"markdown","source":"At [Santander](https://www.santanderbank.com/us/personal) our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"setup\"></a>\n# Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow import keras\nimport sklearn\n\nimport seaborn as sns\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.rcParams['figure.figsize'] = (12, 8)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nprint('Libraries imported.')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T17:25:47.626632Z","iopub.execute_input":"2021-11-19T17:25:47.626894Z","iopub.status.idle":"2021-11-19T17:25:47.643619Z","shell.execute_reply.started":"2021-11-19T17:25:47.626866Z","shell.execute_reply":"2021-11-19T17:25:47.64259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"prep\"></a>\n# Data processing and exploration","metadata":{}},{"cell_type":"markdown","source":"We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set.\n\n**File descriptions**\n* **train.csv** - the training set.\n* **test.csv** - the test set. The test set contains some rows which are not included in scoring.\n* **sample_submission.csv** - a sample submission file in the correct format.","metadata":{}},{"cell_type":"code","source":"transactions = pd.read_csv('../input/train.csv')\nprint('train data imported.')\ntransactions.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-11-19T17:25:47.645927Z","iopub.execute_input":"2021-11-19T17:25:47.646296Z","iopub.status.idle":"2021-11-19T17:25:52.652657Z","shell.execute_reply.started":"2021-11-19T17:25:47.646234Z","shell.execute_reply":"2021-11-19T17:25:52.650549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data set contains numeric features type `float64`, besides the binary target column type `int64` and the ID_code column type `object`","metadata":{}},{"cell_type":"code","source":"print('number of unique ids {}'.format(len(transactions.ID_code.unique())))\nprint('number of rows {}'.format(len(transactions)))\nprint('number of columns {}'.format(len(transactions.columns)))\nprint('missing values: {}'.format(transactions.isna().any().any()))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:52.653898Z","iopub.execute_input":"2021-11-19T17:25:52.654204Z","iopub.status.idle":"2021-11-19T17:25:52.73949Z","shell.execute_reply.started":"2021-11-19T17:25:52.654162Z","shell.execute_reply":"2021-11-19T17:25:52.738635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each row represents one transaction. There are 200000 instances in the dataset and the id per transaction is unique as expected. The data has different type of attributes. The majority of the attributes is type numerical. In addition, the dataset contains the target and the ID_code attributes. There are no `NaN` in the transaction dataset.","metadata":{}},{"cell_type":"code","source":"transactions.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:52.740626Z","iopub.execute_input":"2021-11-19T17:25:52.740851Z","iopub.status.idle":"2021-11-19T17:25:54.529842Z","shell.execute_reply.started":"2021-11-19T17:25:52.740824Z","shell.execute_reply":"2021-11-19T17:25:54.52901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Examine the class label imbalance","metadata":{}},{"cell_type":"code","source":"neg, pos = np.bincount(transactions['target'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:54.532422Z","iopub.execute_input":"2021-11-19T17:25:54.533117Z","iopub.status.idle":"2021-11-19T17:25:54.53878Z","shell.execute_reply.started":"2021-11-19T17:25:54.533068Z","shell.execute_reply":"2021-11-19T17:25:54.537824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nsns.countplot(x=transactions.target)\nplt.title('Class imbalance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:54.539746Z","iopub.execute_input":"2021-11-19T17:25:54.539944Z","iopub.status.idle":"2021-11-19T17:25:54.717915Z","shell.execute_reply.started":"2021-11-19T17:25:54.539922Z","shell.execute_reply":"2021-11-19T17:25:54.717069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the total number of transactions 10% belongs to 1 (the mean in the `target` attribute), so the classes are imbalanced and accuracy won't be a reasonable metric to evaluate our model","metadata":{}},{"cell_type":"markdown","source":"### Clean, split and normalize the data","metadata":{}},{"cell_type":"code","source":"transactions.drop(['ID_code'], axis=1, inplace=True)\ntransactions.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:54.718933Z","iopub.execute_input":"2021-11-19T17:25:54.71916Z","iopub.status.idle":"2021-11-19T17:25:54.839995Z","shell.execute_reply.started":"2021-11-19T17:25:54.719125Z","shell.execute_reply":"2021-11-19T17:25:54.839158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrows = 2\nncols = 2\n\nfig = plt.gcf()\nfig.set_size_inches(ncols*7, nrows*6)\n\nfor i, stat in enumerate(['mean', 'std', 'max', 'min']):\n    ax = plt.subplot(nrows, ncols, i + 1)\n    sns.histplot(transactions.drop(['target'], axis=1).describe().loc[stat], ax= ax)\n    ax.set(xlabel=None)\n    plt.title('Distribution of numeric features {} values'.format(stat.upper()))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:25:54.841274Z","iopub.execute_input":"2021-11-19T17:25:54.841553Z","iopub.status.idle":"2021-11-19T17:26:03.300794Z","shell.execute_reply.started":"2021-11-19T17:25:54.841527Z","shell.execute_reply":"2021-11-19T17:26:03.300012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above analysis indicates that the numerical features have different scales, thus feature scaling should be considered in ML algorithms.","metadata":{}},{"cell_type":"markdown","source":"Split the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where overfitting is a significant concern from the lack of training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Use a utility from sklearn to split and shuffle your dataset.\ntrain_df, test_df = train_test_split(transactions, test_size=0.2, stratify=transactions.target)\ntrain_df, valid_df = train_test_split(train_df, test_size=0.2, stratify=train_df.target)\n\n\nX_train, y_train = train_df.drop(['target'], axis=1).values, train_df.target.values\nX_valid, y_valid = valid_df.drop(['target'], axis=1).values, valid_df.target.values\nX_test, y_test = test_df.drop(['target'], axis=1).values, test_df.target.values\n\nbool_train_labels = y_train != 0\n\nprint('Train features shape: {}'.format(X_train.shape))\nprint('Valid features shape: {}'.format(X_valid.shape))\nprint('Test features shape: {}'.format(X_test.shape))\n\nprint('\\nTrain labels shape: {}'.format(y_train.shape))\nprint('Valid labels shape: {}'.format(y_valid.shape))\nprint('Test labels shape: {}'.format(y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:03.301779Z","iopub.execute_input":"2021-11-19T17:26:03.301979Z","iopub.status.idle":"2021-11-19T17:26:04.036556Z","shell.execute_reply.started":"2021-11-19T17:26:03.301955Z","shell.execute_reply":"2021-11-19T17:26:04.035712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize the input features using the sklearn StandardScaler. This will set the mean to 0 and standard deviation to 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\n\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:04.038067Z","iopub.execute_input":"2021-11-19T17:26:04.038505Z","iopub.status.idle":"2021-11-19T17:26:04.633239Z","shell.execute_reply.started":"2021-11-19T17:26:04.038461Z","shell.execute_reply":"2021-11-19T17:26:04.632343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Look at the data distribution","metadata":{}},{"cell_type":"code","source":"pos_df = pd.DataFrame(X_train[bool_train_labels], columns=train_df.drop(['target'], axis=1).columns)\nneg_df = pd.DataFrame(X_train[~bool_train_labels], columns=train_df.drop(['target'], axis=1).columns)\n\nsns.jointplot(x=pos_df['var_5'], y=pos_df['var_6'], kind='hex')\nplt.suptitle(\"Positive distribution\")\n\nsns.jointplot(x=neg_df['var_5'], y=neg_df['var_6'], kind='hex')\n_ = plt.suptitle(\"Negative distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:04.634718Z","iopub.execute_input":"2021-11-19T17:26:04.635002Z","iopub.status.idle":"2021-11-19T17:26:06.494703Z","shell.execute_reply.started":"2021-11-19T17:26:04.634964Z","shell.execute_reply":"2021-11-19T17:26:06.493902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"modeling\"></a>\n# Model training and evaluation","metadata":{}},{"cell_type":"markdown","source":"## Define the model and metrics\nCreate a simple neural network with two densly connected hidden layers, dropout layers to reduce overfitting, and an output sigmoid layer that returns the probability of a customer being target:","metadata":{}},{"cell_type":"code","source":"METRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\ndef build_model(metrics=METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = keras.initializers.Constant(output_bias)\n\n    model = keras.models.Sequential([\n        keras.layers.Dense(100, activation='relu', input_shape=[X_train.shape[-1]]),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(50, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n    ])\n    \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.BinaryCrossentropy(),metrics=metrics)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:06.496318Z","iopub.execute_input":"2021-11-19T17:26:06.496711Z","iopub.status.idle":"2021-11-19T17:26:06.528046Z","shell.execute_reply.started":"2021-11-19T17:26:06.496673Z","shell.execute_reply":"2021-11-19T17:26:06.527442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Understanding useful metrics\n\nNotice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n\n\n*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n*   **True** negatives and **true** positives are samples that were **correctly** classified\n*   **Accuracy** is the percentage of examples correctly classified\n>   $\\frac{\\text{true samples}}{\\text{total samples}}$\n*   **Precision** is the percentage of **predicted** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n*   **Recall** is the percentage of **actual** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n*   **AUPRC** refers to Area Under the Curve of the Precision-Recall Curve. This metric computes precision-recall pairs for different probability thresholds. \n","metadata":{}},{"cell_type":"markdown","source":"## Baseline Model\n\n### Build the model\n\nNow create and train your model using the function that was defined earlier. Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no positive customers to learn from.\n\n**Set the correct initial bias**\n\nThese initial guesses are not great. We know the dataset is imbalanced. Set the output layer's bias to reflect that. The correct bias to set can be derived from:\n\n$$ p_0 = pos/(pos + neg) = 1/(1+e^{-b_0}) $$\n$$ b_0 = -log_e(1/p_0 - 1) $$\n$$ b_0 = log_e(pos/neg)$$\n","metadata":{}},{"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_prc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\n\n# calculate initial bias\ninitial_bias = np.log([pos/neg])\n\nbaseline_model = build_model(output_bias=initial_bias)\nbaseline_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:06.52926Z","iopub.execute_input":"2021-11-19T17:26:06.529655Z","iopub.status.idle":"2021-11-19T17:26:06.570582Z","shell.execute_reply.started":"2021-11-19T17:26:06.529619Z","shell.execute_reply":"2021-11-19T17:26:06.569829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_history = baseline_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping], validation_data=(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:06.573133Z","iopub.execute_input":"2021-11-19T17:26:06.57377Z","iopub.status.idle":"2021-11-19T17:26:22.298068Z","shell.execute_reply.started":"2021-11-19T17:26:06.573728Z","shell.execute_reply":"2021-11-19T17:26:22.297518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation\n\n**Check training history**\n\nI will produce plots of your model's accuracy and loss on the training and validation set. These are useful to check for overfitting.","metadata":{}},{"cell_type":"code","source":"def plot_metrics(history):\n    metrics = ['loss', 'prc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle=\"--\", label='Valid')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':      \n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n        plt.legend()\n\nplot_metrics(baseline_history)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:22.299331Z","iopub.execute_input":"2021-11-19T17:26:22.299658Z","iopub.status.idle":"2021-11-19T17:26:22.886041Z","shell.execute_reply.started":"2021-11-19T17:26:22.299621Z","shell.execute_reply":"2021-11-19T17:26:22.885168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate metrics**\n\nI use a confusion matrix to summarize the actual vs. predicted labels, where the X axis is the predicted label and the Y axis is the actual label:","metadata":{}},{"cell_type":"code","source":"train_predictions_baseline = baseline_model.predict(X_train, batch_size=BATCH_SIZE)\ntest_predictions_baseline = baseline_model.predict(X_test, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:22.88759Z","iopub.execute_input":"2021-11-19T17:26:22.888181Z","iopub.status.idle":"2021-11-19T17:26:23.388398Z","shell.execute_reply.started":"2021-11-19T17:26:22.888138Z","shell.execute_reply":"2021-11-19T17:26:23.387629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    \n    print('Non-Target Customers Detected (True Negatives): ', cm[0][0])\n    print('Non-Target Customers Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Target Customers Missed (False Negatives): ', cm[1][0])\n    print('Target Customers Detected (True Positives): ', cm[1][1])\n    print('Total Target Customers: ', np.sum(cm[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:23.389601Z","iopub.execute_input":"2021-11-19T17:26:23.389798Z","iopub.status.idle":"2021-11-19T17:26:23.395404Z","shell.execute_reply.started":"2021-11-19T17:26:23.389775Z","shell.execute_reply":"2021-11-19T17:26:23.39485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_results = baseline_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n\nfor name, value in zip(baseline_model.metrics_names, baseline_results):\n    print(name, ': ', value)\nprint()\n\nplot_cm(y_test, test_predictions_baseline)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:23.39631Z","iopub.execute_input":"2021-11-19T17:26:23.396615Z","iopub.status.idle":"2021-11-19T17:26:23.842985Z","shell.execute_reply.started":"2021-11-19T17:26:23.396585Z","shell.execute_reply":"2021-11-19T17:26:23.842169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the ROC\n\nThis plot is useful because it shows, at a glance, the range of performance the model can reach just by tuning the output threshold.","metadata":{}},{"cell_type":"code","source":"def plot_roc(name, labels, predictions, **kwargs):\n    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n    plt.xlabel('False positives [%]')\n    plt.ylabel('True positives [%]')\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')\n    \nplot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:23.844616Z","iopub.execute_input":"2021-11-19T17:26:23.845149Z","iopub.status.idle":"2021-11-19T17:26:24.048901Z","shell.execute_reply.started":"2021-11-19T17:26:23.845085Z","shell.execute_reply":"2021-11-19T17:26:24.04813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class weights\n\n### Calculate class weights\n\nThe goal is to identify fraudulent transactions, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","metadata":{}},{"cell_type":"code","source":"# Scaling by total/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 / neg) * (total / 2.0)\nweight_for_1 = (1 / pos) * (total / 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:24.050543Z","iopub.execute_input":"2021-11-19T17:26:24.050834Z","iopub.status.idle":"2021-11-19T17:26:24.057508Z","shell.execute_reply.started":"2021-11-19T17:26:24.050794Z","shell.execute_reply":"2021-11-19T17:26:24.056663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train a model with class weights\n\nNow try re-training and evaluating the model with class weights to see how that affects the predictions.","metadata":{}},{"cell_type":"code","source":"weighted_model = build_model(output_bias=initial_bias)\n\nweighted_history = weighted_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping], \n                                      validation_data=(X_valid, y_valid),\n                                      class_weight=class_weight)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:24.05904Z","iopub.execute_input":"2021-11-19T17:26:24.059355Z","iopub.status.idle":"2021-11-19T17:26:36.650287Z","shell.execute_reply.started":"2021-11-19T17:26:24.059315Z","shell.execute_reply":"2021-11-19T17:26:36.649547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check training history","metadata":{}},{"cell_type":"code","source":"plot_metrics(weighted_history)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:36.651469Z","iopub.execute_input":"2021-11-19T17:26:36.651671Z","iopub.status.idle":"2021-11-19T17:26:37.231019Z","shell.execute_reply.started":"2021-11-19T17:26:36.651647Z","shell.execute_reply":"2021-11-19T17:26:37.230135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate metrics","metadata":{}},{"cell_type":"code","source":"train_predictions_weighted = weighted_model.predict(X_train, batch_size=BATCH_SIZE)\ntest_predictions_weighted = weighted_model.predict(X_test, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:37.232787Z","iopub.execute_input":"2021-11-19T17:26:37.233232Z","iopub.status.idle":"2021-11-19T17:26:37.712918Z","shell.execute_reply.started":"2021-11-19T17:26:37.23319Z","shell.execute_reply":"2021-11-19T17:26:37.712141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weighted_results = weighted_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(weighted_model.metrics_names, weighted_results):\n    print(name, ': ', value)\nprint()\nplot_cm(y_test, test_predictions_weighted)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:37.714021Z","iopub.execute_input":"2021-11-19T17:26:37.714251Z","iopub.status.idle":"2021-11-19T17:26:38.141359Z","shell.execute_reply.started":"2021-11-19T17:26:37.714224Z","shell.execute_reply":"2021-11-19T17:26:38.140565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the ROC","metadata":{}},{"cell_type":"code","source":"plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[1])\nplot_roc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:38.142774Z","iopub.execute_input":"2021-11-19T17:26:38.143386Z","iopub.status.idle":"2021-11-19T17:26:38.395426Z","shell.execute_reply.started":"2021-11-19T17:26:38.143343Z","shell.execute_reply":"2021-11-19T17:26:38.394811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Oversampling\n\n### Oversample the minority class using numpy\n\nA related approach would be to resample the dataset by oversampling the minority class.","metadata":{}},{"cell_type":"code","source":"pos_features = X_train[bool_train_labels]\nneg_features = X_train[~bool_train_labels]\n\npos_labels = y_train[bool_train_labels]\nneg_labels = y_train[~bool_train_labels]\n\nprint('Positive features shape: {}'.format(pos_features.shape))\nprint('Negative features shape: {}'.format(neg_features.shape))\n\nprint('\\nPositive labels shape: {}'.format(pos_labels.shape))\nprint('Negative labels shape: {}'.format(neg_labels.shape))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:38.396425Z","iopub.execute_input":"2021-11-19T17:26:38.396727Z","iopub.status.idle":"2021-11-19T17:26:38.575567Z","shell.execute_reply.started":"2021-11-19T17:26:38.3967Z","shell.execute_reply":"2021-11-19T17:26:38.574579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = np.arange(len(pos_features))\nchoices = np.random.choice(ids, len(neg_features))\n\nres_pos_features = pos_features[choices]\nres_pos_labels = pos_labels[choices]\n\nprint('Resampled Positive features shape: {}'.format(res_pos_features.shape))\nprint('Resampled Positive labels shape: {}'.format(res_pos_labels.shape))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:38.576899Z","iopub.execute_input":"2021-11-19T17:26:38.57712Z","iopub.status.idle":"2021-11-19T17:26:38.649472Z","shell.execute_reply.started":"2021-11-19T17:26:38.577083Z","shell.execute_reply":"2021-11-19T17:26:38.648525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\nresampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n\norder = np.arange(len(resampled_labels))\nnp.random.shuffle(order)\nresampled_features = resampled_features[order]\nresampled_labels = resampled_labels[order]\n\nresampled_features.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:38.650854Z","iopub.execute_input":"2021-11-19T17:26:38.651166Z","iopub.status.idle":"2021-11-19T17:26:38.94173Z","shell.execute_reply.started":"2021-11-19T17:26:38.651125Z","shell.execute_reply":"2021-11-19T17:26:38.940903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train on the oversampled data\n\nNow try training the model with the resampled data set instead of using class weights to see how these methods compare.","metadata":{}},{"cell_type":"code","source":"resampled_model = build_model(output_bias=[0])\n\nresampled_history = resampled_model.fit(resampled_features, resampled_labels, batch_size=BATCH_SIZE, \n                                        epochs=EPOCHS, callbacks=[early_stopping], validation_data=(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:38.943074Z","iopub.execute_input":"2021-11-19T17:26:38.943546Z","iopub.status.idle":"2021-11-19T17:26:56.999462Z","shell.execute_reply.started":"2021-11-19T17:26:38.943503Z","shell.execute_reply":"2021-11-19T17:26:56.998863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check training history","metadata":{}},{"cell_type":"code","source":"plot_metrics(resampled_history)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:26:57.000576Z","iopub.execute_input":"2021-11-19T17:26:57.001086Z","iopub.status.idle":"2021-11-19T17:27:02.027004Z","shell.execute_reply.started":"2021-11-19T17:26:57.001054Z","shell.execute_reply":"2021-11-19T17:27:02.026151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate metrics","metadata":{}},{"cell_type":"code","source":"train_predictions_resampled = resampled_model.predict(X_train, batch_size=BATCH_SIZE)\ntest_predictions_resampled = resampled_model.predict(X_test, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:02.028538Z","iopub.execute_input":"2021-11-19T17:27:02.028861Z","iopub.status.idle":"2021-11-19T17:27:02.513463Z","shell.execute_reply.started":"2021-11-19T17:27:02.028822Z","shell.execute_reply":"2021-11-19T17:27:02.512628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resampled_results = resampled_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(resampled_model.metrics_names, resampled_results):\n    print(name, ': ', value)\nprint()\n\nplot_cm(y_test, test_predictions_resampled)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:02.514692Z","iopub.execute_input":"2021-11-19T17:27:02.514997Z","iopub.status.idle":"2021-11-19T17:27:02.946042Z","shell.execute_reply.started":"2021-11-19T17:27:02.514958Z","shell.execute_reply":"2021-11-19T17:27:02.945275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the ROC","metadata":{}},{"cell_type":"code","source":"plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[1])\nplot_roc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n\nplot_roc(\"Train Resampled\", y_train, train_predictions_resampled, color=colors[2])\nplot_roc(\"Test Resampled\", y_test, test_predictions_resampled, color=colors[2], linestyle='--')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:02.947707Z","iopub.execute_input":"2021-11-19T17:27:02.947998Z","iopub.status.idle":"2021-11-19T17:27:03.257696Z","shell.execute_reply.started":"2021-11-19T17:27:02.94796Z","shell.execute_reply":"2021-11-19T17:27:03.256806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"predictions\"></a>\n# Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\nprint('test data imported.')\nprint(test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:03.259173Z","iopub.execute_input":"2021-11-19T17:27:03.259496Z","iopub.status.idle":"2021-11-19T17:27:08.460568Z","shell.execute_reply.started":"2021-11-19T17:27:03.259456Z","shell.execute_reply":"2021-11-19T17:27:08.459741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_df.drop(['ID_code'], axis=1)\nX_test = scaler.transform(X_test)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:08.461513Z","iopub.execute_input":"2021-11-19T17:27:08.461702Z","iopub.status.idle":"2021-11-19T17:27:08.778344Z","shell.execute_reply.started":"2021-11-19T17:27:08.461679Z","shell.execute_reply":"2021-11-19T17:27:08.777436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['target'] = (weighted_model.predict(X_test, batch_size=BATCH_SIZE) > 0.5).astype(int)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:08.779553Z","iopub.execute_input":"2021-11-19T17:27:08.779756Z","iopub.status.idle":"2021-11-19T17:27:09.192507Z","shell.execute_reply.started":"2021-11-19T17:27:08.779732Z","shell.execute_reply":"2021-11-19T17:27:09.191699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['ID_code', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:09.193624Z","iopub.execute_input":"2021-11-19T17:27:09.193846Z","iopub.status.idle":"2021-11-19T17:27:09.424255Z","shell.execute_reply.started":"2021-11-19T17:27:09.193819Z","shell.execute_reply":"2021-11-19T17:27:09.423621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\n[Tensorflow tutorial | imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}