{"cells":[{"metadata":{"_uuid":"7689ad79c1d585537791dc6e8e5e0878644dd4bf"},"cell_type":"markdown","source":"Hi there guys. <br />\nI'm a **person who have jumped into kaggle three month ago**. During studying many enlighting expert's kernels, I've felt kind of **embarrassed feeling** about using hyperparameters of major algorithms such as Xgboost and LightGBM. You guys could reply my opinion like this, **\"Why you blame your fault to them?\"** \n### But, I definitely have **HUGE THANKS TO THEM!!** **Thanks to SUPER BRILLIANT EXPERTS OF KAGGLE** <br />\n\nThe reason why I make this kernel is that some people use **\"lightgbm.train\"** and the others use **\"lightgbm.LGBMClassifier\"** for their model. When I see the differences of them, It makes me insane!! Because the **parameters btw two kinds of kernel as I said above seem pretty different!!** <br />\n\nSo, In this kernel, I'll discover <br />\n**1. the true meaning of them and aliases of hyperparameters by looking official document of lightgbm.** <br />\n**2. parameter tuning by referring two website where I'll comment below notebooks** <br />\n\n### I hope that **two kinds of people** to see this kernel,\n\n1. **One is for people who have felt simliar feeling like me.** For them, I'll describe as detail as I can what I've learned and I'd like to share magnificant post for explaining what the Gradient Boosting and the Xgboost are!!<br />\n\nThe posts are below!!\n* Gradient Boost\n>  https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n* Xgboost\n> https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n\n2. **I hope THE OTHERS are enlighting experts** who could give comment about what I understands through this kernel, I would really happy if you guys comment this kernel !! and Could you guys give me a great post about LightGBM parameter or parmeter tuning?? cuz I already have few posts about GBM and XGBoost but I don't have about LightGBM!! (I know generally it seems same one but I think there is regularization in LightGBM) \n\nThere is Korean comments for my studying for each sentences by Gabriel Preda's explaination. But I didn't only copy and paste this code. I've changed some code for my own!! \n\n# I'm staying to tune hyperparameters and I will frequently update this Kernel frequently!!."},{"metadata":{"_uuid":"6774d6c76f6c90db7018c89e81273695b354c7a2"},"cell_type":"markdown","source":"# Reference\n\nGabriel Preda's santander-eda-and-prediction\n> https://www.kaggle.com/gpreda/santander-eda-and-prediction\nthis kernel uses lightgbm.train for prediction\n\nWill Koehrsen's A Complete Introduction and Walkthrough [Costa Rican Houshold] \n> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\nthis kernel uses LGBMClassifier for prediction\n\nRudolph's Porto: xgb+lgb kfold LB 0.282 [Porto Seguro’s Safe Driver Prediction]\n> https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282\nthis kernel uses lightgbm.train for prediction"},{"metadata":{"_uuid":"12e7e6563a81e4953ded9e809e59e8b91eb5fd01"},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n\n이번 컴피티션에서는 어느 소비자들이 훗날에 현금을 인출할 것인지를 구분하는 것이 목표입니다. 이번 대회의 데이터는 실제 데이터와 같은 구조로 제공되어있습니다.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n\n데이터는 무기명으로 되어있고 각각의 row는 200개의 서로 다른 컬럼을 가지고 있습니다.\n\nIn the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n\n다음에서 우리는 데이터를 살펴보고, 모델링 준비를하고, 모델을 훈련시키고 타겟 값을 테스트셋에서 예측하고 제출까지 해솝시다."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Load data   \n\nLet's check what data files are available.\n\n우리가 사용가능한 데이터 파일들을 알아 봅시다."},{"metadata":{"trusted":true,"_uuid":"01b7ef93be4b7c674a0ddf10f01cba6f4948af81"},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"../input/Santander/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41aca8e348e9c9ea425624465c60467b45794acf"},"cell_type":"markdown","source":"Let's load the train and test data files."},{"metadata":{"trusted":true,"_uuid":"e8acb64caa294e8ceced10f4864c7ed557188da6"},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2262ca673e6d8ff98e169cc3dc680a91308e260d"},"cell_type":"markdown","source":"# <a id='3'>Data exploration</a>  \n\n## <a id='31'>Check the data</a>  \n\nLet's check the train and test set.\n\n훈련셋과 테스트셋을 확인해봅시다."},{"metadata":{"trusted":true,"_uuid":"30419aab68318f7cc3f3b91ff4418653778bac6e"},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e287be469179172613588da601ca22099d09b4"},"cell_type":"markdown","source":"Both train and test data have 200,000 entries and 202, respectivelly 201 columns. \n\n훈련셋과 테스트셋 모두 200,000개의 행을가지고 각각 202, 201 개의 컬럼수를 가지고 있습니다.\n\nLet's glimpse train and test dataset.\n\n간단하게 두 세트를 살펴볼까요.\n"},{"metadata":{"trusted":true,"_uuid":"38ca151ba4733c38aa7417a6f09bcea5f19ad009"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5ee35bebbb9c4686a6de8d1cb2f0df482dcf633"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f31f2eadde6ce6d0e5276487ec359caa8e1841a0"},"cell_type":"markdown","source":"Train contains:  \n\n* **ID_code** (string);  \n* **target**;  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\n훈련세트는. ID, Target 그리고 200개의 숫자값들이 있습니다.\n\nTest contains:  \n\n* **ID_code** (string);  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\n테스트 셋에는 타겟값을 제외한 것들이 있습니다.\n\nLet's check if there are any missing data. We will also chech(*k) the type of data.\n\n손실값들에 대해서 한번 살펴볼까요> 그리고 데이터들의 타입에 대해서도 알아봅시다.\n\nWe check first train.\n\n먼저 훈련세트입니다."},{"metadata":{"trusted":true,"_uuid":"911af9ce864265e5fb3cec557fbd6840eea64229"},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return (np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edc5ebb4774343475bc4d28288d1e58ed0688453"},"cell_type":"code","source":"%%time\nmissing_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a69fabd8c0b9f58863617a4dc7062615dea1619d"},"cell_type":"code","source":"%%time\nmissing_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d28158daa8f5a5e23551e2aa8f383bc04b2a090"},"cell_type":"code","source":"%%time\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d862e23da464a5f660d9cd0108d08c124ded177f"},"cell_type":"code","source":"%%time\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1faffa45e7b72aff6a217cb999615077d77cd513"},"cell_type":"markdown","source":"We can make few observations here:   \n우리가 찾은 것들은 아래와 같습니다.\n\n* standard deviation is relatively large for both train and test variable data;\n훈련 데이터와 테스트 데이터 모두 표준편차가 크다는 것\n* min, max, mean, sdt values for train and test data looks quite close;\n최소,최대,평균,표준편차 값이 훈련과 테스트셋에서 밀접해 보인다는 것\n* mean values are distributed over a large range.\n평균값의 변동이 크다는 것\n\nThe number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n훈련과 테스트 셋에서의 값의 수는 동일하다. 그렇다면 몇몇 특징들에 대해서 산포도를 그려봅시다.\n"},{"metadata":{"trusted":true,"_uuid":"3628068560e373e116d364380d96b2a5e1032667"},"cell_type":"code","source":"def plot_feature_scatter(df1,df2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig,ax=plt.subplots(4,4,figsize=[14,14])\n    \n    for feature in features:\n        i+=1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature],df2[feature],marker='+')\n        plt.xlabel(feature,fontsize=9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c017fe6c41d222297f98f2e5c6c194426b17f1b"},"cell_type":"code","source":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(train_df[::20],test_df[::20], features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a94f4b90dce4145f83191583808e31c02ed5aae"},"cell_type":"code","source":"sns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07247c1e8975feaf9b2df52f438045f181b9d3b9"},"cell_type":"code","source":"print(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]/train_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b46e62cd7659a54b808c5e8c47c634f73ec27f7"},"cell_type":"markdown","source":"\n## <a id='32'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**."},{"metadata":{"trusted":true,"_uuid":"e37d2afdc0a00fa32e11ae70180d405ab5ba1a3d"},"cell_type":"code","source":"def plot_feature_distribution(df1,df2,label1,label2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig,ax = plt.subplots(10,10,figsize=[18,22])\n    \n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature],bw=0.5,label=label1)\n        sns.kdeplot(df2[feature],bw=0.5,label=label2)\n        plt.xlabel(feature,fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n        plt.tick_params(axis='y',which='major',labelsize=6)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bbd8679f1b9871ffa8d7bcf7e3e6889f6f65a14"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target']==0]\nt1 = train_df.loc[train_df['target']==1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0,t1,'0','1',features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a827c1d0e90ee61ee159b3acee8a3d965ca1c990"},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7378f50464e6b055bb07ee6dda7bddd82e660442"},"cell_type":"markdown","source":"We can observe that there is a considerable number of features with significant different distribution for the two target values.  \nFor example, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** and many others.\n\n우리는 두 개의 타겟값에 따라서 상당이 다른 분포를 가지고 있는 상당한 수의 특징들을 살펴볼 수 있습니다.\n예를 들면, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** 와 다른 것들 말입니다.\n\nAlso some features, like **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196** shows a distribution that resambles to a bivariate distribution.\n\n그리고 몇몇 특징들, **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196**, 은 이변량분포와 닮은 분포를 보여줍니다.\n\nWe will take this into consideration in the future for the selection of the features for our prediction model.  \n\n우리는 이것들을 우리의 예측모델에 feature selection시에 고려하는 참고자료로 사용할 것입니다.\n\nLe't s now look to the distribution of the same features in parallel in train and test datasets. \n\n그렇다면 이제는 훈련셋과 테스트셋을 평행적으로 같이 보겠습니다.\n\nThe first 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots.\n\n첫 번째 100개의 값들은 아래의 그림과 같이 생겼습니다."},{"metadata":{"trusted":true,"_uuid":"c7199d8d0f94821c8e315e83710a7c2528edbacf"},"cell_type":"code","source":"features = train_df.columns.values[2:102]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be1144074ad375c63637c5cf40fdbca1b4671ad5"},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eae985dbeeb12c5f321e51e0674a8d549513a254"},"cell_type":"markdown","source":"The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n\n훈련셋과 테스트셋은 numeric 변수들의 분포들이 잘 균형을 갖추고 있는 듯 합니다.\n\n## <a id='33'>Distribution of mean and std</a>  \n평균과 표준편차의 분포\n\nLet's check the distribution of the mean values per row in the train and test set.\n\n행별로 훈련과 테스트셋의 평균 값의 분포를 알아봅시다."},{"metadata":{"trusted":true,"_uuid":"8d29c17a27a147e331804b5f1fe2ad104ae3b8bf"},"cell_type":"code","source":"plt.figure(figsize=[6,6])\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\",kde=True,bins=120,label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\",kde=True,bins=120,label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a875fa838dcb85f52c4a4119d400c685196b6510"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"151c55a54dbeb96a53d6f39fdc5ab551656266e7"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d9f35081efc8a8cafa3e24a3de8493a80ecd83"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d81dd820f2e2397c9b2cb003695479470338996"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b97c93323db86c4c0f16882e49ac14a9519a4913"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28e3820bc6b0ac34bfea0e1c5389449bfe627138"},"cell_type":"markdown","source":"## <a id='34'>Features correlation</a>  컬럼간 상관관계\n\nWe calculate now the correlations between the features in train set.  \nThe following table shows the first 10 the least correlated features.\n\n우리는 훈련세트에 컬럼간에 상관관계를 계산해보려고합니다. \n아래의 테이블은 처음 10개의 상관관계 특징들을 보여줍니다.\n\n\n> Reference from about guidelines about correlations <br />https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n\n#### The general guidelines for correlation values are below, but these will change depending on who you ask (source for these)\n\n* 00-.19 “very weak” <br />\n* 20-.39 “weak” <br />\n* 40-.59 “moderate” <br />\n* 60-.79 “strong” <br />\n* 80-1.0 “very strong” <br />\n\nWhat these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n\nIn that Kernel, he droped one of the columns what have high corrleation between them above 0.95\nSo, I'd like to drop them also here.\nBut we don't have any columns what I told above. So I don't delete anything about 200 coulmns"},{"metadata":{"trusted":true,"_uuid":"901c9da9843c27e324289c770b9da59c65c76a07"},"cell_type":"code","source":"#I think this code is better than orgin code\n\ncorrelations = train_df[features].corr().where(np.triu(np.ones(train_df[features].corr().shape),k=1).astype(np.bool))\ncorrelations_df = correlations.abs().unstack().dropna().sort_values().reset_index()\ncorrelations_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a0c3f90355cb7ffba13d5b24fcb549827b1085f"},"cell_type":"code","source":"correlations_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"775d51033e72cab9342e9209ab3440b8ea990bbb"},"cell_type":"code","source":"correlations_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"948efc71cd37f29afca35e96e1c2403baa5431c9"},"cell_type":"code","source":"[col for col in correlations.columns if any(abs(correlations[col])>0.95)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7c8bc6eb874e1a91e920d772d219e469bd16be4"},"cell_type":"markdown","source":"The correlation between the features is very small. \n\n## <a id='35'>Duplicate values</a>  중복값 처리\n\nLet's now check how many duplicate values exists per columns.\n\n컬럼당 얼마나 중복된 값들이 있는지 확인 해보자"},{"metadata":{"trusted":true,"_uuid":"b9aa59f04369ff99cf5e594bcb2c6ab2a0743f16"},"cell_type":"code","source":"%%time\nfeatures = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature,values.max(),values.idxmax()])\n\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature,values.max(),values.idxmax()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb74734c419f563d0184a17b6b5b0926de23ce57"},"cell_type":"code","source":"np.transpose(pd.DataFrame(unique_max_train,columns=['Feature','Max duplicates','Values']).sort_values(by='Max duplicates',ascending=False).head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4faaea7deb8130ad91e5d37868b24c14dcaa8486"},"cell_type":"code","source":"np.transpose(pd.DataFrame(unique_max_test,columns=['Feature','Max duplicates','Values']).sort_values(by='Max duplicates',ascending=False).head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63b8e4ac1217a4475c5e460f0cd4ba5e4c12cadd"},"cell_type":"markdown","source":"Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.\n\n훈련세트와 테스트세트에서 같은 컬럼들이 같거나 가까운 양의 중복값을 가지며 이 중복값의 값 또한 같거나 비슷했다. 이는 나중에 사용하기에도 흥미로운 패턴이다."},{"metadata":{"_uuid":"7cb482415d7e0db3171b50485c0c9ffb80f49207"},"cell_type":"markdown","source":"# <a id='4'>Feature engineering</a>  \n\nThis section is under construction.  \n\nLet's calculate for starting few aggregated values for the existing features."},{"metadata":{"trusted":true,"_uuid":"d7ad6474d830504dff44d0cf1ed5989193693aea"},"cell_type":"code","source":"%%time\n\ni = 1\nfor df in [test_df, train_df]:\n    idx = df.columns.values[i:i+200]\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n    df['range'] = df['max']-df['min']\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d05508cd3f81f9f0b9d6031319c4874d24b800d3"},"cell_type":"code","source":"train_df[train_df.columns[202:]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64095873404cda01c3fee36d5e86447f9c1c1123"},"cell_type":"code","source":"test_df[test_df.columns[201:]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87670d9159d3f15c67c84488b1f423634eccaae4"},"cell_type":"code","source":"features = train_df.columns.values[2:]\ncorrelations = train_df[features].corr().where(np.triu(np.ones(train_df[features].corr().shape),k=1).astype(np.bool))\ncorrelations_df = correlations.abs().stack().reset_index().rename(columns={0:'corr'}).sort_values(by='corr')\ncorrelations_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8fda566fa4c9c0fe991924be4854e45da439d6"},"cell_type":"code","source":"correlations_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c75c753eea67918d1bc48d933c69c160b4397836"},"cell_type":"code","source":"correlations_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"251a548d3f21c5bdefd6eddb653c125e8ac86e91"},"cell_type":"code","source":"drop_cols = [col for col in correlations.columns if any(abs(correlations[col])>0.95)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84eaf2ed1608e3e9aa3266ad72225752923ad9c5"},"cell_type":"markdown","source":"sum has perfect correaltion wth mean. So, I'd like to delete sum instead of mean."},{"metadata":{"trusted":true,"_uuid":"7a4369cbc337d3bca3aeaf889ea07f7ad621dddb"},"cell_type":"code","source":"print(\"Shape of train_df: {}, test_df: {}\".format(train_df.shape,test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8cc72b15f9409de535595e058ed8b7e3f61c456"},"cell_type":"code","source":"train_df = train_df.drop(columns=drop_cols)\ntest_df = test_df.drop(columns=drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf22f1a0c1560db9bd1f7a33e256f87a69ddf58"},"cell_type":"code","source":"print(\"Shape of train_df: {}, test_df: {}\".format(train_df.shape,test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d346f05be37b924dda4cb514e534593aff59978b"},"cell_type":"code","source":"def plot_new_feature_distribution(df1,df2,label1,label2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    fig, ax = plt.subplots(2,4,figsize=[18,8])\n    \n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature],bw=0.5,label=label1)\n        sns.kdeplot(df2[feature],bw=0.5,label=label2)\n        plt.xlabel(feature,fontsize=11)\n        locs, lables = plt.xticks()\n        plt.tick_params(axis=\"x\",which=\"major\",labelsize=8)\n        plt.tick_params(axis=\"y\",which=\"major\",labelsize=8)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6ec36d19530067320974ee669bd6ad8b5695c18"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6614f92eae942fe5a6414be37ad1cc524220e41"},"cell_type":"code","source":"features = train_df.columns.values[202:]\nplot_new_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb48579d8c2f171284d204643ca235260836c532"},"cell_type":"code","source":"print('Train and test columns: {} {}'.format(len(train_df.columns), len(test_df.columns)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2634147a7ea61ff76313034fb6985638b1fe19fa"},"cell_type":"markdown","source":"# Feature Selection\n\n**In here, I'd like to select features via SFM and REFCV but I couldn't. Because this data set is so huge as you guys know!! So this I'll try later..."},{"metadata":{"trusted":true,"_uuid":"4fb4f8b22a4b740d8e1853b4e85d6c3aa39d5991"},"cell_type":"code","source":"train = train_df.drop(columns=['ID_code','target'])\ntrain_label = train_df['target']\ntest = test_df.drop(columns='ID_code')\ntest_ids = test_df['ID_code']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a040e8b6b889a3aed2054cad4d9105eb15084579"},"cell_type":"markdown","source":"## SFM"},{"metadata":{"trusted":true,"_uuid":"939e4f1e5e045001dfaf407eaa0496cc3807e1b5"},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import make_scorer,roc_auc_score\n# from sklearn.model_selection import cross_val_score\n\n# scorer = make_scorer(roc_auc_score,greater_is_better=True)\n\n# rf = RandomForestClassifier(random_state=12,n_estimators=15000,n_jobs=-1)\n\n# rf.fit(train,train_label)\n\n# # cv_score = cross_val_score(rf,train,train_label,cv=5,scoring=scorer)\n# # print(f\"5 fold cv score is {cv_score.mean()}\")\n\n# indices = np.argsort(rf.feature_importances_)[::-1]\n# feature_names = train.columns\n# importances = rf.feature_importances_\n\n# df = pd.DataFrame(columns=['feature','importance'])\n# df['feature'] = feature_names\n# df['importance'] = importances\n\n\n# df.sort_values(by='importance',ascending=False).tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91558a654936f338045eed7b14fafc1e175bd475"},"cell_type":"markdown","source":"## REFCV"},{"metadata":{"trusted":true,"_uuid":"6181df4acc4bbe7da9d362fb0c1d260b54e3447d"},"cell_type":"code","source":"# from sklearn.feature_selection import RFECV\n\n# estimator = RandomForestClassifier(random_state=12,n_estimators=15000,n_jobs=-1)\n\n# selector = RFECV(estimator,step=1,cv=5,scoring=scorer,n_jobs=-1)\n\n# selector.fit(train,train_label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c74be2848a4f36b02ecc84e72b3a6fb396bc52a"},"cell_type":"markdown","source":"# <a id='5'>Model</a>  \n\nFrom the train columns list, we drop the ID and target to form the features list."},{"metadata":{"_uuid":"7fae62e17371a3b2d1f142872541743cfa86f628"},"cell_type":"markdown","source":"## INFOMATION ABOUT PARAMS\n\n### The params what used at Gabriel's code\n\n    params = {\n        'num_leaves': 6,\n        'max_bin': 63,\n        'min_data_in_leaf': 45,\n        'learning_rate': 0.01,\n        'min_sum_hessian_in_leaf': 0.000446,\n        'bagging_fraction': 0.55, \n        'bagging_freq': 5, \n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 31452,\n        'feature_fraction_seed': 31415,\n        'feature_fraction': 0.51,\n        'bagging_seed': 31415,\n        'drop_seed': 31415,\n        'data_random_seed': 31415,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }\n\n### The params when I make lightgbm.LGBMClassifier.get_params()\n\n    params = {   \n      'boosting_type': 'gbdt', \n      'class_weight': None,\n      'colsample_bytree': 1.0,\n      'importance_type': 'split',\n      'learning_rate': 0.1,\n      'max_depth': -1,\n      'min_child_samples': 20,\n      'min_child_weight': 0.001,\n      'min_split_gain': 0.0,\n      'n_estimators': 100,\n      'n_jobs': -1,\n      'num_leaves': 31,\n      'objective': None,\n     'random_state': None,\n     'reg_alpha': 0.0,\n     'reg_lambda': 0.0,\n     'silent': True,\n     'subsample': 1.0,\n     'subsample_for_bin': 200000,\n     'subsample_freq': 0\n        }\n\n> Reference from <br />\nhttps://lightgbm.readthedocs.io/en/latest/Python-API.html <br />\nhttps://lightgbm.readthedocs.io/en/latest/Parameters.html\n\n* 'boosting_type': 'gbdt' <br />\n **alias with boosting** (Default:gbdt, options gbdt,gbrt,rf,random_forest,dart,goss)\n \n* 'class_weight': None <br />\n(default=None) – Weights associated with classes in the form {class_label: weight}. **Use this parameter only for multi-class classification task**; **for binary classification task** you may use **is_unbalance or scale_pos_weight parameters.** The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  \n \n* 'colsample_bytree': 1.0 <br />\n(Default=1.0 / constraints 0.0 < value <= 1.0) **alias: feature_fraction**[simliar with max_features of GBM]  <br />    \n lightgbm will randomly select iteration if feature_fraction smaller than 1.0.\n e.g if I set it 0.8 lightgbm will select 80% of features before training each tree.\n can be used to speed up training.\n can be used to deal with over-fitting.\n\n* 'importance_type': 'split' (default=\"split\") <br />\nHow the importance is calculated. If “split”, result contains numbers of times the feature is used in a model. If “gain”, result contains total gains of splits which use the feature. <br />\n=> sort of method how to gain feature_importance\n\n* 'learning_rate': 0.1 <br />\n (Default=1.0 / constraints learning_rate > 0.0) **alias with with shrinkage_rate,eta**\n \n* 'max_depth': -1 <br />\n  limit the max depth for tree model. This is used to deal with overfitting when data is small\n \n* 'min_child_samples': 20 <br />\n  (Default = 20 / constraints min_data_in_leaf >= 0) **alias with min_data_in_leaf,min_data-per_leaf,min_data,min_child_samples** \n\n* 'min_child_weight': 0.001 <br />\n (Default = 1e-3 // Default min_sum_hessian_in_leaf >= 0.0) <br />\n **alias with min_sum_hessian_in_leaf,min_sum_hessian_per_leaf,min_sum_hessian,min_hessian,min_child_weight **<br />\n\n* 'min_split_gain': 0.0  **only in lightgbm.LGBMClassifier()[not in lightgbm.train()]<br />\n (Default =0.0 / constraints: min_gain_to_splot >= 0.0) **alias with min_gain_to_split,min_split_gain** <br />\n the minimal gain to perform split  <br />\n \n* 'n_estimators': 100 <br />\n (Default = 100 / constraints n_estimator >= 0) **alias with num_iteration,n_iter,num_tree,num_trees,num_round,num_rounds,num_boost_round,n_estimators** <br />\n number of boosting iterations\n\n* 'n_jobs': -1 <br />\n(Default = 0) **alias with num_thread,nthread,nthreads,n_jobs**\n\n* 'num_leaves': 31 <br />\n (Default = 31 / constraints: num_leaves > 1) **aliases: num_leaf, max_leaves, max_leaf** <br />  \n max number of leaves in one tree\n    \n* 'objective': None <br />\n(Default = regression / options: regression, regression_l1, huber, fair, poisson, quantile, mape, gammma, tweedie, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank)\n **aliases: objective_type, app, application**\n    \n* 'random_state': None <br />\n(Default = None) **aliases: random_seed, random_state** <br />\nthis seed is used to generate other seeds, e.g. data_random_seed, feature_fraction_seed, etc. <br />\nby default, this seed is unused in favor of default values of other seeds <br />\nthis seed has lower priority in comparison with other seeds, which means that it will be overridden, if you set other seeds explicitly <br />\n\n* 'reg_alpha': 0.0 <br />\n(Default = 0.0 / constraints: lambda_l1 >= 0.0) **aliases: reg_alpha** <br /> \nL1 regularization <br />\n\n* 'reg_lambda': 0.0 <br />\n(Default = 0.0 /  constraints: lambda_l2 >= 0.0) **aliases: reg_lambda, lambda** <br />\nL2 regularization <br />\n\n* 'silent': True **only in lightgbm.LGBMClassifier()(not in lightgbm.train())**<br />\nsilent (bool, optional (default=False)) – Whether to print messages during construction\n\n* 'subsample': 1.0 <br />\n(Default = 1.0 / constraints: 0.0 < bagging_fraction <= 1.0 ) **aliases: sub_row, subsample, bagging** <br /> \nlike feature_fraction, but this will randomly select part of data without resampling <br /> \ncan be used to speed up training <br /> \ncan be used to deal with over-fitting <br /> \nNote: to enable bagging, bagging_freq should be set to a non zero value as well <br /> \n\n* 'subsample_for_bin': 200000 <br />\n(Default = 200000 / constraints: bin_construct_sample_cnt > 0)  **aliases: subsample_for_bin** <br /> \nnumber of data that sampled to construct histogram bins <br />\nsetting this to larger value will give better training result, but will increase data loading time <br />\nset this to larger value if data is very sparse <br />\n\n* 'subsample_freq': 0\n(Default = 0) **aliases: subsample_freq, frequency for bagging** <br />\n0 means disable bagging; k means perform bagging at every k iteration <br />\nNote: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as wel <br />l\n\n* 'reg_alpha': 0.0\n(default = 0.0) **aliases: reg_alpha**<br /> \nconstraints: lambda_l1 >= 0.0 //  L1 regularization<br />\n\n* 'reg_lambda': 0.0\n(default = 0.0) **aliases: reg_lambda, lambda** <br />\nconstraints: lambda_l2 >= 0.0 // L2 regularization"},{"metadata":{"_uuid":"d9ec2cd4233bd2c6fba88be2a5ad489ebaf95670"},"cell_type":"markdown","source":"## So We could get some results about comparing two API \"lightgbm.train()\" and \"lightgbm.LGBMClassifier\"\n\n### common params btw two APIs\n\n* boosting_type': 'gbdt' ==  'boosting_type': 'gbdt' \n* 'feature_fraction' == 'colsample_bytree'\n* 'is_unbalance': True == 'class_weight': None \n* 'learning_rate' == 'learning_rate' \n* 'max_depth' == 'max_depth'\n* 'min_data_in_leaf' == 'min_child_samples'\n* 'min_sum_hessian_in_leaf' == 'min_child_weight'    \n* num_round == 'n_estimators'\n* 'num_leaves' ==  'num_leaves'\n* 'objective' == 'objective'\n* 'seed' == 'random_state'\n* 'subsample' == 'bagging_fraction'\n* 'subsample_freq' == 'baggin_freq'\n* 'subsample_for_bin' == 'bin_construct_sample_cnt' [**Gabriel didn't tuning it**]\n\n### only in lightgbm.LGBMClassifier()\n\n* 'importance_type'\n* 'min_split_gain'\n* 'silent'\n* 'class_weight'\n* 'reg_alpha'\n* 'reg_lambda'\n**(But, I don't know when I should tune about 'reg_xx' If someone knows it plz comment at this kernel)**"},{"metadata":{"trusted":true,"_uuid":"7261d0fbe32f7c95923efca9ff9cc2415d61fb2f"},"cell_type":"code","source":"import re\nimport string\n\ndef del_punct(one_list):\n    \n    return_list = []    \n    regex = re.compile('['+re.escape(\"'\")+']')\n    \n    for element in one_list:\n        return_list.append(regex.sub(\" \",element).strip())\n    \n    return return_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0d3b3558175f34dbb69f8f99ca2bd4951243240"},"cell_type":"code","source":"def distinguish_str(value_list):\n    \n    output = []\n    \n    regex = re.compile('[0-9]')\n    \n    for i,element in enumerate(value_list):\n        if regex.search(element):\n            output.append(float(element))\n        else:\n            output.append(element)\n    \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42db5d805cff02ef07f2750de6aa113762a8e860"},"cell_type":"code","source":"def model_gbm(train,train_label,test,test_ids,nfolds=5,hyperparameters=None):\n    \n    feature_names = list(train.columns)\n    \n    valid_scores = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    \n    feature_importance_df = pd.DataFrame()\n    \n    max_iters_df = pd.DataFrame(columns=[\"folds\",\"iters\"])\n    \n    iters = []\n    folds = []\n    \n    if hyperparameters:\n        params = hyperparameters\n        \n#         If you guys get hyperparams below dataframe by hyperopt, the dictionary will be string type!! \n#         So You should change to dict following commented area. \n#         But, As I mentioned below, I'll put my hyperparams what tested at colab environment already!!\n        \n#         keys = []\n#         values = []\n        \n#         integer_elements = ['subsample_freq','max_depth','num_leaves','subsample_for_bin','min_child_samples','n_estimators']\n        \n#         for element in params[1:-1].split(\",\"):\n#             keys.append(element.split(\":\")[0])\n#             values.append(element.split(\":\")[1])\n            \n#         keys = del_punct(keys)\n#         values = distinguish_str(del_punct(values)) \n        \n#         params = dict(zip(keys,values))\n\n#         for element in integer_elements:\n#             params[element] = int(params[element])\n\n        del(params['n_estimators'])\n        \n        params['boost_from_average'] = True\n        params['seed'] = 31452\n        params['feature_fraction_seed'] = 31415\n        params['bagging_seed'] = 31415\n        params['drop_seed'] = 31415\n        params['data_random_seed'] =31415\n        params['metric'] = 'auc'\n    \n    #The hyperparams where I got from Gabriel's code\n    else:\n        params = {\n        'num_leaves': 6,\n        'max_bin': 63,\n        'min_data_in_leaf': 45,\n        'learning_rate': 0.01,\n        'min_sum_hessian_in_leaf': 0.000446,#min_child_weight\n        'bagging_fraction': 0.55, \n        'bagging_freq': 5, \n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 31452,\n        'feature_fraction_seed': 31415, \n        'feature_fraction': 0.51, #colsample_by_tree => 매 트리 생성시 가져오는 피쳐의 개수\n        'bagging_seed': 31415, #배깅을 사용한다면 쓰는 시드\n        'drop_seed': 31415,\n        'data_random_seed': 31415,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False\n    }\n    \n    strfkold = StratifiedKFold(n_splits=nfolds,shuffle=True,random_state=12)\n    \n    for i,(train_indices,valid_indices) in enumerate(strfkold.split(train.values,train_label.values)):\n        \n        print(\"{} fold processing\".format(i+1)+\"#\"*20)\n        \n        d_train = lgb.Dataset(train.values[train_indices,:],label = train_label[train_indices])\n        d_valid = lgb.Dataset(train.values[valid_indices,:],label = train_label[valid_indices])\n        \n        n_rounds = 15000\n        \n        lgb_model = lgb.train(params,d_train,num_boost_round=n_rounds,valid_sets=[d_train,d_valid],valid_names=['train','valid'],verbose_eval=1000,early_stopping_rounds=250)\n        \n        valid_scores[valid_indices] = lgb_model.predict(train.values[valid_indices,:],num_iteration=lgb_model.best_iteration)\n        \n        fold_importance_df = pd.DataFrame(columns=[\"Feature\",\"importance\",\"fold\"])\n        fold_importance_df[\"Feature\"] = feature_names\n        fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n        fold_importance_df[\"fold\"] = i + 1\n        \n        feature_importance_df = pd.concat([feature_importance_df,fold_importance_df],axis=0)\n        \n        folds.append(i+1)\n        iters.append(lgb_model.best_iteration)\n        \n        predictions += lgb_model.predict(test.values,num_iteration=lgb_model.best_iteration)/nfolds    \n        \n        display(\"valid_set score is %f and best_iteration is %d of %d fold\"%(roc_auc_score(train_label[valid_indices],valid_scores[valid_indices]),lgb_model.best_iteration,i+1))\n        \n    max_iters_df[\"folds\"] = folds\n    max_iters_df[\"iters\"] = iters\n    \n    display(\"CV score of valid_set for %d fold is %f and maximum of best_iteration is %d of %d fold\"%(nfolds,roc_auc_score(train_label,valid_scores),max_iters_df['iters'].max(),max_iters_df['iters'].idxmax()+1))\n    \n    return valid_scores,predictions,feature_importance_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1675cc80b96850fad3456e3184fe5a61b4790095"},"cell_type":"markdown","source":"## Hyperparameter Tunning using Hyperopt\n\n### The thing what I can do from below kernel is tunning parameters what we saw above through hyperopt!!\n> https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n\n#### In this phase we need to comply following 4 phases\n1. making objective function\n2. defining space for parameters\n3. choosing algorithm for hyperopt\n4. using all of them through fmin of hyperopt\n\n### I'd like to complie all of precess using hyperopt but you guys know this process is pretty time-consuming!!!\n**So I'll post my hyperparameters via this process and finally I put in the gbm_model for making predictions!!**"},{"metadata":{"trusted":true,"_uuid":"65cf772df198537d08211687465041990a5166be"},"cell_type":"code","source":"from hyperopt import hp,tpe,Trials,fmin, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"072f8b4baa1b2d12c94b43aa14118860622c5847"},"cell_type":"code","source":"import csv\nimport ast\nfrom timeit import default_timer as timer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"115c8ddd7f01e64b3ca81a3a4ce089b3593d6fd0"},"cell_type":"markdown","source":"### Making user metric for objective function"},{"metadata":{"trusted":true,"_uuid":"efe39d8ea570346feb906b763cac06cd5a695346"},"cell_type":"code","source":"def lgb_roc_auc(labels,predictions):\n#     print(predictions)\n#     predictions = predictions.reshape(len(np.unique(labels)),-1).argmax(axis=0)\n    \n    metric_value = roc_auc_score(labels,predictions)\n    \n    return 'roc_auc',metric_value,True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55f6c80079c06338fe8d017ad82c4f6dab4601e2"},"cell_type":"markdown","source":"### Objective Function\n\nP.S) I do this process **briefly**, cuz this is **time-consumming process** as I mentioned before <br />\nSo, I recommend to set like this if you do yourself in own environment <br />\n\n* n_estimators => 15000\n* early_stopping_rounds => 250\n* verbose => 1000\n"},{"metadata":{"trusted":true,"_uuid":"9393142aa9e06161909f1c3f5e95545694617db1"},"cell_type":"code","source":"def objective(hyperparameters, nfold=5):\n    \n    global ITERATION\n    ITERATION += 1\n    \n    for parameter_name in ['max_depth','num_leaves','subsample_for_bin','min_child_samples','subsample_freq']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    strkfold = StratifiedKFold(n_splits=nfold,shuffle=True)\n        \n    features = np.array(train)\n    labels = np.array(train_label).reshape((-1))\n        \n    valid_scores = []\n    best_estimators = []\n    run_times = []\n    \n    model = lgb.LGBMClassifier(**hyperparameters,n_jobs=-1,metric='None',n_estimators=1000)\n        \n    for i, (train_indices,valid_indices) in enumerate(strkfold.split(features,labels)):\n\n        print(\"#\"*20,\"%d fold of %d itertaion\"%(i+1,ITERATION))\n        \n        X_train,X_valid = features[train_indices],features[valid_indices]\n        y_train,y_valid = labels[train_indices], labels[valid_indices]\n            \n        start = timer()\n        #250 / 1000    \n        model.fit(X_train,y_train,early_stopping_rounds=50,\n                eval_metric=lgb_roc_auc,eval_set=[(X_train,y_train),(X_valid,y_valid)],\n                eval_names=['train','valid'],verbose=200)\n            \n        end = timer()\n            \n        valid_scores.append(model.best_score_['valid']['roc_auc'])\n            \n        best_estimators.append(model.best_iteration_)\n            \n        run_times.append(end-start)\n            \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1-score\n        \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n        \n    estimators = int(np.mean(best_estimators))\n    hyperparameters['n_estimators'] = estimators\n        \n    of_connection = open(OUT_FILE,'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss,hyperparameters,ITERATION,run_time,score,score_std])\n    of_connection.close()\n    \n    display(f'Iteration: {ITERATION}, Score: {round(score, 4)}.')\n    \n    if ITERATION % PROGRESS == 0:\n        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n    \n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e1ec7b97ff77e35f39136cfc5935f62e4895a33"},"cell_type":"markdown","source":"### Defining Space for Hyperparameters"},{"metadata":{"trusted":true,"_uuid":"bd3b3eff010ffe12b937ad3e441c6cf64eb79a12"},"cell_type":"code","source":"space = {\n    'boosting_type':'gbdt',\n    'objective':'binary',\n    'is_unbalance':True,\n    'subsample': hp.uniform('gbdt_subsample',0.5,1),\n    'subsample_freq':hp.quniform('gbdt_subsample_freq',1,10,1),\n    'max_depth': hp.quniform('max_depth',5,20,3),\n    'num_leaves': hp.quniform('num_leaves',20,60,10),\n    'learning_rate':hp.loguniform('learning_rate',np.log(0.025),np.log(0.25)),\n    'subsample_for_bin':hp.quniform('subsample_for_bin',2000,100000,2000),\n    'min_child_samples': hp.quniform('min_child_samples',5,80,5),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0),\n    'min_child_weight':hp.uniform('min_child_weight',0.01,0.000001)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85515c54b66f2ee1bc47ad6db5c8712d5294b30e"},"cell_type":"markdown","source":"### Make a sample via space what we defined"},{"metadata":{"trusted":true,"_uuid":"4baa6b8424d85dc9d19ac724b775ad66a846ac7a"},"cell_type":"code","source":"sample(space)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63515b2fae6e8c9a9e2feebefa6072034415cc22"},"cell_type":"markdown","source":"### Selecting algorithm\nThis algorithm is called by Tree Parzen Estimators. but I don't know how it works.. **So I'll keep trying to understanding!!! Or if you guys have a good site for TPE plz comment below!!**"},{"metadata":{"trusted":true,"_uuid":"a5f4a51284316eac6923988b2a8ed29669332280"},"cell_type":"code","source":"algo = tpe.suggest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"835aabda93fab9fc040ca7613490206e18e3cdd2"},"cell_type":"markdown","source":"### For recording our result of hyperopt"},{"metadata":{"trusted":true,"_uuid":"7a91978760a134da73289d4b503d0b85e37647c0"},"cell_type":"code","source":"# Record results\ntrials = Trials()\n\n# Create a file and open a connection\nOUT_FILE = 'optimization.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nMAX_EVALS = 10\nPROGRESS = 10\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b6f5527aa3f155a6d91c4f2d5d784c235491f0c"},"cell_type":"markdown","source":"### Final phase of hyperopt"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"308a8273b743d423b3f12e01815f62bfabd3f619"},"cell_type":"code","source":"import datetime\n\nprint(\"beginning time is {}\".format(datetime.datetime.now()))\ndisplay(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,max_evals = MAX_EVALS)\n\nprint(\"end time is {}\".format(datetime.datetime.now()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59c3adeb7bee1b9cd771edc1b917d13930efc103"},"cell_type":"code","source":"import json\n\nwith open('trials.json','w') as f:\n    f.write(json.dumps(str(trials)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad5c2fcf5688ec9b9ff5bdb9872b55b79dee13c"},"cell_type":"code","source":"results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d84570b0d8601e6ea274a22220f3a75805ee1a"},"cell_type":"markdown","source":"### making the plot using hyperopt"},{"metadata":{"trusted":true,"_uuid":"568e062eecb4c27a27ad10ac77bae592550cdefd"},"cell_type":"code","source":"plt.figure(figsize=[8,6])\nsns.regplot('iteration','score',data=results);\nplt.title('OPT Scores')\nplt.xticks(list(range(1,results.iteration.max()+1,3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"607ac2e8b2f65f89c9296d23e7c8b7bc242ccefa"},"cell_type":"code","source":"hyperparameters = results.hyperparameters[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbd05c84bfc6c5768d87021790c4c219ca10b667"},"cell_type":"markdown","source":"## I could get the params below like that by changing params few times using colab, You guys shoud do that not getting someone's params!! \nIf Kaggle's session was long, I'll use hyperparameters what I got from hyporpot above. But you know, it's not! So, I'll use my own parameters to be gotten through hyperopt in colab environment!!"},{"metadata":{"trusted":true,"_uuid":"f58836e7e755cb4b43ad58f9ee4a7a7f824c5440"},"cell_type":"code","source":"hyperparameters = {\n    'boosting_type': 'gbdt',\n    'colsample_bytree': 0.7812943473676428,\n    'is_unbalance': True,\n    'learning_rate': 0.012732207618246335,\n    'max_bin': 200,\n    'max_depth': 14,\n    'min_child_samples': 70,\n    'min_child_weight': 0.0010242091278688855,\n    'num_leaves': 10,\n    'objective': 'binary',\n    'subsample': 0.8026192939361728,\n    'subsample_for_bin': 72000,\n    'subsample_freq': 7,\n    'n_estimators': 6589}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af59d7dd90e216a0278be151f44b4502a49bdc97"},"cell_type":"markdown","source":"## make a model using our own hyperparameters!!"},{"metadata":{"trusted":true,"_uuid":"a63be1fcfaa8d37337ff92fbb3b32896a5e90076"},"cell_type":"code","source":"val_scores, predictions, gbm_fi= model_gbm(train,train_label,test,test_ids,hyperparameters=hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc05693f204eb964ce8be4c4338b0111624d4855"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}