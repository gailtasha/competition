{"cells":[{"metadata":{"_uuid":"437299c850b4d88a1982caea1259683e38cddf0a"},"cell_type":"markdown","source":"# Santander Customer Transaction Prediction"},{"metadata":{"_uuid":"af53bef62a871dab8568b841d4ab296402c6a83c"},"cell_type":"markdown","source":"In this compeition , we are asked to predict whether a customer will make transaction in future or not irrespective of the amount of money transfered . It is a binary classification task and we have been provided with anonymised dataset of numeric transactions for this.The binary column **target** is what we need to predict and a string column **ID_code** .Lets begin."},{"metadata":{"_uuid":"3bc2436dfba436a2916e467427858c9cc08c2b8a"},"cell_type":"markdown","source":"A lot of codes and ideas have been inspired from fellow kagglers - Oliver , Bojan , Will Koherson .Due credits to them."},{"metadata":{"_uuid":"1e0bd4822117b16ec0aef71898397eef0ea0b6f0"},"cell_type":"markdown","source":"### Loading the required libraries"},{"metadata":{"trusted":true,"_uuid":"2645c879e86d7d2cd1777ec618aaf739921e2e30"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3de25026484767284da3e44d90c7e04785e51592"},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"trusted":true,"_uuid":"83126d45e5a11ffde7109555dc3c83e1c0c3ca16"},"cell_type":"code","source":"Kaggle=1\nif Kaggle==0:\n    train=pd.read_csv(\"train.csv\")\n    test=pd.read_csv(\"test.csv\")\n    sample_sub=pd.read_csv(\"sample_submission.csv\")\nelse:\n    train=pd.read_csv(\"../input/train.csv\")\n    test=pd.read_csv(\"../input/test.csv\")\n    sample_sub=pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"710a3d4286e730cb8fc02dea05d2a89a2014115e"},"cell_type":"code","source":"print(f'Train has {train.shape[0]} rows and {train.shape[1]} columns' )\nprint(f'Test has {test.shape[0]} rows and {test.shape[1]} columns' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af0a9f1876c1b58af890afa9ad6045ace67e561e"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8b655dff960c7bce2e7cc0bca7dcd090b56872e"},"cell_type":"markdown","source":"We find that the data is numeric with id_code being a character column .The task is to predict the target.Lets check this column."},{"metadata":{"trusted":true,"_uuid":"679191773532a6b5b3986eab64cddfa0704e5f53"},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08613876b6c47b378e4c952a01ca9a4b9a36106"},"cell_type":"markdown","source":"We find that the target column is unbalanced with 179902 values being 0 whereas there are only 20098 rows with value 1 ."},{"metadata":{"_uuid":"fa0bda5c1039f7d5602deb8218270debd8e1e497"},"cell_type":"markdown","source":"Since we dont know the description of each of the columns , lets quickly create a random forest model and look at the feature importance .After that we can select the important features alone for modelling."},{"metadata":{"_uuid":"d1902fe18662078f9e989f0ce5731058de8edc5c"},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true,"_uuid":"9bd385e3a818b7f1c0ba6542a4e8df23fcd6f919"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,KFold, cross_val_score, GridSearchCV,StratifiedKFold,StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport feather\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd21c0848cf3097f1fbca9ac67ffa94e944d86a4"},"cell_type":"markdown","source":"Lets drop the ID Code column."},{"metadata":{"trusted":true,"_uuid":"5af1e5f2c2ba591993d12b0ca0400f835456d317"},"cell_type":"code","source":"train_model=train.drop('ID_code',axis=1)\ntest_model=test.drop('ID_code',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d68554a41009ffff5604fc4c1d4801bbb8d547f"},"cell_type":"markdown","source":"Split the data as X and Y for modelling."},{"metadata":{"trusted":true,"_uuid":"44a95ef6006e91de35f65ac88634f26e6aa466c7"},"cell_type":"code","source":"X=train_model.drop('target',axis=1)\nY=train_model['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ed1be312448c480a487756cc8c7168bc054f06c"},"cell_type":"code","source":"train_x,valid_x,train_y,valid_y=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"760e5e5fef2b1f4e0729b46c5177eddc445b29e3"},"cell_type":"code","source":"print(f'Training has {train_x.shape[0]} rows and {train_x.shape[1]} columns' )\nprint(f'Validation has {valid_x.shape[0]} rows and {valid_x.shape[1]} columns' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e2c58f352a306d9a222f7c071f635877847af79"},"cell_type":"code","source":"train_y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb8886e917b891c1e5a46cb7961f3645b3fcc1df"},"cell_type":"code","source":"valid_y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"943e52722efde5021aa068d36760ac6b1d843414"},"cell_type":"markdown","source":"Since it is an imbalanced dataset , lets try out stratified k fold cross validation and train xgboost model to find out the feature importance."},{"metadata":{"trusted":true,"_uuid":"f713330d1105f1e2acc40b949ec9611fff52be41"},"cell_type":"code","source":"# folds = StratifiedKFold(n_splits=5,shuffle=True,random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bcfca56be9002e20d607db1651322476e844734"},"cell_type":"code","source":"feature_name=[f for f in train_x.columns if f not in ['target']]\nmean_auc=0.0\nN_SPLITS=5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape,train_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_x_sample=train_x.iloc[1:1000,]\n# train_y_sample=train_y.iloc[1:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_x_sample.shape,train_y_sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def rf_model(**params):\n#     params['min_samples_leaf']=int(params['min_samples_leaf'])\n#     params['max_features']=int(params['max_features'])\n#     params['max_depth']=int(params['max_depth'])\n#     params['n_estimators']=int(params['n_estimators'])\n   \n    \n#     test_pred=np.zeros(train_x.shape[0])\n    \n#     for n_folds,(train_idx,valid_idx) in enumerate(folds.split(train_x,train_y)):\n#         x_train,x_valid=train_x.iloc[train_idx],train_x.iloc[valid_idx]\n#         y_train,y_valid=train_y.iloc[train_idx],train_y.iloc[valid_idx]\n#         clf=RandomForestClassifier(**params,random_state=100,n_jobs=-1,verbose=True)\n#         clf.fit(x_train,y_train)\n#         y_pred_proba=clf.predict_proba(x_valid)\n        \n#         test_pred[valid_idx]=clf.predict_proba(x_valid)[:,1]\n        \n#     gc.collect()\n        \n#     return roc_auc_score(y_valid,test_pred[valid_idx])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params ={'n_estimators':(100,1000),\n#           'max_depth':(10,100),\n#           'min_samples_leaf':(1,10),\n#          'max_features':(1,10)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bo = BayesianOptimization(rf_model, params)\n# bo.maximize(init_points=5, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bo.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3abe96d9d2d6d5a3d47380f6a3ad2989c3ec19d5"},"cell_type":"code","source":"rf_oof_preds=np.zeros(train_x.shape[0])\nrf_sub_preds=np.zeros(test_model.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds=StratifiedShuffleSplit(n_splits=5,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nauc_score=[]\nimportance=pd.DataFrame()\n%time\nfor n_folds,(train_idx,valid_idx) in enumerate(folds.split(train_x,train_y)):\n    x_train,x_valid=train_x.iloc[train_idx],train_x.iloc[valid_idx]\n    y_train,y_valid=train_y.iloc[train_idx],train_y.iloc[valid_idx]\n    clf=RandomForestClassifier(n_estimators=720 ,max_depth= 50,min_samples_leaf=9 ,max_features=2 ,n_jobs=-1,random_state=100,verbose=True)\n    clf.fit(x_train,y_train)\n    y_preds_proba=clf.predict_proba(x_valid)\n    rf_oof_preds[valid_idx]=y_preds_proba[:,1]\n    rf_sub_preds=clf.predict_proba(test_model[feature_name])[:,1]/folds.n_splits\n    auc_score.append(roc_auc_score(y_valid,rf_oof_preds[valid_idx]))\n    \n    print(\"\\n {} fold ROC AUC Score is : {}\".format(n_folds+1,roc_auc_score(y_valid,rf_oof_preds[valid_idx])))\n    \n    importance['feature']=feature_name\n    importance['gini']=clf.feature_importances_\n    importance['fold']=n_folds+1\n    \nprint(\"\\n Average ROC Score is {}\",np.mean(auc_score))\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### https://www.kaggle.com/gpreda/santander-eda-and-prediction\nlgb_oof_preds=np.zeros(train_x.shape[0])\nlgb_sub_preds=np.zeros(test_model.shape[0])\nauc_valid=[]\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ce365b97b82796b1e9fdb8c6d175accb162b842"},"cell_type":"code","source":"importances=pd.DataFrame()\nfor fold_idx, (train_ids, valid_ids) in enumerate(folds.split(train_x,train_y)):\n    # Split traninig data set.\n    trn_data = lgb.Dataset(train_x.iloc[train_ids],label=train_y.iloc[train_ids])\n    val_data = lgb.Dataset(train_x.iloc[valid_ids],label=train_y.iloc[valid_ids])\n    ## Building the model:\n    num_rounds=10000\n    clf = lgb.train(param, trn_data, num_rounds, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    # Train estimator.\n    \n    # Prediction and evaluation on validation data set.\n    lgb_oof_preds[valid_ids] = clf.predict(train_x.iloc[valid_ids],num_iteration=clf.best_iteration)\n    # Set feature importances.\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = feature_name\n    imp_df['gain'] = clf.feature_importance()\n    imp_df['fold'] = fold_idx + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    \n    # Prediction of testing data set.\n    lgb_sub_preds += clf.predict(test_model[feature_name],num_iteration=clf.best_iteration)/ folds.n_splits\n    \n    \n    \n    gc.collect()\nprint(\"Mean AUC: %.5f\" % (roc_auc_score(train_y,lgb_oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93695427cff5b9983712a6a7534235d75ebbf3d3"},"cell_type":"code","source":"# ### Taken from Oliver's awesome kernel - \n\n# def display_importances(feature_importance_df_):\n#     # Plot feature importances\n#     cols = feature_importance_df_[[\"feature\", \"gain\"]].groupby(\"feature\").mean().sort_values(\n#         by=\"gain\", ascending=False)[:50].index\n    \n#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n#     plt.figure(figsize=(8,10))\n#     sns.barplot(x=\"gain\", y=\"feature\", \n#                 data=best_features.sort_values(by=\"gain\", ascending=False))\n#     plt.title('LightGBM Features (avg over folds)')\n#     plt.tight_layout()\n#     #plt.savefig('lgbm_importances.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c88801d3bd61ad8306cc8aa8fd5c44c310776bc7"},"cell_type":"code","source":"#  display_importances(importances)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c109281ef6615d8a34a33a74e8fba7e8a06f4c9e"},"cell_type":"markdown","source":"Submission,"},{"metadata":{"trusted":true,"_uuid":"1d1dcea23835f87870888ab533ea2b86f57eab4c"},"cell_type":"code","source":"## Blending and submitting\n\nsample_submission = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsample_submission[\"target\"] = (0.4*rf_sub_preds)+(0.6*lgb_sub_preds)\nsample_submission.to_csv(\"blend_submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}