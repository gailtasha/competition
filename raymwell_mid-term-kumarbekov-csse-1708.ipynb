{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_32 = train.drop(['ID_code', 'target'], axis = 1).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_32.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_32 = train.drop(['ID_code'], axis = 1).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_32.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_32.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_32.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_32.shape, test_32.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_32.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_32.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x='target',data=train, palette='hls')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.iloc[:, 2:].values\ny = train.target.values\nX_test = test.iloc[:, 1:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X\ny_train = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create an instance and fit the model \nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = logmodel.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_train,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({'ID_code':test.ID_code.values})\nsub_df['target'] = predictions\nsub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes\n**"},{"metadata":{},"cell_type":"markdown","source":"When dealing with continuous variable, how do you calculate the probabilty for a given value(e.g. x=1)? The probability should be zero. So we'd better calculate the probability of a interval(e.g.  1−Δ<x<1+Δ )."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.linspace(-5, 5)\ny = norm.pdf(x)\nplt.plot(x, y)\nplt.vlines(ymin=0, ymax=0.4, x=1, colors=['red'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the interval is small enough(i.e.  Δ→0 ), the probability of a given value(e.g. x=1) can be represented by probability density(pdf) value. How can we know the probability function of a variable? The convenient way is to estimate using normal distribution. This is the Gaussian Naive Bayes."},{"metadata":{},"cell_type":"markdown","source":"Let's apply Gaussian Naive Bayes to our Santander data.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate mean/sd of train data for each each feature.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train_32.columns:\n    stats.append([\n        train_32.loc[pos_idx, col].mean(),\n        train_32.loc[pos_idx, col].std(),\n        train_32.loc[neg_idx, col].mean(),\n        train_32.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using normal distribution to estimate each feature\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# priori probability\nppos = pos_idx.sum() / len(pos_idx)\npneg = neg_idx.sum() / len(neg_idx)\n\ndef get_proba(x):\n    # we use odds P(target=1|X=x)/P(target=0|X=x)\n    return (ppos * norm.pdf(x, loc=stats_df.pos_mean, scale=stats_df.pos_sd).prod()) /\\\n           (pneg * norm.pdf(x, loc=stats_df.neg_mean, scale=stats_df.neg_sd).prod())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_pred = train_32.apply(get_proba, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(target, tr_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gaussian Naive Bayes can give us 0.876 AUC, which is quite good!\n\n"},{"metadata":{},"cell_type":"markdown","source":"Remove the Gaussian constrain"},{"metadata":{},"cell_type":"markdown","source":"Infact our data is not normal distributed, we can achive better score with Gaussian constran removed. let's take var_0 as an example."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'pos_mean'], scale=stats_df.loc[0, 'pos_sd']))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'neg_mean'], scale=stats_df.loc[0, 'neg_sd']))\nplt.title('target==0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data is very different from normal distribution, we need use more accurate probability density function to estimate, this can be done by kernel function estimation. Let's use scipy.stats.kde.gaussian_kde"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats.kde import gaussian_kde\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nkde = gaussian_kde(train.loc[pos_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nkde = gaussian_kde(train.loc[neg_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kernel funtion can fit the data better, which will give us better accuracy.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df['pos_kde'] = None\nstats_df['neg_kde'] = None\nfor i, col in enumerate(train_32.columns):\n    stats_df.loc[i, 'pos_kde'] = gaussian_kde(train_32.loc[pos_idx, col].values)\n    stats_df.loc[i, 'neg_kde'] = gaussian_kde(train_32.loc[neg_idx, col].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_col_prob(df, coli, bin_num=100):\n    bins = pd.cut(df.iloc[:, coli].values, bins=bin_num)\n    uniq = bins.unique()\n    uniq_mid = uniq.map(lambda x: (x.left + x.right) / 2)\n    dense = pd.DataFrame({\n        'pos': stats_df.loc[coli, 'pos_kde'](uniq_mid),\n        'neg': stats_df.loc[coli, 'neg_kde'](uniq_mid)\n    }, index=uniq)\n    return bins.map(dense.pos).astype(float) / bins.map(dense.neg).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_pred = ppos / pneg\nfor i in range(200):\n    tr_pred *= get_col_prob(train_32, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(target, tr_pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using more accurate kernel function, we can achieve 0.909 AUC(maybe a little overfit, since we fit train's data, but it's not too much). Let's use this model to predict the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"te_pred = ppos / pneg\nfor i in range(200):\n    te_pred *= get_col_prob(test_32, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': te_pred\n}).to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion.**"},{"metadata":{},"cell_type":"markdown","source":"\nIn this kernel we demonstrate how Naive bayes works, we build Gaussian Naive Bayes, which gives us 0.890 AUC. By remove Gaussian constrain and choosing more accurate kernel function, we can get better performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}