{"cells":[{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"## **Tuning hyperparameters for LightGBM by Bayesian Optimization**"},{"metadata":{},"cell_type":"markdown","source":"Because Santandar competition welcomes many beginners including myself, I share this kernel. As I referred to this kernel (https://www.kaggle.com/sz8416/simple-bayesian-optimization-for-lightgbm), you may get more info there."},{"metadata":{},"cell_type":"markdown","source":"If there are any problems, I appreciate it if you inform me of it. Thanks!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you split data, you can calculate faster. Accuracy would not get so worse."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain, extratrain = train_test_split(train, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember, if you choose bigger numbers for init_round, opt_round, n_estimators, you will get better results though it takes more time. You also can set other params easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('target', axis=1).drop('ID_code', axis=1)\ny = train.target\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, min_split_gain, min_child_weight, learning_rate, num_threads, min_data_in_leaf, min_sum_hessian_in_leaf):\n        # fixed parameters\n        params = {'application':'binary',\n                  'num_iterations': n_estimators,\n                  'learning_rate':learning_rate,\n                  'early_stopping_round':100,\n                  'metric':'auc',\n                  'max_depth':-1,\n                  'bagging_freq':7,\n                  'verbosity':-1}\n        # variables\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['learning_rate'] = learning_rate\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['num_threads'] = int(num_threads)\n        params['min_data_in_leaf'] = int(min_data_in_leaf)\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    # range of variables\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (22, 50),\n                                            'feature_fraction': (0.01, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 50),\n                                            'learning_rate': (0.001, 0.01),\n                                            'num_threads': (6, 10),\n                                            'min_data_in_leaf': (60, 100),\n                                            'min_sum_hessian_in_leaf': (5.0 , 15.0)},\n                                             random_state=0)\n    # optimize!\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return best parameters\n    return lgbBO.res\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=100) #15&25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basically, purple params indicate the best score (you can see target score is strengthened compared to past ones). Therefore under the given condition, purple param which have largest iter number is the best."},{"metadata":{},"cell_type":"markdown","source":"Thank you for seeing my humble kernel! Hope your success!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}