{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hereby in this Kernal I will Implement Deep Learning technique (Deep Neural Network) from scratch, the implementation is gonna start by data importing as next.\n    after importing data, i will display the shape of the training set and sample from data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport math\nimport os\n\ndata = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Divide data into Features and Labels**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.target.values\nx_data = data.drop(['target','ID_code'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Do the feature scaling**\n\n**Then Split Features and Labels into training set and testing set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(x_data)  \nx_data = scaler.transform(x_data)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reshape Features and Labels datasets to have the fixed shape and prevent (x, ) shape**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= y_train.reshape((y_train.shape[0], 1))\ny_test= y_test.reshape((y_test.shape[0] , 1))\n\nx_train=x_train.T\ny_train=y_train.T\n\nprint(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import needed Library for implementation**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initialize the Weights and Biases for all layers**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def initialize_parameters(layer_dims):\n    parameters = {}\n    L = len(layer_dims)           \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n    return parameters  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Forward propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward(A, W, b):\n    Z = np.dot(W,A)+b\n    cache = (A, W, b)\n    return Z, cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define sigmoid activation function for binary classification layer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    A = 1/(1 + np.exp(-z))\n    activation_cache = A.copy()\n    return A, activation_cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define relu activation function for Hidden layers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu(z):\n    A = z*(z > 0)\n    activation_cache = z\n    return A, activation_cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apply the activation code over the linear forward propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    cache = (linear_cache, activation_cache)\n    return A, cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Aggregate the Forward propagation all together**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagation(minibatch_X, parameters):\n    caches = []\n    A = minibatch_X\n    L = len(parameters) // 2                  \n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n    caches.append(cache)\n    return AL, caches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate the cross-entropy cost function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL, Y,parameters,lambd ):\n    m = Y.shape[1]\n    cost = (-1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))\n    cost = np.squeeze(cost)  \n    \n    L = len(parameters) // 2 \n    regularization = 0;\n    \n    for l in range(L):\n        regularization +=  np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n        \n    L2_regularization_cost = lambd / (2 * m) * regularization\n    cost = cost + L2_regularization_cost\n    return cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compute the Mini-batches by determined size**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n    np.random.seed(seed)  \n    m = X.shape[1] \n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1, m))\n    \n    num_complete_minibatches = math.floor(m / mini_batch_size) \n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, (k + 1) * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:, (k + 1) * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initialize Velocity parameters that is needed in momentum gradients updating**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_velocity(parameters):\n    L = len(parameters) // 2  \n    v = {}\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n    return v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Update parameters using momentum gradients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    L = len(parameters) // 2  \n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n    return parameters, v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initialize adam optimizer parameters that is needed in adam gradients updating**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_adam(parameters):\n    L = len(parameters) // 2 \n    v = {}\n    s = {}\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n        s[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        s[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n    return v, s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Update parameters using adam gradients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,beta1=0.9, beta2=0.999, epsilon=1e-8):\n    L = len(parameters) // 2  \n    v_corrected = {}  \n    s_corrected = {}  \n\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n        s[\"dW\" + str(l + 1)] = beta1 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta1 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (\n        v_corrected[\"dW\" + str(l + 1)] / (np.sqrt(s_corrected[\"dW\" + str(l + 1)]) + epsilon))\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (\n        v_corrected[\"db\" + str(l + 1)] / (np.sqrt(s_corrected[\"db\" + str(l + 1)]) + epsilon))\n    return parameters, v, s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Update parameters using normal BGD gradients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_gd(parameters, grads, learning_rate):\n    L = len(parameters) // 2 \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear backpropagation calculation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dZ, cache,lambd):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = (1/m)*np.dot(dZ,A_prev.T) + lambd / m * W\n    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate sigmoid activation derivative function for binary classification layer backpropagation calculations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_backward(dA, activation_cache):\n    return dA*(activation_cache*(1-activation_cache))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate relu activation derivative function for hiddin layers backpropagation calculations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu_backward(dA, activation_cache):\n    return dA*(activation_cache > 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apply the Linear activation code over the linear backward propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndef linear_activation_backward(dA, cache, activation, lambd):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache,lambd)        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache,lambd)    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Put all backppropagation together**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_propagation(minibatch_X, minibatch_Y, caches, lambd):\n    grads = {}\n    L = len(caches) \n    m = minibatch_X.shape[1]\n    minibatch_Y = minibatch_Y.reshape(1,minibatch_X.shape[1])\n    dAL = - (np.divide(minibatch_Y, minibatch_X) - np.divide(1 - minibatch_Y, 1 - minibatch_X))\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\", lambd)\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\", lambd)\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All together model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,beta1=0.9, beta2=0.999, epsilon=1e-8,lambd = 0.7,  num_epochs=10000, print_cost=True):\n    L = len(layers_dims)  \n    costs = []  \n    t = 0  \n    seed = 10  \n    parameters = initialize_parameters(layers_dims)\n\n    if optimizer == \"gd\":\n        pass  \n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n\n    for i in range(num_epochs):\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        for minibatch in minibatches:\n            (minibatch_X, minibatch_Y) = minibatch\n            AL, caches = forward_propagation(minibatch_X, parameters)\n            cost = compute_cost(AL, minibatch_Y,parameters,lambd )\n            grads = backward_propagation(AL, minibatch_Y, caches, lambd)\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1  \n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,t, learning_rate, beta1, beta2, epsilon)\n        if print_cost and i % 1 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 1 == 0:\n            costs.append(cost)\n\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predection function to use the model and check the results **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X, y, parameters): \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    AL, caches = forward_propagation(X, parameters)\n    for i in range(0, AL.shape[1]):\n        if AL[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(str(np.mean((p[0,:] == y[0,:]))))\n    return p","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Run the code and define the layers size and number**"},{"metadata":{"trusted":true},"cell_type":"code","source":"layers_dims =np.array([200,1000,1000,1000,1000,1000,1])\nparameters = model(x_train, y_train, layers_dims, optimizer=\"adam\", learning_rate=0.001, mini_batch_size=10000, beta=0.9,beta1=0.9, beta2=0.999, epsilon=1e-8,lambd = 13.24, num_epochs=100, print_cost=True)\npred_train = predict(x_train, y_train, parameters)\nprint(pred_train)\npred_test = predict(x_test.T, y_test.T, parameters)\nprint(pred_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**- Code have been inspired by Deep Learning specialaization coursera**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}