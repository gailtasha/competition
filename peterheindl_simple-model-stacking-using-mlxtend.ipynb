{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Simple model stacking using mlxtend**\n\nEnsemble learning can be highly useful to improve the accuracy of classification. The basic idea is that more than one model work together to predict some outcome. Actually, boosting as used in xgboost or lightgbm was originally proposed as an ensemble learning technique. A good introduction to ensemble learning can be found in Hastie et al. (2017), “Elements of Statistical Learning” (Chapter 16 http://web.stanford.edu/~hastie/ElemStatLearn/). Averaging and Stacking are described in Section 8.8 of the book. \n\nA nice “hands on” description of Stacking can be found at the Kaggle Blog, posted by Ben Gorman http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/. \n\nA good overview on ensemble methods is provided here: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/.\n\nWriting a stacking code on your own can be fun. However, there are many little helpers out there. I use *mlxtend* in this post: http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/.\n\nLet’s load and prepare some data first. Please note that I only use 5000 obs. here for demonstration:\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn import model_selection\nimport numpy as np\nfrom mlxtend.classifier import StackingCVClassifier\n\n# Load data for training \nmydata = pd.read_csv('../input/train.csv', sep=',')\n# Select only 5000 obs. to let the kernel run here\nmydata = mydata.head(5000)\nmydata = mydata.drop('ID_code', 1)\n\n# Load prediction data\npreddata = pd.read_csv('../input/test.csv', sep=',')\npredids = preddata[['ID_code']] \niddf = preddata[['ID_code']] \npreddata = preddata.drop('ID_code', 1)\n\n# Format train data\ny_train = mydata['target']\nx_train = mydata.drop('target', 1)\n\n# Scale data\nscaler = preprocessing.StandardScaler()\nscaled_df = scaler.fit_transform(x_train)\nx_train = pd.DataFrame(scaled_df)\nscaled_df = scaler.fit_transform(preddata)\npreddata = pd.DataFrame(scaled_df)\n\n# x,y to np (needed for scipy CV)\nx_train = x_train.values\ny_train = y_train.values","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to train and stack some models. Here I use KNN, RF, and NB. The tree models will be stacked using Logit. In the code below, the models and the stacking classifier are defined first. Then each model is trained using CV.\n\nFinally, the stacking classifier is fitted and predictions are obtained. All that we need to do now is to get the predictions into the right shape to make a submission.\n\nAs you can see, the AUC is rather low here and not at all competitive compared to the LB. However, its just a minimal example.\n\nLet me know if you have any comments: happy coding!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up models\nclf1 = KNeighborsClassifier(n_neighbors=600)\nclf2 = RandomForestClassifier(random_state=1, n_estimators=300)\nclf3 = GaussianNB()\n# Logit will be used for stacking\nlr = LogisticRegression(solver='lbfgs')\nsclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, use_probas=True, cv=3)\n\n# Do CV\nfor clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, x_train, y_train, cv=3, scoring='roc_auc')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n# Fit on train data / predict on test data\nsclf_fit = sclf.fit(x_train, y_train)\nmypreds = sclf_fit.predict_proba(preddata)\n# \"predict\" delivers classes, \"predict_proba\" delivers probabilities\n\n# Probabilities for classes (1,0)\nzeros = [i[0] for i in mypreds]\nones  = [i[1] for i in mypreds]\n\n# Get IDs and predictions\ny_id = predids.values.tolist()\npreddf = pd.DataFrame({'ID_code': y_id,'target': ones})\npreddf['ID_code'] = preddf['ID_code'].map(lambda x: str(x)[:-2])\npreddf['ID_code'] = preddf['ID_code'].map(lambda x: str(x)[2:])\n\n# Look at predictions\nprint(preddf.head())\n\n# Save DF\npreddf.to_csv('submission.csv', index=False)","execution_count":13,"outputs":[{"output_type":"stream","text":"Accuracy: 0.78 (+/- 0.01) [KNN]\nAccuracy: 0.79 (+/- 0.01) [Random Forest]\nAccuracy: 0.85 (+/- 0.01) [Naive Bayes]\nAccuracy: 0.85 (+/- 0.01) [StackingClassifier]\n  ID_code    target\n0  test_0  0.098904\n1  test_1  0.058602\n2  test_2  0.144069\n3  test_3  0.089550\n4  test_4  0.051216\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}