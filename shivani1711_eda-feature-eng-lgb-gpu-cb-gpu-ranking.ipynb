{"cells":[{"metadata":{"_uuid":"392ec03ff000d32fa1e26af2cba307f8ab250e09"},"cell_type":"markdown","source":"I have been following this competition since the beginning and it has been a huge learning experience for me as a beginner. I am documenting here all the techniques I learn via kernels and discussions in this competition.They include - \n* EXPLORATORY DATA ANALYSIS - \n>      Basic data exploration \n>      Visualizing target variable distribution\n>      Principal Component Analysis\n*  FEATURE ENGINEERING - \n>      Genetic features engineering\n>      Data Augmentation\n*  ALGORITHMS - \n>       LightGBM (with GPU accelaration)\n>       CatBoost (with GPU accelaration)\n*  HYPERPARAMETER TUNING -\n>       Bayesian Optimization\n>       RandomizedSearchCV\n*  RESULT AGGREGATION - \n>       Ranking\n>       Blending\n     "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nimport gplearn\nfrom gplearn.genetic import SymbolicTransformer, SymbolicRegressor\nfrom gplearn.functions import make_function\nfrom gplearn.fitness import make_fitness\nfrom sklearn.model_selection import StratifiedKFold,KFold\nimport random\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nfrom bayes_opt import BayesianOptimization\nimport warnings\nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostClassifier\nfrom scipy.stats import rankdata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ffbbd381d3a10ed0a77ff69ecc1e6ef012b4850"},"cell_type":"markdown","source":"Reading train and test files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ddc04bccef6c10988ff57eb8d01a2af51d7f24"},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"trusted":true,"_uuid":"e0a84df164726356e98f07b86384556937b5a6ed"},"cell_type":"code","source":"#distribution of classes in our dataset is uneven\nsns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f44b2e74934ff7b52cab2d6db7a2f6399abc6b7f"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"836225d34e55e7857b41a13fa0ff709041e0a488"},"cell_type":"markdown","source":"PRINCIPAL COMPONENT ANALYSIS"},{"metadata":{"trusted":true,"_uuid":"a61d61701cde449fe78ec43c7245286d42653189"},"cell_type":"code","source":"x = train_df.iloc[:, 2:]\ny = train_df.iloc[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe3f457ca9e1faa6ad04b840eda91dcc12721557"},"cell_type":"code","source":"#using randomized PCA and trying to map the input features into 100 components\nrpca = PCA(n_components=100, svd_solver='randomized')\nrpca.fit(x)\nplt.plot(np.cumsum(rpca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e173dc608f4d29f40d2fdb37c35822bc0bac49d"},"cell_type":"markdown","source":"Clearly, 100 features can explain only 90 percent of variance in this data.\nIt looks like this data was PCA'd already"},{"metadata":{"_uuid":"9dffe96b194a6e001f098d12f464ec8bdf30835e"},"cell_type":"markdown","source":"# FEATURE ENGINEERING"},{"metadata":{"_uuid":"8a5ec7eafcce1471daddadc6a9dad49a9d711b8f"},"cell_type":"markdown","source":"GENETIC FEATURE ENGINEERING"},{"metadata":{"trusted":true,"_uuid":"9a1bd2006e179cd6a6758bfc4c48603f907ae662"},"cell_type":"code","source":"#defining a fitness function analogous to loss function\ndef _my_fit(y, y_pred, w):\n    return mean_squared_error(y,y_pred)\nmy_fit = make_fitness(_my_fit, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52cea3dd57280ae89a3a5d3acc8fb99cb316d4cd"},"cell_type":"code","source":"# Choose the mathematical functions we will combine together\nfunction_set = ['add', 'sub']\n#We can use the below functions as well to create features\n#'div', 'log', 'sqrt', 'log', 'abs', 'neg', 'inv',  'max', 'min', 'sin', 'cos', 'tan' \n\n# Create the genetic learning regressor\n#generations is an important parameter in this function\ngp = SymbolicRegressor(function_set=function_set, metric = my_fit,\n                       verbose=1, generations = 2, \n                       random_state=0, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a53853e91bed02304830574e95e4aacfa0c7b4b"},"cell_type":"code","source":"# Using NUMPY structures, remove one feature (column of data) at a time from the training set\n# Use that removed column as the target for the algorithm\n# Use the genetically engineered formula to create the new feature\n# Do this for both the training set and the test set\n\nX1a = np.array(x)\nsam = X1a.shape[0]\ncol = X1a.shape[1]\nX2a = np.zeros((sam, col))\n\nX_test1a = np.array(test_df.drop('ID_code',axis=1))\nsam_test = X_test1a.shape[0]\ncol_test = X_test1a.shape[1]\nX_test2a = np.zeros((sam_test, col_test))\n\nfor i in range(col) :\n    X = np.delete(X1a,i,1)\n    y = X1a[:,i]\n    gp.fit(X, y) \n    X2a[:,i] = gp.predict(X)\n    X = np.delete(X_test1a, i, 1)\n    X_test2a[:,i] = gp.predict(X)\n    \nX2 = pd.DataFrame(X2a)\nX_test2 = pd.DataFrame(X_test2a) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e2e6920816e1369f5a322327e75b0a602a22bb5"},"cell_type":"code","source":"# Add the new features to the existing 200 features\nX_test1 = test_df.drop('ID_code',axis=1)\ny = train_df['target']\ntrain_df = pd.concat([x, X2], axis=1, sort=False) \ntest_df = pd.concat([X_test1, X_test2], axis=1, sort=False)  \ntrain_df = pd.concat([train_df,y],axis=1)\ntrain_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4b5a76a71bb56b3ab4c12c919796115e23273e2"},"cell_type":"markdown","source":"DATA AUGMENTATION  --> Generating more samples from existing data to enhance the training set"},{"metadata":{"trusted":true,"_uuid":"98314086caa8f67d014985a2471b9194b8447d41"},"cell_type":"code","source":"#augment train in each fold, don't touch valid and test.\n#upsample positive instances more.\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t//2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a885246ac17e172f2a097dcbd49631e5b51bf7a"},"cell_type":"markdown","source":"It prevents overfitting fake interaction appearances. Without data augmentation, it may appear that certain combinations of variables predict target=0 or target=1 but this is just overfitting train. By shuffling values, you remove the possibility of fitting fake interaction appearances in train."},{"metadata":{"_uuid":"d6ac8293ec41d61c6e5e5663cbd5af243a83801e"},"cell_type":"markdown","source":"ALGORITHMS TO USE :\n* LightGBM\n* CatBoost \n\nBefore running any of the models, we will first learn to exploit them on GPU's"},{"metadata":{"_uuid":"1bbb0d03d5bafc56d910d632420e3f88662318af"},"cell_type":"markdown","source":"In Kaggle notebook setting, set the Internet option to Internet connected, and GPU to GPU on.\nWe first remove the existing CPU-only lightGBM library and clone the latest github repo."},{"metadata":{"trusted":true,"_uuid":"c189df83a9f6a036a1c97e09adb701af46a0fe0d"},"cell_type":"code","source":"!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM\n!apt-get install -y -qq libboost-all-dev\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4bda8355ab4c5784681b1c8a7ab4a6203316ed9"},"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"707fbdf36bb5d5055d064f1bd2a20264e5145442"},"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile\n!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d884370b4ff30db51fdfda8f5ee6d643ea565e10"},"cell_type":"code","source":"#check if the GPU is blocked or not\n!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93979c7514eda2455fe5dd198c1dd2b911f8cd15"},"cell_type":"markdown","source":"In order to leverage the GPU, we need to set the following parameters:\n*  'device': 'gpu',\n*  'gpu_platform_id': 0,\n* 'gpu_device_id': 0\n\nWe are now good to go on using LightGBM on GPU"},{"metadata":{"_uuid":"798268c5890ffd53092b894af112a1984bd440f7"},"cell_type":"markdown","source":"**CATBOOST on GPU** - Add  task_type=\"GPU\" in model params"},{"metadata":{"_uuid":"2832fd3242bbd72a03478dd257a05d937db39ab4"},"cell_type":"markdown","source":"**HYPERPARAMETER TUNING**\n\nBefore running the models, we need to tune the parameters. We will be using two techniques to do so :\n* RandomizedSearchCV\n* Bayesian Optimization"},{"metadata":{"trusted":true,"_uuid":"83a7bf817a55eab6d3bb6dea3fe5627b695c967e"},"cell_type":"code","source":"#Use Kfold or StratifiedKFold to split the training set for cross validation\n#n_splits is kept around 10 in this competition to prevent overfitting and get better results\nbayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df.iloc[:,2:], train_df.target.values))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50725416aa30cc4797d200989d90b3e01e6a86bc"},"cell_type":"code","source":"#Defining the lightgbm function with nfolds \nimport lightgbm as lgb\ntrain_df.columns = [str(x) for x in train_df.columns]\ntest_df.columns = [str(x) for x in test_df.columns]\ntarget = 'target'\npredictors = train_df.columns.values.tolist()\npredictors.remove('target')\ndef LGB_bayesian(\n    num_leaves,  # int\n    min_data_in_leaf,  # int\n    learning_rate,\n    min_sum_hessian_in_leaf,    # int  \n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth):\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n\n    param = {\n        'num_leaves': num_leaves,\n        'max_bin': 63,\n        'min_data_in_leaf': min_data_in_leaf,\n        'learning_rate': learning_rate,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'min_gain_to_split': min_gain_to_split,\n        'max_depth': max_depth,\n        'save_binary': True, \n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,   \n\n    }    \n    \n    \n    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n                           label=train_df.iloc[bayesian_tr_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    \n    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n                           label=train_df.iloc[bayesian_val_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    num_round = 50\n    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=10, early_stopping_rounds = 10)\n    print('predict')\n    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n    score = roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7193c2dbd7275a331fa094ccdf394b1883d794a2"},"cell_type":"code","source":"#Defining the bounds of parameters for parameter tuning in bayesian optimization\nbounds_LGB = {\n    'num_leaves': (2, 4), \n    'min_data_in_leaf': (5, 8),  \n    'learning_rate': (0.005, 0.01),\n    'min_sum_hessian_in_leaf': (0.0001, 0.01),    \n    'feature_fraction': (0.5, 0.6),\n    'lambda_l1': (1.0, 3.0), \n    'lambda_l2': (0, 1.0), \n    'min_gain_to_split': (0, 1.0),\n    'max_depth':(2,4)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"469c566a7a2d37c4b7ea9197ce9fb1ba5db6ddff"},"cell_type":"code","source":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)\ninit_points = 8\nn_iter = 8\nprint('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a0560c0816a51868024029a8c1d96bdee44f8f2"},"cell_type":"code","source":"#Parameter grid for RandomizedSearchCV\nparam_grid = {\n    'is_unbalance': ['True'],\n    'boosting_type': ['gbdt'],\n    'max_depth' : [2,3],\n    'min_sum_hessian_in_leaf' : [0.0005,0.001],\n    'min_gain_to_split': list(range(0,5)),\n    'max_delta_step' : list(range(0,5)),\n    'lambda_l1': list(range(0,10)),\n    'lambda_l2': list(range(0,1)),\n    'learning_rate': [0.01],\n    'min_data_in_leaf': [50,100,200],\n    'max_bin': [500,750]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3ac5523df9849610b1b62940fe89b05a5304cd"},"cell_type":"code","source":"lgb = LGBMClassifier(verbose=1,metric='auc',objective= 'binary')\nclf = RandomizedSearchCV(estimator = lgb, param_distributions = param_grid, n_iter = 5,\ncv = 3, verbose=2, random_state=42, n_jobs = -1)\nclf.fit(train_df.iloc[:,2:],train_df.target)\nprint(clf.best_params_)\nprint(clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a561e0b6792659fda9821b45640bacfde46922"},"cell_type":"code","source":"#Best parameters from Bayesian Optimization\nprint(LGB_BO.max['target'])\nprint(LGB_BO.max['params'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8be8227c48ff1559623c6485f5e22a4e40fa7cf1"},"cell_type":"markdown","source":"**ALGORITHMS**\n\nLIGHTGBM with DATA AUGMENTATION"},{"metadata":{"trusted":true,"_uuid":"f56e5f628752d24308a0fccabdbf3c5a6fa38453"},"cell_type":"code","source":"#Number of iterations has been kept very large in range of 20000 and max_depth,learning rate very small to make the model learn slowly but correctly\n\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc45904bea0d54271d90d67441d53a71344f28b4"},"cell_type":"code","source":"n_split = 11 #kept around 11 for better results\nkf = KFold(n_splits=n_split, random_state=432013, shuffle=True)\ny_valid_pred = 0 * train_df.target\ny_test_pred = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90f341c057a437e708fa23f8ac9e362ab07ff227"},"cell_type":"code","source":"import lightgbm as lgb\nfor idx, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    \n    y_train, y_valid = train_df['target'].iloc[train_index], train_df['target'].iloc[valid_index]\n    X_train, X_valid = train_df[predictors].iloc[train_index,:], train_df[predictors].iloc[valid_index,:]\n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print( \"\\nFold \", idx)\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    #num_iterations are taken very large , here 50 is just for sample purposes\n    fit_model = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    pred = fit_model.predict(X_valid)\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict(test_df)\ny_test_pred /= n_split\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4c4f67c4224cdd257667d38578109be18177936"},"cell_type":"code","source":"lightgbm_results = y_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6781024417a255c1afa8e96c0abcd6b8654f65e"},"cell_type":"markdown","source":"CATBOOST"},{"metadata":{"trusted":true,"_uuid":"f260ffabfcbaf0a654792fb05924c7d94ee0ba61"},"cell_type":"code","source":"#iterations should be taken large for better results\nmodel = CatBoostClassifier(loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           task_type=\"GPU\",\n                           learning_rate=0.01,\n                           iterations=50,\n                           l2_leaf_reg=50,\n                           random_seed=432013,\n                           od_type=\"Iter\",\n                           depth=5,\n                           early_stopping_rounds=10,\n                           border_count=64\n                           #has_time= True \n                          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29fd1d637dd426bb86b1de62db0bd4d554ae4a06"},"cell_type":"code","source":"for idx, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    y_train, y_valid = train_df['target'].iloc[train_index], train_df['target'].iloc[valid_index]\n    X_train, X_valid = train_df[predictors].iloc[train_index,:], train_df[predictors].iloc[valid_index,:]\n    _train = Pool(X_train, label=y_train)\n    _valid = Pool(X_valid, label=y_valid)\n    print( \"\\nFold \", idx)\n    fit_model = model.fit(_train,\n                          eval_set=_valid,\n                          use_best_model=True,\n                          verbose=5000,\n                          plot=True\n                         )\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict_proba(test_df)[:,1]\ny_test_pred /= n_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94e052840f836997a0f4ed1a2c5796f96b14091a"},"cell_type":"code","source":"catboost_results = y_test_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f6e7317d215f3f5b757f39c08857ff09a58700b"},"cell_type":"markdown","source":"RANK AVERAGING - \n\nAssign ranks to data, dealing with ties appropriately"},{"metadata":{"trusted":true,"_uuid":"403a9547cb504698584db751270cd7d7ecb6fb7d"},"cell_type":"code","source":"predict_list = []\npredict_list.append(catboost_results)\npredict_list.append(lightgbm_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"794e89b63da1e42cd9320666de2e9aba8767f7c0"},"cell_type":"code","source":"print(\"Rank averaging on \", len(predict_list), \" outputs\")\npredictions = np.zeros_like(predict_list[0])\nfor predict in predict_list:\n    predictions = np.add(predictions, rankdata(predict)/predictions.shape[0])  \npredictions /= len(predict_list)\n\ntest_df = pd.read_csv('../input/test.csv')\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('rank_average.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d792710d3eb5761c98db036a89620da82283578"},"cell_type":"markdown","source":"BLENDING"},{"metadata":{"trusted":true,"_uuid":"f73f9da2aee8a1811adb6c5c07594d620601ec46"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\npredictions = 0.5 * (catboost_results + lightgbm_results)\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('blending.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef12c6fcfef616d25396e9009354c4cd6391c38d"},"cell_type":"code","source":"predictions = lightgbm_results\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('lgb_best_params.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}