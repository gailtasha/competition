{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport csv\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns; sns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.externals.six import StringIO  \nfrom sklearn.metrics import mean_squared_error as mse\n%matplotlib inline \nfrom skopt import BayesSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import StratifiedKFold,KFold\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom tqdm import tqdm\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in tqdm(df.columns):\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a913515d9ed155fc7dd04b1a2aca12b0fff09aa9"},"cell_type":"code","source":"train = pd.read_csv(r'../input/train.csv',low_memory=True,index_col='ID_code')  \nprint(train.head(1))\ntrain=reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb4f915cae3fef8976d493a5cdf7fa11a65d772"},"cell_type":"code","source":"def get_percentage_missing(series):\n    \"\"\" Calculates percentage of NaN values in DataFrame\n    :param series: Pandas DataFrame object\n    :return: float\n    \"\"\"\n    num = series.isnull().sum()\n    den = len(series)\n    return round(num/den, 2)\n\n# Only include columns that contain any NaN values\ndf_with_any_null_values = train[train.columns[train.isnull().any()].tolist()]\ndata =get_percentage_missing(df_with_any_null_values)\nfilter_df=data[data>0.5]\nprint(train.shape)\nremove_cols=[]\nfor n,v in filter_df.iteritems():\n    remove_cols.append(n)\nremove_cols#no column is having more than 50% missing data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b96c5c6a56d7572008e1c7970a790ba221b480f"},"cell_type":"code","source":"#Lets start by plotting a heatmap to determine if any variables are correlated\nplt.figure(figsize = (12,8))\nsns.heatmap(data=train.corr())\nplt.show()\nplt.gcf().clear()\n#no variable is correlated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a2eb32be1122df2efc8188d1e0e91b6fc59a7f7"},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = train.corr().abs()\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\nfor each in to_drop:\n    remove_cols.append(each)\nremove_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58778e968a3e88e2884530eba81a87ae47d240cb"},"cell_type":"code","source":"train.replace(np.nan,0,inplace=True)\n#train.drop('ID_code',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Seaborn visualization library\n# #Lets start by plotting a heatmap to determine if any variables are correlated\n# plt.figure(figsize = (12,8))\n# # Create the default pairplot\n# sns.pairplot(train)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2759f9b099e76fb946f88992ebeff8cb13159c88"},"cell_type":"code","source":"test = pd.read_csv(r'../input/test.csv',low_memory=True,index_col='ID_code')\ntest=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {   \n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'nthread': 6,\n        'learning_rate': 0.05,\n        'max_depth': 5,\n        'num_leaves': 40,\n        'sub_feature': 0.9,\n        'sub_row':0.9,\n        'bagging_freq': 1,\n        'lambda_l1': 0.1,\n        'lambda_l2': 0.1,\n        'random_state': 15,\n        'tree_learner':'serial',\n        'boost_from_average':'false'\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\nfeats = [f for f in train.columns if f not in ['target']]\n    \nfor n_fold, (train_idx, valid_idx) in enumerate(kf.split(train[feats], train['target'])):\n    print(n_fold)\n    trn_data = lgb.Dataset(train.iloc[train_idx][feats], label=train['target'].iloc[train_idx])    \n    val_data = lgb.Dataset(train.iloc[valid_idx][feats], label=train['target'].iloc[valid_idx])\n        \n    clf = lgb.train(param, trn_data,5120, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)   \n\n    oof_preds[valid_idx] = clf.predict(train.iloc[valid_idx][feats], num_iteration=clf.best_iteration) \n    \n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = feats\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = n_fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    \n    # we perform predictions by chunks\n    initial_idx = 0\n    chunk_size = 100000\n    current_pred = np.zeros(len(test))\n    \n    while initial_idx < test.shape[0]:\n        final_idx = min(initial_idx + chunk_size, test.shape[0])\n        idx = range(initial_idx, final_idx)\n        current_pred[idx] = clf.predict(test.iloc[idx][feats], num_iteration=clf.best_iteration)\n        initial_idx = final_idx\n        \n    sub_preds += current_pred / 5\n\nprint('Full AUC score %.6f' % roc_auc_score(train['target'], oof_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\nimport seaborn as sns\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967682c11a446989005cd3ac6fb528cfa847afab"},"cell_type":"code","source":"# x=train.loc[:,train.columns != 'target']\n# y=train.loc[:,train.columns == 'target']\n# test_data=test.copy()\n# #test.drop('ID_code',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5ccefb2bbe9c7269a3ffd10f0ad60b9e6c82026e"},"cell_type":"code","source":"# rf=RandomForestRegressor(random_state=23,verbose=0, warm_start=True,n_jobs=-1,n_estimators=100)\n# rf.fit(x, y)\n# imp=pd.DataFrame({'label':x.columns,'imp':rf.feature_importances_})\n# feature_select=imp[imp['imp']>0.01]['label']\n# feature_select","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bafd7893cd39b5404571a030be2f4ad06c10e9f"},"cell_type":"code","source":"# test=test[feature_select]\n# x=x[feature_select]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34afdd4964e1bc6036053e60823afe0959650388"},"cell_type":"markdown","source":"### light GBM"},{"metadata":{"trusted":true,"_uuid":"c54bc1b3377375f9c313cc731baad7a349329f24"},"cell_type":"code","source":"# from skopt  import BayesSearchCV\n# import lightgbm as lgb \n# from sklearn.metrics import accuracy_score \n# from sklearn.model_selection import train_test_split \n# from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10943866a8ef31207f8130bb7da444fba20374d2"},"cell_type":"code","source":"# #Now splitting our dataset into test and train \n# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c0b642a6e52df816e9c372a290a3a032ce8caa7"},"cell_type":"code","source":"# train_data=lgb.Dataset(x_train,label=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"482e62cf1c5132c928a866dabbf8ebf89b55e5cd"},"cell_type":"code","source":"# #setting parameters for lightgbm\n# param = {'num_leaves':150, 'objective':'binary:logistic','max_depth':7,'learning_rate':.05,'max_bin':200}\n# param['metric'] = ['auc', 'binary_logloss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de20b60dfadfc41be7021d012e779d72218d271f"},"cell_type":"code","source":"# #training our model using light gbm\n# num_round=50\n# lgbm=lgb.train(param,train_data,num_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c775ca946b6ee1962a8350bb71003270863a5e9f"},"cell_type":"code","source":"# ypred2=lgbm.predict(x_test)\n# ypred2\n# #converting probabilities into 0 or 1\n# for i in range(len(ypred2)):\n#     if ypred2[i]>=.3:       # setting threshold to .5\n#        ypred2[i]=1\n#     else:  \n#        ypred2[i]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b43a931c006f7f34a485c8657d4e98576270d3e9"},"cell_type":"code","source":"# #calculating accuracy\n# accuracy_lgbm = accuracy_score(ypred2,y_test)\n# accuracy_lgbm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66f0f7b44f8c79e042af84b54d96a97f6c90d020"},"cell_type":"code","source":"# #calculating roc_auc_score for light gbm. \n# auc_lgbm = roc_auc_score(y_test,ypred2)\n# auc_lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"251e9d121060629a9cd329cef752930553d8727b"},"cell_type":"code","source":"# print(test.head(5))\n# ypred_final= lgbm.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d9defc1adae426a4a86f15fc235a0f0ff9d8a25"},"cell_type":"code","source":"output_xgb=pd.DataFrame({'ID_code':test.index,'target':sub_preds})\noutput_xgb.to_csv(r'predictions.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9b5a4a4cf44e65d0395e7097d3708dbab1f42bf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}