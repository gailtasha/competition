{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# MIDTERM"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.isna().sum().sum(), test_df.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both test_df and train_df haven't got any Nan's assigned"},{"metadata":{},"cell_type":"markdown","source":"### Train set description:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test set description:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**According to two cells above, I can say that: **\n1. standard deviation of train and test columns is large\n2. values of mean, std, min, max for both data look similar\n3. also, mean values are flactuated between a large range of values for each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"target\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the values for target column variables, as we see, distinguish in amount\n\nSo we need to implement *data augmentation* on this dataset, in order to avoid imbalanced classification\n\nBy data augmentation I mean: to artificially expand the size of a training dataset, \n\nto which we referred in classes as SMOTE (or Synthetic Minority Oversampling Technique)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_df[train_df.columns[2:]].mean(), kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above you may notice a histogram\n\nI plotted distribution of all column means, so to say, in order to observe visually the dataset\n\nLooking at this plot, I can say, it's pattern is bimodal"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, we have all float/int values in column, except one column which is ID_code\n\nSo I do not have to do any datattype changes"},{"metadata":{},"cell_type":"markdown","source":"I want to evaluate correlation for each column with target values, just to see how much do I need them in the dataset\n\nSo we'll see which ones are irreplaceble (and which one are not)"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_corr = train_df.corr()['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(column_corr.sort_values().tail(11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I showed last 11 elements, because it includes target values too (of course, target correlates well with itself)\n\nAnd so, we have 10 positive parameters that correlate relatively well with target parameter "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(column_corr.sort_values().head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the parameters that correlate also well with target, but correlate negatively \n\nI mean, they are also important for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.iloc[:,2:202]\ny = train_df.iloc[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogReg"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0,)\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred = logreg.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(test_df.drop(columns = ['ID_code']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nproba = logreg.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_logreg = pd.DataFrame({ \"ID_code\": test_df[\"ID_code\"], \"target\": y_pred })\nsubmission_logreg.to_csv('submission_logreg.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, I didn't have enough strengths of will to wait until SVC fits data, so I just left it\n\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.svm import SVC, LinearSVC"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"SVC = SVC()\nLSVC = LinearSVC()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"SVC.fit(X_train,y_train)\nSVC_predict = SVC.predict(X_test)\nprint(\"SVC Accuracy :\", accuracy_score(y_test, SVC_predict))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(classification_report(y_test, LSVC_predict))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"roc_auc_score(y_test, LSVC_predict)"},{"metadata":{},"cell_type":"markdown","source":"## NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(X_train, y_train)\npredicted= model.predict(X_test)\nprint(\"NBGaussian Accuracy :\", accuracy_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = model.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DT and RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nTree = tree.DecisionTreeClassifier()\nTree = Tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted= Tree.predict(X_test)\nprint(\"Decision Tree Accuracy :\", accuracy_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = Tree.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nForest = RandomForestClassifier(n_estimators = 100)\nForest = Forest.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted= Forest.predict(X_test)\nprint(\"Random Forest Accuracy :\", accuracy_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = Forest.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nXGB_model = xgb.XGBClassifier()\n\nXGB_model = XGB_model.fit(X_train, y_train)\n\npredicted= XGB_model.predict(X_test)\n\nprint(\"XGBoost Accuracy :\", accuracy_score(y_test, predicted))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = XGB_model.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To sum up, we saw, that the model's auc_roc_score of each model showed not well performance and also I know the reason why.\n\nI did not implement SMOTE on training set, that's why the predicted list perceived incorrect outputs.\n\nAs I mentioned above, I didn't have enough time to wait for SMOTE class cell to run \n\n*(because it takes forever for kaggle notebook even to open )*\n\n\nSo if I would apply training set on SMOTE model, I believe the results would be much greater. But it was not the only reason.\n\nThe second point to mention, a lot of parameters of model do affect the prediction. \n\nAnd as you have noticed, I didn't set any parameter, because (of course slow processing of the notebook) I wanted to see the pure model behavior."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}