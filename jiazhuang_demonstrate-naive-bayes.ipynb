{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"In the kernel [Modified Naive Bayes scores 0.899 LB - Santander](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899), [Chris Deotte](https://www.kaggle.com/cdeotte) has demonstrated that Naive Bayes can be a simple but powful method when there is little or no interaction between the features. I think it's a good time to study Naive Bayes Method.\n\nIn this kernel, I'll try to introduce Naive Bayes method step by step."},{"metadata":{"_uuid":"8dd02f4ef085cc538d6f7902e723416f203c41f7"},"cell_type":"markdown","source":"### Bayes' theorem\n\nBayes' theorem can be deduced by conditional probability:\n\n$$\nP(A|B) = \\frac {P(A \\bigcap B)}{P(B)}\n$$\n\n$$\nP(B|A) = \\frac {P(A \\bigcap B)}{P(A)}\n$$\n\n$$\n\\Rightarrow P(A|B) = \\frac {P(B|A) \\cdot P(A)} {P(B)}\n$$"},{"metadata":{"_uuid":"cb6047c03d0cd76265dd429a2eb0f4b14fe48c25"},"cell_type":"markdown","source":"Suppose we have features $X \\in R^n$, target $y \\in \\{+1, -1\\}$, for a given $x_0$ our goal is to predict $y=+1$ or $-1$.\n\nwe can achive this by calculate\n\n$$P(y=+1 | X=x_0)$$ \nand\n$$P(y=-1 |  X=x_0)$$\n\nthen choose the one with bigger probability."},{"metadata":{"_uuid":"f7aa4621dcbecb0a4a9d2a9b6f44f22f456c6e62"},"cell_type":"markdown","source":"How can we calculate $P(y=+1|X=x_0)$ ? We can use Bayes's theorem:\n$$\nP(y=+1|X=x_0) = \\frac {P(X=x_0 | y=+1) \\cdot P(y=+1)} {P(X=x_0)}\n$$\n\nBecause $P(X=X_0)$ is the probability(or frequency) of a sample in test set, it's the same for every sample, so we can simply write the formular as:\n\n$$\nP(y=+1|X=x_0) = P(X=x_0 | y=+1) \\cdot P(y=+1)\n$$\n\n$P(y=+1/-1)$ is called priori probability and $P(X=x_0 | y=+1)$ is called conditional probability. Training process is to estimate this two kind of probability.\n"},{"metadata":{"_uuid":"7a8c6415151fdbb788771642cc049e3a9d074d1c"},"cell_type":"markdown","source":"### Naive Bayes\n\nIf we have many features, then:\n$$\nP(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)}, X^{(2)}=x_0^{(2)}, ..., X^{(n)}=x_0^{(n)} | y=+1)\n$$\n\nTo simplify this problem, we assume the features as independent(this is the Naive means):\n$$\nP(X=x_0 | y=+1) = P(X^{(1)}=x_0^{(1)} | y=+1) \\cdot P(X^{(2)}=x_0^{(2)} | y=+1) \\cdot, ..., \\cdot P(X^{(n)}=x_0^{(n)} | y=+1)\n$$"},{"metadata":{"_uuid":"60a5f702cf07cf03871936a11daf0366828cd1cb"},"cell_type":"markdown","source":"### Discrete variable\n\nWe will use a sample example to demostrate the training and predicting process of Naive Bayes for discrete variable."},{"metadata":{"trusted":true,"_uuid":"d12ac1e1526c8e8b8d32dd4178c10ff50f103bd3"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f59496f16109edcb0c69b4d61502dbf9b80886ae"},"cell_type":"code","source":"data = pd.DataFrame({\n    'X1': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n    'X2': ['S', 'M', 'M', 'S', 'S', 'S', 'M', 'M', 'L', 'L', 'L', 'M', 'M', 'L', 'L'],\n    'y': [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f67918d6c8a5d3f3ac24933ed34644d5d204340d"},"cell_type":"markdown","source":"Try to use data to training a Naive Bayes classifier, and classify the sample $X_0=(2, S)$."},{"metadata":{"_uuid":"d84f4465df830c381758a21e5c1edb8e27f6218f"},"cell_type":"markdown","source":"Calculate priori probability $P(y=1)$ and $P(y=-1)$"},{"metadata":{"trusted":true,"_uuid":"dce19737d5299b48f6977e266c0eec1a09b61129"},"cell_type":"code","source":"data.y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0baa0a3a03df98fdfe0345ad800223b9e2eb2300"},"cell_type":"markdown","source":"$$\nP(y=1) = \\frac {9}{15}, P(y=-1) = \\frac {6}{15}\n$$"},{"metadata":{"trusted":true,"_uuid":"8a44bb4e1224f0f1d8089c850cedb644c193945d"},"cell_type":"markdown","source":"Calculate conditional probability"},{"metadata":{"trusted":true,"_uuid":"8799fed8490a6d5514bd464802a069e3789be5ff"},"cell_type":"code","source":"data[data.y == 1].X1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56fbd021bd3a835cf5d2ede882bb62c55ffaa955"},"cell_type":"markdown","source":"$$\nP(X1=1 | y=1) = \\frac{2}{9}, P(X1=2 | y=1) = \\frac{3}{9}, P(X1=3 | y=1) = \\frac{4}{9}\n$$"},{"metadata":{"trusted":true,"_uuid":"da2f09bd98bb720a0ff306b301e772b3ef81d0d8"},"cell_type":"code","source":"data[data.y == 1].X2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b59a122f1f20db3906dbf56a8ce8c254ca3981b"},"cell_type":"markdown","source":"$$\nP(X2=S | y=1) = \\frac{1}{9}, P(X2=M | y=1) = \\frac{4}{9}, P(X2=L | y=1) = \\frac{4}{9}\n$$"},{"metadata":{"trusted":true,"_uuid":"cf1a82644aeb6515d8d87043a6474971cacf2d54"},"cell_type":"code","source":"data[data.y == -1].X1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4b6684b87140fd312f0ac0310c51d291a3f68f5"},"cell_type":"markdown","source":"$$\nP(X1=1 | y=-1) = \\frac{3}{6}, P(X1=2 | y=-1) = \\frac{2}{6}, P(X1=3 | y=-1) = \\frac{1}{6}\n$$"},{"metadata":{"trusted":true,"_uuid":"ff74f30812cc9b6b4488eb85a3a99a3bd4dac288"},"cell_type":"code","source":"data[data.y == -1].X2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6565267f51090f5326a2dddeac23590677170d2"},"cell_type":"markdown","source":"$$\nP(X2=S | y=1) = \\frac{3}{6}, P(X2=M | y=1) = \\frac{2}{6}, P(X2=L | y=1) = \\frac{1}{6}\n$$"},{"metadata":{"_uuid":"0045a961f67e07746a57ba06aeb911cf1c45157c"},"cell_type":"markdown","source":"For the given sample $X_0=(2, S)$:\n$$\nP(y=1 | X=X_0) = P(y=1) \\cdot P(X1=2|y=1) \\cdot P(X2=S|y=1) = \\frac{9}{15} \\cdot \\frac{3}{9} \\cdot \\frac{1}{9} = \\frac{1}{45}\n$$\n\n$$\nP(y=-1 | X=X_0) = P(y=-1) \\cdot P(X1=2|y=-1) \\cdot P(X2=S|y=-1) = \\frac{6}{15} \\cdot \\frac{2}{6} \\cdot \\frac{3}{6} = \\frac{1}{15}\n$$\n\nBecause $P(y=-1|X=X_0) > P(y=1|X=X_0)$, so $y=-1$\n\nThis is Naive Bayes for discrete variable, pretty simple."},{"metadata":{"_uuid":"40bb736793ccb87bee056714dc5b3c394014bb3c"},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"_uuid":"4faaf865d5290a830130d94ee16f99983c26d7c5"},"cell_type":"markdown","source":"When dealing with continuous variable, how do you calculate the probabilty for a given value(e.g. x=1)? The probability should be zero. So we'd better calculate the probability of a interval(e.g. $1-\\Delta < x < 1+\\Delta$)."},{"metadata":{"trusted":true,"_uuid":"7431c9ee3b1f0a4d6816dbf6d73b403059b3b4f6"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bdf97aaaa5939291e29aa745b5613daf09f2252"},"cell_type":"code","source":"x = np.linspace(-5, 5)\ny = norm.pdf(x)\nplt.plot(x, y)\nplt.vlines(ymin=0, ymax=0.4, x=1, colors=['red'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6d9f46ce7d61cc507406be54dec034bfc314e43"},"cell_type":"markdown","source":"If the interval is small enough(i.e. $\\Delta \\rightarrow 0$), the probability of a given value(e.g. x=1) can be represented by probability density(pdf) value. How can we know the probability function of a variable? The convenient way is to estimate using normal distribution. This is the **Gaussian Naive Bayes**.  "},{"metadata":{"_uuid":"86564032a9961cff3367891ed8b1dc70ab6452d1"},"cell_type":"markdown","source":"Let's apply Gaussian Naive Bayes to our Santander data."},{"metadata":{"trusted":true,"_uuid":"25adffaf80990374e869449413b7d59b45fd3d8f"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col=0)\ntest = pd.read_csv('../input/test.csv', index_col=0)\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2115ed9b52e92d4af1396b2c3e8cedfc5d69cf37"},"cell_type":"markdown","source":"Calculate mean/sd of train data for each each feature."},{"metadata":{"trusted":true,"_uuid":"3ae5a1248e82ac902ac8c59cb145e56a4bfef54b"},"cell_type":"code","source":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train.columns:\n    stats.append([\n        train.loc[pos_idx, col].mean(),\n        train.loc[pos_idx, col].std(),\n        train.loc[neg_idx, col].mean(),\n        train.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29a30364ac6bfe7eefd61e1e05d23910fa31307e"},"cell_type":"markdown","source":"Using normal distribution to estimate each feature"},{"metadata":{"trusted":true,"_uuid":"4c0040e4e54714d3da1245cca1c8fa43ddc536c5"},"cell_type":"code","source":"# priori probability\nppos = pos_idx.sum() / len(pos_idx)\npneg = neg_idx.sum() / len(neg_idx)\n\ndef get_proba(x):\n    # we use odds P(target=1|X=x)/P(target=0|X=x)\n    return (ppos * norm.pdf(x, loc=stats_df.pos_mean, scale=stats_df.pos_sd).prod()) /\\\n           (pneg * norm.pdf(x, loc=stats_df.neg_mean, scale=stats_df.neg_sd).prod())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd5b26fc2f9c4d2485c7f72f137a6740a0837a32"},"cell_type":"code","source":"tr_pred = train.apply(get_proba, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b4bc3faf6651f90e0549394b48dfe016cff3ba"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(target, tr_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9e94efb07cf3fe476b66be5a1b9b65d77991387"},"cell_type":"markdown","source":"Gaussian Naive Bayes can give us 0.890 AUC, which is quite good!"},{"metadata":{"_uuid":"898de2098843fbd1f137376a84262fe631ac680e"},"cell_type":"markdown","source":"### Remove the Gaussian constrain"},{"metadata":{"_uuid":"f5903c8ed4770da420d29cd9977e54f38c13a5ec"},"cell_type":"markdown","source":"Infact our data is not normal distributed, we can achive better score with Gaussian constran removed. let's take `var_0` as an example."},{"metadata":{"trusted":true,"_uuid":"7639b24ba0f023ab7c8d476e781776928419ae10"},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'pos_mean'], scale=stats_df.loc[0, 'pos_sd']))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nplt.plot(np.linspace(0, 20), norm.pdf(np.linspace(0, 20), loc=stats_df.loc[0, 'neg_mean'], scale=stats_df.loc[0, 'neg_sd']))\nplt.title('target==0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c7b5c4b5820fd551424133c4a64635580bebc59"},"cell_type":"markdown","source":"We can see that the data is very different from normal distribution, we need use more accurate probability density function to estimate, this can be done by kernel function estimation. Let's use `scipy.stats.kde.gaussian_kde` "},{"metadata":{"trusted":true,"_uuid":"ef9e32ac4b93bdabb8fe9d4481a52757c0e0c634"},"cell_type":"code","source":"from scipy.stats.kde import gaussian_kde","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"744a742714dc6c5a9faafacc96954cf60a65b21d"},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'])\nkde = gaussian_kde(train.loc[pos_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==1')\nplt.subplot(1, 2, 2)\nsns.distplot(train.loc[neg_idx, 'var_0'])\nkde = gaussian_kde(train.loc[neg_idx, 'var_0'].values)\nplt.plot(np.linspace(0, 20), kde(np.linspace(0, 20)))\nplt.title('target==0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69a2f5ac344be7874fb3fc131a8bd08efbbbd3c"},"cell_type":"markdown","source":"Kernel funtion can fit the data better, which will give us better accuracy."},{"metadata":{"trusted":true,"_uuid":"a63b47abfa1e92085c7e5db0f2ec7d61fea0416b"},"cell_type":"code","source":"stats_df['pos_kde'] = None\nstats_df['neg_kde'] = None\nfor i, col in enumerate(train.columns):\n    stats_df.loc[i, 'pos_kde'] = gaussian_kde(train.loc[pos_idx, col].values)\n    stats_df.loc[i, 'neg_kde'] = gaussian_kde(train.loc[neg_idx, col].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b688630f4d490d2aab9be0267c4d6a89e67dfcd7"},"cell_type":"code","source":"def get_proba2(x):\n    proba = ppos / pneg\n    for i in range(200):\n        proba *= stats_df.loc[i, 'pos_kde'](x[i]) / stats_df.loc[i, 'neg_kde'](x[i])\n    return proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"350c9306e74cf582db0716cbce863b78cd624f14"},"cell_type":"code","source":"%%time\nget_proba2(train.iloc[0].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7db055dd6a6d7b4531aac66b72ae3d6e86a42639"},"cell_type":"markdown","source":"It's too slow, we can speed up by binize the variable values."},{"metadata":{"trusted":true,"_uuid":"dd6d8d0ef6344a9cbebd4610862e067b1629155a"},"cell_type":"code","source":"def get_col_prob(df, coli, bin_num=100):\n    bins = pd.cut(df.iloc[:, coli].values, bins=bin_num)\n    uniq = bins.unique()\n    uniq_mid = uniq.map(lambda x: (x.left + x.right) / 2)\n    dense = pd.DataFrame({\n        'pos': stats_df.loc[coli, 'pos_kde'](uniq_mid),\n        'neg': stats_df.loc[coli, 'neg_kde'](uniq_mid)\n    }, index=uniq)\n    return bins.map(dense.pos).astype(float) / bins.map(dense.neg).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ff0e27c68e0e389375fac11fa0f29d18c9abddf"},"cell_type":"code","source":"tr_pred = ppos / pneg\nfor i in range(200):\n    tr_pred *= get_col_prob(train, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"381767d0c050542559bee1ae01e2555dee39d368"},"cell_type":"code","source":"roc_auc_score(target, tr_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e85bdce173255d29741530c47f2aa2e19eba2723"},"cell_type":"markdown","source":"Using more accurate kernel function, we can achieve 0.909 AUC(maybe a little overfit, since we fit train's data, but it's not too much). Let's use this model to predict the test data."},{"metadata":{"trusted":true,"_uuid":"85a9d1669f3f338e9cd7eb883a268b5155087117"},"cell_type":"code","source":"te_pred = ppos / pneg\nfor i in range(200):\n    te_pred *= get_col_prob(test, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8338918dda5d5d3c46e992e021264691095715"},"cell_type":"code","source":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': te_pred\n}).to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"443a45769dd551c10a2d9d8b18aa1cd02b1c8fb3"},"cell_type":"markdown","source":"### Conclusion\n\nIn this kernel we demonstrate how Naive bayes works, we build Gaussian Naive Bayes, which gives us 0.890 AUC. By remove Gaussian constrain and choosing more accurate kernel function, we can get better performance.\n\nHolp this can help, thanks!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}