{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nsns.set_style('dark')\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data\nWe set a few flags for whether we are running locally or on Kaggle, or whether we want the reduced or full dataset. For the reduced dataset, the test set consists of samples from the full train set. For use during model evaluation, this has been augmented with reverse-engineered targets. This is simply to allow us to evaluate the AUC score on this set as well - it is never used for training."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"IS_LOCAL = False  # Sets whether we are running locally or on kaggle\nUSE_REDUCED = False  # Sets whether we should use the smaller dataset\ndata_index = 2*int(IS_LOCAL) + int(USE_REDUCED)\ntrain_path = ('../input/santander-customer-transaction-prediction/train.csv',\n             '../input/santandersmall/train_small.csv',\n             'train.csv',\n             'train_small.csv')[data_index]\ntest_path = ('../input/santander-customer-transaction-prediction/test.csv',\n             '../input/santandersmall/test_small_with_targets.csv',\n             'test.csv',\n             'test_small.csv')[data_index]\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in train_df.columns if col not in ['target', 'ID_code']]\nif not 'target' in test_df:\n    test_df['target'] = -1\n\nall_df = pd.concat([train_df, test_df], sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Removing fake test samples\nIt was discovered during the competition that some of the test samples were synthetic. Moreover, it was stated that not all of the data in the test set was used for evaluation, and the synthetic data corresponds to this. In order to achieve the best scores possible, the fake samples should be removed before calculating features such as counts. We calculate the indices of the fake rows below using the method provided by the Kaggle user YaG320. When using the reduced dataset, we have no synthetic samples. This is handled correctly by the algorithm below."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_count = np.zeros((test_df.shape[0], len(features)))\n\nfor f, feature in tqdm(enumerate(features), total=len(features)):\n    _, i, c = np.unique(test_df[feature], return_counts=True, return_index=True)\n    unique_count[i[c == 1], f] += 1\n\nreal_sample_indices = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_sample_indices = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\nprint('Real:', len(real_sample_indices))\nprint('Synthetic:', len(synthetic_sample_indices))\n\ndel unique_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate counts\nCounts of the values of the different features is often a powerful feature in itself. We calculate these below."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_real_df = pd.concat([train_df, test_df.iloc[real_sample_indices, :]], sort=False)\n\nfor feature in tqdm(features):\n    real_series = all_real_df[feature]\n    \n    # We only use the real samples to produce the count\n    counts = real_series.groupby(real_series).count()\n    \n    full_series = all_df[feature]\n    all_df[f'{feature}_count'] = full_series.map(counts)\n\ndel all_real_df\ndel real_series\ndel full_series","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization\nNormalizing features to have zero mean, unit variance can often speed up training."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in tqdm(features):\n    all_df[feature] = StandardScaler().fit_transform(all_df[feature].values.reshape(-1, 1))\n    all_df[f'{feature}_count'] = MinMaxScaler().fit_transform(all_df[f'{feature}_count'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in range(len(features)):\n    features.append(f'{features[f]}_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting datasets back up\nWe are now done with our feature engineering, so we can split the data back into train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = all_df.iloc[:train_df.shape[0], :]\ntest_df = all_df.iloc[train_df.shape[0]:, :]\n\ndel all_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"We use a neural network model to make predictions. 10-fold stratified cross validation is used, and we rely on early stopping, setting a large default number of epochs of 100. Binary cross-entropy is used for the loss function, which is a good fit for a binary classification task such as this one. The output of the network is a single output from a sigmoid function, which can be interpreted as a probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"N_SPLITS = 5\nBATCH_SIZE = 256\nEPOCHS = 100\nEARLY_STOPPING_PATIENCE = 15\n\nOPTIMIZER = tf.keras.optimizers.Nadam()\nLOSS='binary_crossentropy'\nMETRICS=[tf.keras.metrics.AUC()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cnn_model_1():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n        tf.keras.layers.Dense(64, activation='relu'),\n        #tf.keras.layers.Conv1D(64, 1, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.Dense(128, activation='relu'),\n        #tf.keras.layers.Conv1D(128, 1, activation='relu'),\n        #tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        #tf.keras.layers.Conv1D(256, 1, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.Dense(512, activation='relu'),\n        #tf.keras.layers.Conv1D(512, 1, activation='relu'),\n        #tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1024, activation='relu'),\n        #tf.keras.layers.Conv1D(1024, 1, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.Dense(2048, activation='relu'),\n        #tf.keras.layers.Conv1D(2048, 1, activation='relu'),\n        #tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    \n    return model\ndef get_cnn_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n        tf.keras.layers.Conv1D(32, 1, activation='elu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Conv1D(64, 1, activation='elu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        #tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005)),\n        #tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        #tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    \n    return model\n\ndef get_nn_model_2():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nmodels = []\nhistories = []\n\nfor fold_num, (train_index, val_index) in tqdm(enumerate(kfold.split(train_df[features].values, train_df['target'].values)), total=N_SPLITS):\n    print(f'Fold {fold_num+1}/{N_SPLITS}:')\n    \n    X_train = train_df.loc[train_index, features].values\n    y_train = train_df.loc[train_index, 'target'].values.reshape(-1, 1)\n    X_val = train_df.loc[val_index, features].values\n    y_val = train_df.loc[val_index, 'target'].values.reshape(-1, 1)\n    \n    model = get_cnn_model_1()\n    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n    \n    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n    \n    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping_callback])\n    histories.append(history)\n    \n    val_preds = model.predict(X_val)\n    val_auc = roc_auc_score(y_val, val_preds)\n    print(f'Fold validation AUC: {val_auc}')\n    print()\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5 * (len(histories) // 5 + 1)))\nfor i, h in enumerate(histories):\n    plt.subplot(len(histories) // 5 + 1, 5, i+1)\n    plt.plot(h.history['loss'], label='Train loss')\n    plt.plot(h.history['val_loss'], label='Val loss')\n    plt.plot(h.history['auc'], label='Train AUC')\n    plt.plot(h.history['val_auc'], label='Val AUC')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create predictions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = np.zeros(train_df.shape)\ntest_preds = np.zeros(test_df.shape)\n\nfor model in models:\n    pred_train = model.predict(train_df[features].values)\n    pred_test = model.predict(test_df[features].values)\n    \n    train_preds += pred_train\n    test_preds += pred_test\n\ntrain_preds /= len(models)\ntest_preds /= len(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = train_preds[:, 0]\ntest_preds = test_preds[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_auc = roc_auc_score(train_df['target'], train_preds)\nprint(f'Train AUC: {train_auc}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('test_small_with_targets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if test_df['target'][0] != -1:\n    test_auc = roc_auc_score(test_df['target'], test_preds)\n    print(f'Test AUC: {test_auc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'ID_code': test_df['ID_code'], 'target': test_preds})\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}