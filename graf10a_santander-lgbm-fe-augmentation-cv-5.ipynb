{"cells":[{"metadata":{},"cell_type":"markdown","source":"Loading libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom scipy.stats import rankdata\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To reduce memory usage let's make a dictionary holding the data types:"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['var_' + str(i) for i in range(200)]\ncol_types = [np.float32 for i in range(200)]\n\ntypes = dict(zip(col_names, col_types))\n\ntypes['target'] = np.uint8\ntypes['ID_code'] = str","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_train='../input/santander-customer-transaction-prediction/train.csv'\npath_test='../input/santander-customer-transaction-prediction/test.csv'\n\ntrain = pd.read_csv(path_train, dtype=types)\ntest = pd.read_csv(path_test, dtype=types)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSeparating the 'target' and 'ID_code' columns from the numeric features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.pop('target')\ntrain_ids = train.pop('ID_code')\ntest_ids = test.pop('ID_code')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Memory usage for the train: {0:.2f}MB\".\n      format(train.memory_usage().sum() / (1024**2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Memory usage the test: {0:.2f}MB\".\n      format(test.memory_usage().sum() / (1024**2)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To add the count features, we will need to remove the fake data from the test set. To acomplish that, we will be using the indecies for the fake, private, and public LB data from the [famous kernel](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) by [YaG320](https://www.kaggle.com/yag320). "},{"metadata":{"trusted":true},"cell_type":"code","source":"path_load_fakes = '../input/list-of-fake-samples-and-public-private-lb-split/'\n\n#Synthetic data indecies:\npath_synth = path_load_fakes + 'synthetic_samples_indexes.npy'\nsynth_idx = np.load(path_synth)\n\n#Public LB data indecies:\npath_public_LB = path_load_fakes + 'public_LB.npy'\npublic_idx = np.array(list(np.load(path_public_LB).tolist()))\n\n#Private LB data indecies:\npath_private_LB = path_load_fakes + 'private_LB.npy'\nprivate_idx = np.array(list(np.load(path_private_LB).tolist()))\n\n#Indecies for public and private data combined:\ntrue_idx=np.append(private_idx, public_idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separating the fake data."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_synth = test.iloc[synth_idx.tolist(), :]\ntest_true = test.iloc[true_idx.tolist(), :]\n\nprint(\"\\nThe shape of the synthetic test data:\")\nprint(test_synth.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nThe shape of the true test data:\")\nprint(test_true.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_train = len(train)\nlen_test_true = len(test_true)\nlen_test_synth = len(test_synth)\n\nmerged = pd.concat([train, test_true])\n\nprint(\"\\nThe shape of the merged data:\")\nprint(merged.shape)\n\ndel test, train, test_true\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to add the count features."},{"metadata":{"trusted":true},"cell_type":"code","source":"original_features=merged.columns\n\nfor col in original_features:\n    \n    unq_val, inv, cnts = np.unique(merged[col].values, \n                                   return_inverse=True, \n                                   return_counts=True)\n    \n    merged[col+'_counts']=cnts[inv]\n    \n    #Populate the count column of the sythetic test with zeroes.\n    test_synth[col+'_counts']=0\n    \ntrain = merged.iloc[:len_train, :]\n    \ntest_true=merged.iloc[len_train:, :]\n\ntest = pd.concat([test_true, test_synth]).sort_index()\n\ndel merged, test_true, test_synth\n\ngc.collect()\n\nprint(\"\\nThe shape of the processed train data:\")\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nThe shape of the processed test data:\")\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a function for doing augmentation (it is a modified version of the augmentation function from [another famous kernel](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment) by [Jiwei Liu](https://www.kaggle.com/jiweiliu))."},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment(x,y,t=2):\n        \n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        \n        x1_var=x1[:, 0:200]\n        x1_counts=x1[:, 200:]\n        \n        del x1\n        \n        ids = np.arange(x1_var.shape[0])\n        for c in range(x1_var.shape[1]):\n            np.random.shuffle(ids)\n            x1_var[:,c] = x1_var[ids][:,c]\n            x1_counts[:,c] = x1_counts[ids][:,c]\n            x1=np.column_stack((x1_var, x1_counts))\n        xs.append(x1)\n\n    for i in range(t//2):\n        mask = y==0\n        x1 = x[mask].copy()\n        \n        x1_var=x1[:, 0:200]\n        x1_counts=x1[:, 200:]\n        \n        del x1\n        \n        ids = np.arange(x1_var.shape[0])\n        for c in range(x1_var.shape[1]):\n            np.random.shuffle(ids)\n            x1_var[:,c] = x1_var[ids][:,c]\n            x1_counts[:,c] = x1_counts[ids][:,c]\n            x1=np.column_stack((x1_var, x1_counts))\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making folds and setting up LightGBM parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ntest_preds = np.zeros((len(test), 1))\noof_preds = np.zeros((len(train), 1))\n\nroc_cv =[]\n\nparams = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_bin\": 255, \n    \"max_depth\" : -1,\n    \"num_leaves\" : 3,\n    \"learning_rate\" : 0.1,\n    \"min_data_in_leaf\": 150,\n    \"min_sum_hessian_in_leaf\": 25,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"lambda_l1\" : 1, \n    \"verbosity\" : 1,\n    \"min_gain_to_split\": 0.3,\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training a LightGBM model."},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_, (trn_, val_) in enumerate(folds.split(y, y), 1):\n    \n    print(\"\\nStarting fold {}\".format(fold_))\n    \n    trn_x, trn_y = train.iloc[trn_, :], y.iloc[trn_].values.ravel()\n    val_x, val_y = train.iloc[val_, :], y.iloc[val_].values.ravel()    \n            \n    features=trn_x.columns\n        \n    trn_aug_x, trn_aug_y = augment(trn_x.values, trn_y, 9)\n    trn_aug_x = pd.DataFrame(trn_aug_x)\n    trn_aug_x.columns = features\n    \n    #Compute the row sums of the counts (may give us a slight boost):\n    count_feats=['var_'+str(i)+'_counts' for i in range(200)]\n    trn_aug_x['tot']=trn_aug_x[count_feats].sum(axis=1)\n    val_x['tot']=val_x[count_feats].sum(axis=1)\n    test['tot']=test[count_feats].sum(axis=1)\n        \n    print(\"The shapes of trn_x and trn_y after augmentation are {} and {}, respectively\".\\\n          format(trn_x.shape, trn_y.shape))\n        \n    print(\"The shapes of trn_aug_x and trn_aug_y after augmentation are {} and {}, respectively\".\\\n          format(trn_aug_x.shape, trn_aug_y.shape))\n        \n    print(\"The shapes of val_x and val_y after augmentation are {} and {}, respectively\".\\\n          format(val_x.shape, val_y.shape))\n        \n    print(\"The shape of test is {}\".format(test.shape))\n            \n    print(\"\\nConverting the data to lgbm format\")\n    trn_data = lgb.Dataset(trn_aug_x, label=trn_aug_y)\n    val_data = lgb.Dataset(val_x, label=val_y)\n    evals_result = {}\n            \n    print(\"Training the classifier\")\n    clf = lgb.train(params,\n                    trn_data,\n                    num_boost_round = 1000000,\n                    valid_sets = [val_data],\n                    early_stopping_rounds=3000,\n                    verbose_eval = 3000,\n                    evals_result=evals_result,\n                   )\n          \n    print(\"\\nMaking predictions for the validation data\")\n    val_pred = clf.predict(val_x, num_iteration=clf.best_iteration)\n    \n    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n    \n    print(\"Computing the AUC score\")\n    roc_auc_fold=roc_auc_score(val_y, val_pred)\n    roc_cv.append(roc_auc_fold)\n        \n    print(\"AUC = {}\".format(roc_auc_fold))\n    \n    print(\"Making predictions for the test data\")\n    test_fold_pred = clf.predict(test, num_iteration=clf.best_iteration)\n\n    test_preds += test_fold_pred.reshape((-1, 1))\n    \ntest_preds /= 5\n#Optional\n#test_preds = rankdata(test_preds)/len(test_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Computing and reporting the summary of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_score = round(sum(roc_cv)/len(roc_cv), 5)\nprint(\"\\nAverage of the folds' AUCs = {}\".format(roc_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_score_1 = round(roc_auc_score(y, oof_preds.ravel()), 5)\nprint(\"Combined folds' AUC = {}\".format(roc_score_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st_dev = round(np.array(roc_cv).std(), 5)\nprint(\"The standard deviation = {}\".format(st_dev))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing and saving the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'ID_code' : test_ids, \n                    'target' : test_preds.astype(np.float16).ravel()})\n\nsub.to_csv('SUBMISSION.CSV', index=False)\n\nprint(\"All done!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}