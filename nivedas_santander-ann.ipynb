{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Loading Training & Testing Dataset\nimport pandas as pd\ntrain_df = pd.read_csv(\"../input/train.csv\")\nval_df = pd.read_csv(\"../input/test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic information\nprint(train_df.shape)\nprint(train_df.columns)\ntrain_df.head(10)","execution_count":4,"outputs":[{"output_type":"stream","text":"(200000, 202)\nIndex(['ID_code', 'target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4',\n       'var_5', 'var_6', 'var_7',\n       ...\n       'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195',\n       'var_196', 'var_197', 'var_198', 'var_199'],\n      dtype='object', length=202)\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n5  train_5       0  11.4763 -2.3182   ...     -3.6241   9.7670  12.5809  -4.7602\n6  train_6       0  11.8091 -0.0832   ...      9.1104   9.1143  10.8869  -3.2097\n7  train_7       0  13.5580 -7.9881   ...      4.2178   9.4237   8.6624   3.4806\n8  train_8       0  16.1071  2.4426   ...     -1.0733   8.1975  19.5114   4.8453\n9  train_9       0  12.5088  1.9743   ...     14.1287   7.9133  16.2375  14.2514\n\n[10 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>train_5</td>\n      <td>0</td>\n      <td>11.4763</td>\n      <td>-2.3182</td>\n      <td>12.6080</td>\n      <td>8.6264</td>\n      <td>10.9621</td>\n      <td>3.5609</td>\n      <td>4.5322</td>\n      <td>15.2255</td>\n      <td>3.5855</td>\n      <td>5.9790</td>\n      <td>0.8010</td>\n      <td>-0.6192</td>\n      <td>13.6380</td>\n      <td>1.2589</td>\n      <td>8.1939</td>\n      <td>14.9894</td>\n      <td>12.0763</td>\n      <td>-1.4710</td>\n      <td>6.7341</td>\n      <td>14.8241</td>\n      <td>19.7172</td>\n      <td>11.9882</td>\n      <td>1.0468</td>\n      <td>3.8663</td>\n      <td>4.7252</td>\n      <td>13.9427</td>\n      <td>-1.2796</td>\n      <td>-4.3763</td>\n      <td>5.1494</td>\n      <td>0.4124</td>\n      <td>-5.0732</td>\n      <td>4.9010</td>\n      <td>1.5459</td>\n      <td>15.6423</td>\n      <td>10.7209</td>\n      <td>15.1886</td>\n      <td>1.8685</td>\n      <td>7.7223</td>\n      <td>...</td>\n      <td>16.5552</td>\n      <td>5.3739</td>\n      <td>6.4487</td>\n      <td>11.5631</td>\n      <td>1.3847</td>\n      <td>14.9638</td>\n      <td>2.8455</td>\n      <td>-9.0953</td>\n      <td>3.8278</td>\n      <td>5.9714</td>\n      <td>-6.1449</td>\n      <td>-2.0285</td>\n      <td>18.4106</td>\n      <td>1.4457</td>\n      <td>21.8853</td>\n      <td>9.2654</td>\n      <td>-6.5247</td>\n      <td>10.7687</td>\n      <td>-7.6283</td>\n      <td>1.0208</td>\n      <td>7.1968</td>\n      <td>11.1227</td>\n      <td>2.2257</td>\n      <td>6.4056</td>\n      <td>21.0550</td>\n      <td>-13.6509</td>\n      <td>4.7691</td>\n      <td>-8.9114</td>\n      <td>15.1007</td>\n      <td>2.4286</td>\n      <td>-6.3068</td>\n      <td>6.6025</td>\n      <td>5.2912</td>\n      <td>0.4403</td>\n      <td>14.9452</td>\n      <td>1.0314</td>\n      <td>-3.6241</td>\n      <td>9.7670</td>\n      <td>12.5809</td>\n      <td>-4.7602</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>train_6</td>\n      <td>0</td>\n      <td>11.8091</td>\n      <td>-0.0832</td>\n      <td>9.3494</td>\n      <td>4.2916</td>\n      <td>11.1355</td>\n      <td>-8.0198</td>\n      <td>6.1961</td>\n      <td>12.0771</td>\n      <td>-4.3781</td>\n      <td>7.9232</td>\n      <td>-5.1288</td>\n      <td>-7.5271</td>\n      <td>14.1629</td>\n      <td>13.3058</td>\n      <td>7.8412</td>\n      <td>14.3363</td>\n      <td>7.5951</td>\n      <td>11.0922</td>\n      <td>21.1976</td>\n      <td>6.2946</td>\n      <td>15.8877</td>\n      <td>24.2595</td>\n      <td>8.1159</td>\n      <td>3.9769</td>\n      <td>7.6851</td>\n      <td>13.3600</td>\n      <td>-0.5156</td>\n      <td>0.0690</td>\n      <td>5.6452</td>\n      <td>4.6140</td>\n      <td>-12.3890</td>\n      <td>12.0880</td>\n      <td>-1.5290</td>\n      <td>9.2376</td>\n      <td>11.1510</td>\n      <td>6.6352</td>\n      <td>4.8462</td>\n      <td>7.0202</td>\n      <td>...</td>\n      <td>39.9599</td>\n      <td>5.5552</td>\n      <td>3.3459</td>\n      <td>9.2661</td>\n      <td>6.1213</td>\n      <td>23.7558</td>\n      <td>3.0298</td>\n      <td>5.9109</td>\n      <td>8.1035</td>\n      <td>6.1887</td>\n      <td>0.2619</td>\n      <td>-1.1405</td>\n      <td>25.1675</td>\n      <td>2.6965</td>\n      <td>17.0152</td>\n      <td>12.7942</td>\n      <td>-3.0403</td>\n      <td>8.1735</td>\n      <td>4.5637</td>\n      <td>3.8973</td>\n      <td>-8.1416</td>\n      <td>10.0570</td>\n      <td>15.7862</td>\n      <td>3.3593</td>\n      <td>11.9140</td>\n      <td>-4.2870</td>\n      <td>7.5015</td>\n      <td>-29.9763</td>\n      <td>17.2867</td>\n      <td>1.8539</td>\n      <td>8.7830</td>\n      <td>6.4521</td>\n      <td>3.5325</td>\n      <td>0.1777</td>\n      <td>18.3314</td>\n      <td>0.5845</td>\n      <td>9.1104</td>\n      <td>9.1143</td>\n      <td>10.8869</td>\n      <td>-3.2097</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>train_7</td>\n      <td>0</td>\n      <td>13.5580</td>\n      <td>-7.9881</td>\n      <td>13.8776</td>\n      <td>7.5985</td>\n      <td>8.6543</td>\n      <td>0.8310</td>\n      <td>5.6890</td>\n      <td>22.3262</td>\n      <td>5.0647</td>\n      <td>7.1971</td>\n      <td>1.4532</td>\n      <td>-6.7033</td>\n      <td>14.2919</td>\n      <td>10.9699</td>\n      <td>6.9190</td>\n      <td>14.2459</td>\n      <td>9.5376</td>\n      <td>-0.7226</td>\n      <td>5.1548</td>\n      <td>17.1535</td>\n      <td>13.7326</td>\n      <td>14.4195</td>\n      <td>1.2375</td>\n      <td>3.1711</td>\n      <td>9.1258</td>\n      <td>13.3250</td>\n      <td>3.3883</td>\n      <td>-0.4418</td>\n      <td>5.4501</td>\n      <td>7.9894</td>\n      <td>-0.9976</td>\n      <td>14.5609</td>\n      <td>-2.0712</td>\n      <td>16.9717</td>\n      <td>11.5257</td>\n      <td>-0.4990</td>\n      <td>2.8303</td>\n      <td>7.5772</td>\n      <td>...</td>\n      <td>23.7765</td>\n      <td>5.4098</td>\n      <td>5.1402</td>\n      <td>10.7013</td>\n      <td>-8.2583</td>\n      <td>26.3286</td>\n      <td>2.6085</td>\n      <td>-10.9163</td>\n      <td>8.7362</td>\n      <td>5.2273</td>\n      <td>8.9519</td>\n      <td>-2.3522</td>\n      <td>6.1335</td>\n      <td>0.0876</td>\n      <td>19.5642</td>\n      <td>13.2008</td>\n      <td>-11.1786</td>\n      <td>17.3041</td>\n      <td>-0.6535</td>\n      <td>0.0592</td>\n      <td>5.1140</td>\n      <td>10.5478</td>\n      <td>6.9736</td>\n      <td>6.9724</td>\n      <td>24.0369</td>\n      <td>-4.8220</td>\n      <td>8.4947</td>\n      <td>-5.9076</td>\n      <td>18.8663</td>\n      <td>1.9731</td>\n      <td>13.1700</td>\n      <td>6.5491</td>\n      <td>3.9906</td>\n      <td>5.8061</td>\n      <td>23.1407</td>\n      <td>-0.3776</td>\n      <td>4.2178</td>\n      <td>9.4237</td>\n      <td>8.6624</td>\n      <td>3.4806</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>train_8</td>\n      <td>0</td>\n      <td>16.1071</td>\n      <td>2.4426</td>\n      <td>13.9307</td>\n      <td>5.6327</td>\n      <td>8.8014</td>\n      <td>6.1630</td>\n      <td>4.4514</td>\n      <td>10.1854</td>\n      <td>-3.1882</td>\n      <td>9.0827</td>\n      <td>0.9501</td>\n      <td>1.7982</td>\n      <td>14.0654</td>\n      <td>-3.0572</td>\n      <td>11.1642</td>\n      <td>14.8757</td>\n      <td>10.0075</td>\n      <td>-8.9472</td>\n      <td>3.8349</td>\n      <td>0.8560</td>\n      <td>10.6958</td>\n      <td>6.3738</td>\n      <td>6.5580</td>\n      <td>2.6182</td>\n      <td>13.2506</td>\n      <td>13.7929</td>\n      <td>-14.4918</td>\n      <td>-2.5407</td>\n      <td>5.9575</td>\n      <td>2.4882</td>\n      <td>-11.1344</td>\n      <td>10.5106</td>\n      <td>-1.0573</td>\n      <td>19.3290</td>\n      <td>12.2897</td>\n      <td>-2.8160</td>\n      <td>6.9208</td>\n      <td>5.1498</td>\n      <td>...</td>\n      <td>30.6740</td>\n      <td>5.7888</td>\n      <td>4.1180</td>\n      <td>9.1486</td>\n      <td>-5.2618</td>\n      <td>14.4422</td>\n      <td>2.6893</td>\n      <td>-9.5251</td>\n      <td>1.7455</td>\n      <td>5.9018</td>\n      <td>3.1838</td>\n      <td>-1.7865</td>\n      <td>4.9105</td>\n      <td>3.5803</td>\n      <td>32.9149</td>\n      <td>13.0201</td>\n      <td>-2.4845</td>\n      <td>11.0988</td>\n      <td>7.4609</td>\n      <td>-2.1408</td>\n      <td>-3.9172</td>\n      <td>7.7291</td>\n      <td>-11.4027</td>\n      <td>2.0696</td>\n      <td>-1.7937</td>\n      <td>-0.0030</td>\n      <td>11.5024</td>\n      <td>-18.3172</td>\n      <td>13.1403</td>\n      <td>0.7014</td>\n      <td>1.4298</td>\n      <td>14.7510</td>\n      <td>1.6395</td>\n      <td>1.4181</td>\n      <td>14.8370</td>\n      <td>-1.9940</td>\n      <td>-1.0733</td>\n      <td>8.1975</td>\n      <td>19.5114</td>\n      <td>4.8453</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>train_9</td>\n      <td>0</td>\n      <td>12.5088</td>\n      <td>1.9743</td>\n      <td>8.8960</td>\n      <td>5.4508</td>\n      <td>13.6043</td>\n      <td>-16.2859</td>\n      <td>6.0637</td>\n      <td>16.8410</td>\n      <td>0.1287</td>\n      <td>7.9682</td>\n      <td>0.8787</td>\n      <td>3.0537</td>\n      <td>13.9639</td>\n      <td>0.8071</td>\n      <td>9.9240</td>\n      <td>15.2659</td>\n      <td>11.3900</td>\n      <td>1.5367</td>\n      <td>5.4649</td>\n      <td>13.6196</td>\n      <td>23.7806</td>\n      <td>4.4221</td>\n      <td>6.1695</td>\n      <td>3.2978</td>\n      <td>4.5923</td>\n      <td>13.3778</td>\n      <td>-3.2200</td>\n      <td>-2.3302</td>\n      <td>6.1120</td>\n      <td>-0.0289</td>\n      <td>-13.1141</td>\n      <td>9.1270</td>\n      <td>2.2580</td>\n      <td>19.8450</td>\n      <td>10.9237</td>\n      <td>2.4796</td>\n      <td>7.2948</td>\n      <td>5.1347</td>\n      <td>...</td>\n      <td>13.7379</td>\n      <td>5.4536</td>\n      <td>6.2403</td>\n      <td>17.1668</td>\n      <td>-5.3527</td>\n      <td>14.3780</td>\n      <td>2.4139</td>\n      <td>-9.1925</td>\n      <td>2.6859</td>\n      <td>5.8540</td>\n      <td>-3.0868</td>\n      <td>-1.2558</td>\n      <td>24.2683</td>\n      <td>-4.5382</td>\n      <td>18.2209</td>\n      <td>7.5652</td>\n      <td>6.3377</td>\n      <td>14.6223</td>\n      <td>-13.8960</td>\n      <td>2.3910</td>\n      <td>2.7878</td>\n      <td>11.3457</td>\n      <td>-9.6774</td>\n      <td>10.3382</td>\n      <td>19.0645</td>\n      <td>-7.6785</td>\n      <td>6.7580</td>\n      <td>-21.6070</td>\n      <td>20.8112</td>\n      <td>-0.1873</td>\n      <td>0.5543</td>\n      <td>6.3160</td>\n      <td>1.0371</td>\n      <td>3.6885</td>\n      <td>14.8344</td>\n      <td>0.4467</td>\n      <td>14.1287</td>\n      <td>7.9133</td>\n      <td>16.2375</td>\n      <td>14.2514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature and target separation\ntrain_X = train_df.drop(columns=['ID_code','target'])\ntrain_y = train_df['target']\n\nval_X = val_df.drop(columns=['ID_code'])\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if target categories are balanced\ntrain_y.value_counts()#Imbalanced","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"0    179902\n1     20098\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Cleaning\nprint(train_X.drop_duplicates().shape) #No duplicates\nprint(train_X.dropna().shape) #No missing values","execution_count":7,"outputs":[{"output_type":"stream","text":"(200000, 200)\n(200000, 200)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.describe()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"               var_0          var_1      ...              var_198        var_199\ncount  200000.000000  200000.000000      ...        200000.000000  200000.000000\nmean       10.679914      -1.627622      ...            15.870720      -3.326537\nstd         3.040051       4.050044      ...             3.010945      10.438015\nmin         0.408400     -15.043400      ...             6.299300     -38.852800\n25%         8.453850      -4.740025      ...            13.829700     -11.208475\n50%        10.524750      -1.608050      ...            15.934050      -2.819550\n75%        12.758200       1.358625      ...            18.064725       4.836800\nmax        20.315000      10.376800      ...            26.079100      28.500700\n\n[8 rows x 200 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>var_39</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.679914</td>\n      <td>-1.627622</td>\n      <td>10.715192</td>\n      <td>6.796529</td>\n      <td>11.078333</td>\n      <td>-5.065317</td>\n      <td>5.408949</td>\n      <td>16.545850</td>\n      <td>0.284162</td>\n      <td>7.567236</td>\n      <td>0.394340</td>\n      <td>-3.245596</td>\n      <td>14.023978</td>\n      <td>8.530232</td>\n      <td>7.537606</td>\n      <td>14.573126</td>\n      <td>9.333264</td>\n      <td>-5.696731</td>\n      <td>15.244013</td>\n      <td>12.438567</td>\n      <td>13.290894</td>\n      <td>17.257883</td>\n      <td>4.305430</td>\n      <td>3.019540</td>\n      <td>10.584400</td>\n      <td>13.667496</td>\n      <td>-4.055133</td>\n      <td>-1.137908</td>\n      <td>5.532980</td>\n      <td>5.053874</td>\n      <td>-7.687740</td>\n      <td>10.393046</td>\n      <td>-0.512886</td>\n      <td>14.774147</td>\n      <td>11.434250</td>\n      <td>3.842499</td>\n      <td>2.187230</td>\n      <td>5.868899</td>\n      <td>10.642131</td>\n      <td>0.662956</td>\n      <td>...</td>\n      <td>24.259300</td>\n      <td>5.633293</td>\n      <td>5.362896</td>\n      <td>11.002170</td>\n      <td>-2.871906</td>\n      <td>19.315753</td>\n      <td>2.963335</td>\n      <td>-4.151155</td>\n      <td>4.937124</td>\n      <td>5.636008</td>\n      <td>-0.004962</td>\n      <td>-0.831777</td>\n      <td>19.817094</td>\n      <td>-0.677967</td>\n      <td>20.210677</td>\n      <td>11.640613</td>\n      <td>-2.799585</td>\n      <td>11.882933</td>\n      <td>-1.014064</td>\n      <td>2.591444</td>\n      <td>-2.741666</td>\n      <td>10.085518</td>\n      <td>0.719109</td>\n      <td>8.769088</td>\n      <td>12.756676</td>\n      <td>-3.983261</td>\n      <td>8.970274</td>\n      <td>-10.335043</td>\n      <td>15.377174</td>\n      <td>0.746072</td>\n      <td>3.234440</td>\n      <td>7.438408</td>\n      <td>1.927839</td>\n      <td>3.331774</td>\n      <td>17.993784</td>\n      <td>-0.142088</td>\n      <td>2.303335</td>\n      <td>8.908158</td>\n      <td>15.870720</td>\n      <td>-3.326537</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.040051</td>\n      <td>4.050044</td>\n      <td>2.640894</td>\n      <td>2.043319</td>\n      <td>1.623150</td>\n      <td>7.863267</td>\n      <td>0.866607</td>\n      <td>3.418076</td>\n      <td>3.332634</td>\n      <td>1.235070</td>\n      <td>5.500793</td>\n      <td>5.970253</td>\n      <td>0.190059</td>\n      <td>4.639536</td>\n      <td>2.247908</td>\n      <td>0.411711</td>\n      <td>2.557421</td>\n      <td>6.712612</td>\n      <td>7.851370</td>\n      <td>7.996694</td>\n      <td>5.876254</td>\n      <td>8.196564</td>\n      <td>2.847958</td>\n      <td>0.526893</td>\n      <td>3.777245</td>\n      <td>0.285535</td>\n      <td>5.922210</td>\n      <td>1.523714</td>\n      <td>0.783367</td>\n      <td>2.615942</td>\n      <td>7.965198</td>\n      <td>2.159891</td>\n      <td>2.587830</td>\n      <td>4.322325</td>\n      <td>0.541614</td>\n      <td>5.179559</td>\n      <td>3.119978</td>\n      <td>2.249730</td>\n      <td>4.278903</td>\n      <td>4.068845</td>\n      <td>...</td>\n      <td>10.880263</td>\n      <td>0.217938</td>\n      <td>1.419612</td>\n      <td>5.262056</td>\n      <td>5.457784</td>\n      <td>5.024182</td>\n      <td>0.369684</td>\n      <td>7.798020</td>\n      <td>3.105986</td>\n      <td>0.369437</td>\n      <td>4.424621</td>\n      <td>5.378008</td>\n      <td>8.674171</td>\n      <td>5.966674</td>\n      <td>7.136427</td>\n      <td>2.892167</td>\n      <td>7.513939</td>\n      <td>2.628895</td>\n      <td>8.579810</td>\n      <td>2.798956</td>\n      <td>5.261243</td>\n      <td>1.371862</td>\n      <td>8.963434</td>\n      <td>4.474924</td>\n      <td>9.318280</td>\n      <td>4.725167</td>\n      <td>3.189759</td>\n      <td>11.574708</td>\n      <td>3.944604</td>\n      <td>0.976348</td>\n      <td>4.559922</td>\n      <td>3.023272</td>\n      <td>1.478423</td>\n      <td>3.992030</td>\n      <td>3.135162</td>\n      <td>1.429372</td>\n      <td>5.454369</td>\n      <td>0.921625</td>\n      <td>3.010945</td>\n      <td>10.438015</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.408400</td>\n      <td>-15.043400</td>\n      <td>2.117100</td>\n      <td>-0.040200</td>\n      <td>5.074800</td>\n      <td>-32.562600</td>\n      <td>2.347300</td>\n      <td>5.349700</td>\n      <td>-10.505500</td>\n      <td>3.970500</td>\n      <td>-20.731300</td>\n      <td>-26.095000</td>\n      <td>13.434600</td>\n      <td>-6.011100</td>\n      <td>1.013300</td>\n      <td>13.076900</td>\n      <td>0.635100</td>\n      <td>-33.380200</td>\n      <td>-10.664200</td>\n      <td>-12.402500</td>\n      <td>-5.432200</td>\n      <td>-10.089000</td>\n      <td>-5.322500</td>\n      <td>1.209800</td>\n      <td>-0.678400</td>\n      <td>12.720000</td>\n      <td>-24.243100</td>\n      <td>-6.166800</td>\n      <td>2.089600</td>\n      <td>-4.787200</td>\n      <td>-34.798400</td>\n      <td>2.140600</td>\n      <td>-8.986100</td>\n      <td>1.508500</td>\n      <td>9.816900</td>\n      <td>-16.513600</td>\n      <td>-8.095100</td>\n      <td>-1.183400</td>\n      <td>-6.337100</td>\n      <td>-14.545700</td>\n      <td>...</td>\n      <td>-7.452200</td>\n      <td>4.852600</td>\n      <td>0.623100</td>\n      <td>-6.531700</td>\n      <td>-19.997700</td>\n      <td>3.816700</td>\n      <td>1.851200</td>\n      <td>-35.969500</td>\n      <td>-5.250200</td>\n      <td>4.258800</td>\n      <td>-14.506000</td>\n      <td>-22.479300</td>\n      <td>-11.453300</td>\n      <td>-22.748700</td>\n      <td>-2.995300</td>\n      <td>3.241500</td>\n      <td>-29.116500</td>\n      <td>4.952100</td>\n      <td>-29.273400</td>\n      <td>-7.856100</td>\n      <td>-22.037400</td>\n      <td>5.416500</td>\n      <td>-26.001100</td>\n      <td>-4.808200</td>\n      <td>-18.489700</td>\n      <td>-22.583300</td>\n      <td>-3.022300</td>\n      <td>-47.753600</td>\n      <td>4.412300</td>\n      <td>-2.554300</td>\n      <td>-14.093300</td>\n      <td>-2.691700</td>\n      <td>-3.814500</td>\n      <td>-11.783400</td>\n      <td>8.694400</td>\n      <td>-5.261000</td>\n      <td>-14.209600</td>\n      <td>5.960600</td>\n      <td>6.299300</td>\n      <td>-38.852800</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.453850</td>\n      <td>-4.740025</td>\n      <td>8.722475</td>\n      <td>5.254075</td>\n      <td>9.883175</td>\n      <td>-11.200350</td>\n      <td>4.767700</td>\n      <td>13.943800</td>\n      <td>-2.317800</td>\n      <td>6.618800</td>\n      <td>-3.594950</td>\n      <td>-7.510600</td>\n      <td>13.894000</td>\n      <td>5.072800</td>\n      <td>5.781875</td>\n      <td>14.262800</td>\n      <td>7.452275</td>\n      <td>-10.476225</td>\n      <td>9.177950</td>\n      <td>6.276475</td>\n      <td>8.627800</td>\n      <td>11.551000</td>\n      <td>2.182400</td>\n      <td>2.634100</td>\n      <td>7.613000</td>\n      <td>13.456400</td>\n      <td>-8.321725</td>\n      <td>-2.307900</td>\n      <td>4.992100</td>\n      <td>3.171700</td>\n      <td>-13.766175</td>\n      <td>8.870000</td>\n      <td>-2.500875</td>\n      <td>11.456300</td>\n      <td>11.032300</td>\n      <td>0.116975</td>\n      <td>-0.007125</td>\n      <td>4.125475</td>\n      <td>7.591050</td>\n      <td>-2.199500</td>\n      <td>...</td>\n      <td>15.696125</td>\n      <td>5.470500</td>\n      <td>4.326100</td>\n      <td>7.029600</td>\n      <td>-7.094025</td>\n      <td>15.744550</td>\n      <td>2.699000</td>\n      <td>-9.643100</td>\n      <td>2.703200</td>\n      <td>5.374600</td>\n      <td>-3.258500</td>\n      <td>-4.720350</td>\n      <td>13.731775</td>\n      <td>-5.009525</td>\n      <td>15.064600</td>\n      <td>9.371600</td>\n      <td>-8.386500</td>\n      <td>9.808675</td>\n      <td>-7.395700</td>\n      <td>0.625575</td>\n      <td>-6.673900</td>\n      <td>9.084700</td>\n      <td>-6.064425</td>\n      <td>5.423100</td>\n      <td>5.663300</td>\n      <td>-7.360000</td>\n      <td>6.715200</td>\n      <td>-19.205125</td>\n      <td>12.501550</td>\n      <td>0.014900</td>\n      <td>-0.058825</td>\n      <td>5.157400</td>\n      <td>0.889775</td>\n      <td>0.584600</td>\n      <td>15.629800</td>\n      <td>-1.170700</td>\n      <td>-1.946925</td>\n      <td>8.252800</td>\n      <td>13.829700</td>\n      <td>-11.208475</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.524750</td>\n      <td>-1.608050</td>\n      <td>10.580000</td>\n      <td>6.825000</td>\n      <td>11.108250</td>\n      <td>-4.833150</td>\n      <td>5.385100</td>\n      <td>16.456800</td>\n      <td>0.393700</td>\n      <td>7.629600</td>\n      <td>0.487300</td>\n      <td>-3.286950</td>\n      <td>14.025500</td>\n      <td>8.604250</td>\n      <td>7.520300</td>\n      <td>14.574100</td>\n      <td>9.232050</td>\n      <td>-5.666350</td>\n      <td>15.196250</td>\n      <td>12.453900</td>\n      <td>13.196800</td>\n      <td>17.234250</td>\n      <td>4.275150</td>\n      <td>3.008650</td>\n      <td>10.380350</td>\n      <td>13.662500</td>\n      <td>-4.196900</td>\n      <td>-1.132100</td>\n      <td>5.534850</td>\n      <td>4.950200</td>\n      <td>-7.411750</td>\n      <td>10.365650</td>\n      <td>-0.497650</td>\n      <td>14.576000</td>\n      <td>11.435200</td>\n      <td>3.917750</td>\n      <td>2.198000</td>\n      <td>5.900650</td>\n      <td>10.562700</td>\n      <td>0.672300</td>\n      <td>...</td>\n      <td>23.864500</td>\n      <td>5.633500</td>\n      <td>5.359700</td>\n      <td>10.788700</td>\n      <td>-2.637800</td>\n      <td>19.270800</td>\n      <td>2.960200</td>\n      <td>-4.011600</td>\n      <td>4.761600</td>\n      <td>5.634300</td>\n      <td>0.002800</td>\n      <td>-0.807350</td>\n      <td>19.748000</td>\n      <td>-0.569750</td>\n      <td>20.206100</td>\n      <td>11.679800</td>\n      <td>-2.538450</td>\n      <td>11.737250</td>\n      <td>-0.942050</td>\n      <td>2.512300</td>\n      <td>-2.688800</td>\n      <td>10.036050</td>\n      <td>0.720200</td>\n      <td>8.600000</td>\n      <td>12.521000</td>\n      <td>-3.946950</td>\n      <td>8.902150</td>\n      <td>-10.209750</td>\n      <td>15.239450</td>\n      <td>0.742600</td>\n      <td>3.203600</td>\n      <td>7.347750</td>\n      <td>1.901300</td>\n      <td>3.396350</td>\n      <td>17.957950</td>\n      <td>-0.172700</td>\n      <td>2.408900</td>\n      <td>8.888200</td>\n      <td>15.934050</td>\n      <td>-2.819550</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.758200</td>\n      <td>1.358625</td>\n      <td>12.516700</td>\n      <td>8.324100</td>\n      <td>12.261125</td>\n      <td>0.924800</td>\n      <td>6.003000</td>\n      <td>19.102900</td>\n      <td>2.937900</td>\n      <td>8.584425</td>\n      <td>4.382925</td>\n      <td>0.852825</td>\n      <td>14.164200</td>\n      <td>12.274775</td>\n      <td>9.270425</td>\n      <td>14.874500</td>\n      <td>11.055900</td>\n      <td>-0.810775</td>\n      <td>21.013325</td>\n      <td>18.433300</td>\n      <td>17.879400</td>\n      <td>23.089050</td>\n      <td>6.293200</td>\n      <td>3.403800</td>\n      <td>13.479600</td>\n      <td>13.863700</td>\n      <td>-0.090200</td>\n      <td>0.015625</td>\n      <td>6.093700</td>\n      <td>6.798925</td>\n      <td>-1.443450</td>\n      <td>11.885000</td>\n      <td>1.469100</td>\n      <td>18.097125</td>\n      <td>11.844400</td>\n      <td>7.487725</td>\n      <td>4.460400</td>\n      <td>7.542400</td>\n      <td>13.598925</td>\n      <td>3.637825</td>\n      <td>...</td>\n      <td>32.622850</td>\n      <td>5.792000</td>\n      <td>6.371200</td>\n      <td>14.623900</td>\n      <td>1.323600</td>\n      <td>23.024025</td>\n      <td>3.241500</td>\n      <td>1.318725</td>\n      <td>7.020025</td>\n      <td>5.905400</td>\n      <td>3.096400</td>\n      <td>2.956800</td>\n      <td>25.907725</td>\n      <td>3.619900</td>\n      <td>25.641225</td>\n      <td>13.745500</td>\n      <td>2.704400</td>\n      <td>13.931300</td>\n      <td>5.338750</td>\n      <td>4.391125</td>\n      <td>0.996200</td>\n      <td>11.011300</td>\n      <td>7.499175</td>\n      <td>12.127425</td>\n      <td>19.456150</td>\n      <td>-0.590650</td>\n      <td>11.193800</td>\n      <td>-1.466000</td>\n      <td>18.345225</td>\n      <td>1.482900</td>\n      <td>6.406200</td>\n      <td>9.512525</td>\n      <td>2.949500</td>\n      <td>6.205800</td>\n      <td>20.396525</td>\n      <td>0.829600</td>\n      <td>6.556725</td>\n      <td>9.593300</td>\n      <td>18.064725</td>\n      <td>4.836800</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>20.315000</td>\n      <td>10.376800</td>\n      <td>19.353000</td>\n      <td>13.188300</td>\n      <td>16.671400</td>\n      <td>17.251600</td>\n      <td>8.447700</td>\n      <td>27.691800</td>\n      <td>10.151300</td>\n      <td>11.150600</td>\n      <td>18.670200</td>\n      <td>17.188700</td>\n      <td>14.654500</td>\n      <td>22.331500</td>\n      <td>14.937700</td>\n      <td>15.863300</td>\n      <td>17.950600</td>\n      <td>19.025900</td>\n      <td>41.748000</td>\n      <td>35.183000</td>\n      <td>31.285900</td>\n      <td>49.044300</td>\n      <td>14.594500</td>\n      <td>4.875200</td>\n      <td>25.446000</td>\n      <td>14.654600</td>\n      <td>15.675100</td>\n      <td>3.243100</td>\n      <td>8.787400</td>\n      <td>13.143100</td>\n      <td>15.651500</td>\n      <td>20.171900</td>\n      <td>6.787100</td>\n      <td>29.546600</td>\n      <td>13.287800</td>\n      <td>21.528900</td>\n      <td>14.245600</td>\n      <td>11.863800</td>\n      <td>29.823500</td>\n      <td>15.322300</td>\n      <td>...</td>\n      <td>58.394200</td>\n      <td>6.309900</td>\n      <td>10.134400</td>\n      <td>27.564800</td>\n      <td>12.119300</td>\n      <td>38.332200</td>\n      <td>4.220400</td>\n      <td>21.276600</td>\n      <td>14.886100</td>\n      <td>7.089000</td>\n      <td>16.731900</td>\n      <td>17.917300</td>\n      <td>53.591900</td>\n      <td>18.855400</td>\n      <td>43.546800</td>\n      <td>20.854800</td>\n      <td>20.245200</td>\n      <td>20.596500</td>\n      <td>29.841300</td>\n      <td>13.448700</td>\n      <td>12.750500</td>\n      <td>14.393900</td>\n      <td>29.248700</td>\n      <td>23.704900</td>\n      <td>44.363400</td>\n      <td>12.997500</td>\n      <td>21.739200</td>\n      <td>22.786100</td>\n      <td>29.330300</td>\n      <td>4.034100</td>\n      <td>18.440900</td>\n      <td>16.716500</td>\n      <td>8.402400</td>\n      <td>18.281800</td>\n      <td>27.928800</td>\n      <td>4.272900</td>\n      <td>18.321500</td>\n      <td>12.000400</td>\n      <td>26.079100</td>\n      <td>28.500700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for Outliers using Z-Score method\n#According to Six Sigma, any data that is 6 Standard deviations away from mean is considered to an outlier\nfrom scipy import stats\ncount=0\nfor var in train_X.columns:\n    var_z = stats.zscore(train_X[var])\n    if((len(var_z[var_z>3.0])>0) or(len(var_z[var_z<-3.0])>0)):\n        #print(\"Feature with Outliers:\",var)\n        count+=1\nprint(\"Total Number of features that has outliers:\",count)","execution_count":9,"outputs":[{"output_type":"stream","text":"Total Number of features that has outliers: 196\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"train_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.subplots(train_df.corr())\n#plt.show()\n#train_df.corr()\nsns.heatmap(train_df.corr(), square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_y)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom keras import utils as np_utils\nlabelencoder_Y = LabelEncoder()\ntrain_y = labelencoder_Y.fit_transform(y_train)\ntest_y = labelencoder_Y.transform(y_test)\n\n# one hot encoding for target values\n#train_y = np_utils.to_categorical(train_y, num_classes=2)\n#test_y = np_utils.to_categorical(test_y, num_classes=2)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom keras import layers, models, optimizers\nimport keras.backend as K","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def create_ann(input_size,  init, output_dim):\ndef create_ann():\n    # input layer\n    input_layer = layers.Input((200, ))\n    # hidden layer\n    hidden_layer_1 = layers.Dense(\n        1000, init='normal', activation=\"relu\")(input_layer)\n    # drop out layer\n    hidden_drop_1 = layers.Dropout(0.3)(hidden_layer_1)\n    hidden_layer_2 = layers.Dense(\n        1000, init='normal', activation=\"relu\")(hidden_drop_1)\n    # drop out layer\n    hidden_drop_2 = layers.Dropout(0.3)(hidden_layer_2)\n    # output layer\n    output_layer = layers.Dense(\n        1,\n        init='normal',\n        activation=\"sigmoid\")(hidden_drop_2)\n    classifier = models.Model(inputs=input_layer, outputs=output_layer)\n    classifier.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy'])\n    return classifier","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmc = ModelCheckpoint(\n        \"model.h5\",\n        monitor='val_loss',\n        mode='min',\n        verbose=1,\n        save_best_only=True)\nclassifier = create_ann(X_train.shape[1],'normal', 1)\n\nclassifier.summary()\nclassifier.fit(np.array(X_train), y_train,         \n        validation_data=(\n            np.array(X_test),\n            y_test),\n        batch_size=8,\n        epochs=1000,\n        callbacks=[            \n            mc])\n    \n#val_y = classifier.predict(val_X)","execution_count":29,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, activation=\"relu\", kernel_initializer=\"normal\")`\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, activation=\"relu\", kernel_initializer=\"normal\")`\n  # Remove the CWD from sys.path while we load stuff.\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n","name":"stderr"},{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         (None, 200)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 1000)              201000    \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 1000)              0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 1000)              1001000   \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 1000)              0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 1)                 1001      \n=================================================================\nTotal params: 1,203,001\nTrainable params: 1,203,001\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 160000 samples, validate on 40000 samples\nEpoch 1/1000\n160000/160000 [==============================] - 99s 617us/step - loss: 1.6198 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00001: val_loss improved from inf to 1.61987, saving model to model.h5\nEpoch 2/1000\n160000/160000 [==============================] - 99s 620us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00002: val_loss did not improve from 1.61987\nEpoch 3/1000\n160000/160000 [==============================] - 98s 615us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00003: val_loss did not improve from 1.61987\nEpoch 4/1000\n160000/160000 [==============================] - 98s 613us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00004: val_loss did not improve from 1.61987\nEpoch 5/1000\n160000/160000 [==============================] - 99s 617us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00005: val_loss did not improve from 1.61987\nEpoch 6/1000\n160000/160000 [==============================] - 99s 617us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00006: val_loss did not improve from 1.61987\nEpoch 7/1000\n   904/160000 [..............................] - ETA: 1:36 - loss: 1.6403 - acc: 0.8982","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-f861439cdc3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         callbacks=[            \n\u001b[0;32m---> 18\u001b[0;31m             mc])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#val_y = classifier.predict(val_X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_y = classifier.predict(val_X)\n\npred_y = classifier.predict(X_test)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nsub_df = pd.read_csv(\"../input/sample_submission.csv\")\nsub_df['target'] = val_y\nsub_df['target']  = [1 if row>0.5 else 0 for row in sub_df['target']]","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('sample_submission.csv', index=False)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nestimator = KerasClassifier(build_fn=create_ann, \n                                           validation_data=(\n            np.array(X_test),\n            y_test),\n        batch_size=8,\n        epochs=1000,callbacks=[            \n            mc])","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.fit(np.array(X_train), y_train)","execution_count":50,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, activation=\"relu\", kernel_initializer=\"normal\")`\n  import sys\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, activation=\"relu\", kernel_initializer=\"normal\")`\n  # This is added back by InteractiveShellApp.init_path()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n","name":"stderr"},{"output_type":"stream","text":"Train on 160000 samples, validate on 40000 samples\nEpoch 1/1000\n160000/160000 [==============================] - 104s 648us/step - loss: 1.6215 - acc: 0.8994 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00001: val_loss did not improve from 1.61987\nEpoch 2/1000\n160000/160000 [==============================] - 103s 645us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00002: val_loss did not improve from 1.61987\nEpoch 3/1000\n160000/160000 [==============================] - 103s 643us/step - loss: 1.6197 - acc: 0.8995 - val_loss: 1.6199 - val_acc: 0.8995\n\nEpoch 00003: val_loss did not improve from 1.61987\nEpoch 4/1000\n 68504/160000 [===========>..................] - ETA: 54s - loss: 1.6249 - acc: 0.8992","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-dc1d9fda0689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(estimator, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = val_X.columns.tolist(), top=150)","execution_count":null,"outputs":[{"output_type":"stream","text":"40000/40000 [==============================] - 8s 206us/step\n40000/40000 [==============================] - 8s 203us/step\n40000/40000 [==============================] - 8s 201us/step\n40000/40000 [==============================] - 8s 200us/step\n40000/40000 [==============================] - 8s 201us/step\n40000/40000 [==============================] - 8s 200us/step\n40000/40000 [==============================] - 8s 205us/step\n40000/40000 [==============================] - 8s 205us/step\n40000/40000 [==============================] - 8s 205us/step\n40000/40000 [==============================] - 8s 204us/step\n 1232/40000 [..............................] - ETA: 8s","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}