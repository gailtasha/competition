{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install bayesian-optimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import rankdata\nimport gc\nimport warnings\nimport os\n\npd.set_option('display.max_columns', 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls \"../input/santander-customer-transaction-prediction/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\ntest_df = pd.read_csv(\"../input/santander-customer-transaction-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = train_df.columns.tolist()[2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = train_df['target']\ntrain = train_df.loc[:,predictors]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 50% rows used for stratified split in order to find optimal parameters\ntrain_index, valid_index = list(StratifiedKFold(n_splits = 2, shuffle = True, random_state = 1).split(train,train_target.values))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Blackbox function to optimize using lightgbm\n\ndef LGB_bayesian(\n                num_leaves,\n                min_data_in_leaf,\n                learning_rate,\n                min_sum_hessian_in_leaf,\n                feature_fraction,\n                lambda_l1,\n                lambda_l2,\n                min_gain_to_split,\n                max_depth):\n    \n    #Below params should be always int\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n    \n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n    param = {\n        'num_leaves' : num_leaves,\n        'min_data_in_leaf':min_data_in_leaf,\n        'max_bin' : 63,#max number of bins that feature values will be bucketed in\n        'learning_rate':learning_rate,\n        'min_sum_hessian_in_leaf' :min_sum_hessian_in_leaf,\n        'bagging_fraction' : 1.0 ,#this will randomly select part of data without resampling\n        'bagging_freq':5, #Note: to enable bagging, bagging_freq should be set to a non zero value as well\n        'feature_fraction':feature_fraction,\n        'lambda_l1':lambda_l1,\n        'lambda_l2' : lambda_l2,\n        'min_gain_to_split':min_gain_to_split, #the minimal gain to perform split\n        'max_depth':max_depth,\n        'save_binary':True, #if true, LightGBM will save the dataset (including validation data) to a binary file. This speed ups the data loading for the next time\n        'seed':1337,\n        'feature_fraction_seed':1337,\n        'bagging_seed' : 1337,\n        'drop_seed':1337,\n        'data_random_seed':1337,\n        'objective':'binary',\n        'boosting_type' : 'gbdt',\n        'verbose' : 1,\n        'metric' : 'auc',\n        'is_unbalance' : True,\n        'boost_from_average':True  \n    }\n    \n    xg_train = lgb.Dataset(train.iloc[train_index][predictors].values, \n                           label = train_target[train_index].values, \n                           feature_name = predictors, \n                           free_raw_data = False #If True, raw data is freed after constructing inner Dataset.\n                          )\n    \n    xg_valid = lgb.Dataset(train.iloc[valid_index][predictors].values,\n                          label = train_target[valid_index].values,\n                          feature_name= predictors,\n                          free_raw_data = False # If True, raw data is freed after constructing inner Dataset.\n                          )\n    \n    num_round = 5000\n    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval = 200,\n                   early_stopping_rounds=50)\n    \n    predictions = clf.predict(train.iloc[valid_index][predictors].values, num_iterations = clf.best_iteration)\n    \n    score = roc_auc_score(train_target[valid_index].values, predictions)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Bounds for params\n\nbounds_LGB = {\n    'num_leaves' : (5,20),\n    'min_data_in_leaf': (5,20),\n    'learning_rate' : (0.01, 0.3),\n    'min_sum_hessian_in_leaf' : (0.00001,0.01),\n    'feature_fraction': (0.05,0.5),\n    'lambda_l1' : (0,5.0),\n    'lambda_l2': (0,5.0),\n    'min_gain_to_split': (0,1.0),\n    'max_depth':(3,15)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = 13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(LGB_BO.space.keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_points = 5 # Number of inital random runs for exploration\nn_iter = 5 #Number of bayesian optimization to perform after initial runs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-'*130)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points = init_points, n_iter = n_iter, acq ='ucb',xi =0.0, alpha = 1e-6)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#You can probe the LGB_bayesian function, if you have an idea of the optimal parameters \nLGB_BO.probe(\n    params={'feature_fraction': 0.1403, \n            'lambda_l1': 4.218, \n            'lambda_l2': 1.734, \n            'learning_rate': 0.07, \n            'max_depth': 14, \n            'min_data_in_leaf': 17, \n            'min_gain_to_split': 0.1501, \n            'min_sum_hessian_in_leaf': 0.000446, \n            'num_leaves': 6},\n    lazy=True, # \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.maximize(init_points = 0, n_iter = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, res in enumerate(LGB_BO.res):\n    print(\"Iteration : {} \\n\\t {}\".format(i, res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training lightGBM model\nparam_lgb = {\n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n        'max_bin': 63,\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits = nfolds, shuffle = True, random_state = 2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train_df))\npredictions = np.zeros((len(test_df),nfolds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 1\nfor train_index, valid_index in skf.split(train, train_target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train.iloc[train_index][predictors].values,\n                           label=train_target.iloc[train_index].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train.iloc[valid_index][predictors].values,\n                           label=train_target.iloc[valid_index].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(roc_auc_score(train_df.target.values, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n\\nCV AUC: {:<0.2f}\".format(roc_auc_score(train_df.target.values, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Rank averaging on\", nfolds, \"fold predictions\")\nrank_predictions = np.zeros((predictions.shape[0],1))\nfor i in range(nfolds):\n    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))/rank_predictions.shape[0]) \n\nrank_predictions /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = rank_predictions\nsub_df[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Reference :\nhttps://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average\n    "},{"metadata":{},"cell_type":"markdown","source":"Lessons Learnt:\nBayesian optimization in LGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}