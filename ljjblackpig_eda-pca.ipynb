{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Show all the rows & Columns of data\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Read in the data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking shape of train & test data\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking missing values\nprint('Is the training data contains any missing values? ' + str(train.isnull().any().any()) + '\\n'\n     + 'Is the testing data contains any missing values? ' + str(test.isnull().any().any()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking column types\ntrain.info(verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Visualizing the response variable\nsns.countplot(train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since we have lots of variables, it would be hard to examing the relations between them, we sample some data out to see\nif we need to reduce our dimensionality\n- Check the relations using heatmap"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Take out the response variable and sample 3 dataset\ntarget = train.iloc[:, train.columns == 'target']\nsample1 = pd.concat([train.iloc[:, 2:10], target], axis = 1)\nsample2 = pd.concat([train.iloc[:, 25:35], target], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample1.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cor1 = sample1.corr()\ncor2 = sample2.corr()\nf, ax = plt.subplots(1, 2, figsize = (12, 8))\nsns.heatmap(cor1, vmax = 0.9, annot = True, square = True, fmt = '.2f', ax=ax[0])\nsns.heatmap(cor2, vmax = 0.9, annot = True, square = True, fmt = '.2f', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking correlation between features\n#referening to : https://www.kaggle.com/allunia/santander-customer-transaction-eda\ntrain_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems that most of these variables are not pretty independent of each other\n- It also seems that most of these variales are weakly corrlated with the response variables\n- We will try to PCA to reduce the dimensionality of our model while retaining a good amount of information avaliable"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Take out the data without id & response variable\ntrain_tran = train.iloc[:, 2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Doing PCA for our model\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\npca = PCA()\npca.fit(train_tran)\npca.data = pca.transform(train_tran)\n\n\n#Percentage variance of each pca component stands for\nper_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n#Create labels for the scree plot\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\n#Plot the data\nplt.figure(figsize = (12, 10))\nplt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label = labels)\nplt.ylabel('percentage of Explained Variance')\nplt.xlabel('Principle Component')\nplt.title('Scree plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('PC1 + PC150 add up to ' +  str(sum(per_var[:150])) + ' % of the variance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- These 150 PCs should be sufficient for us to predict model, it preserve roughly 99.5% percent of our variance with 24.75% of \n    variables\n- Now we prepare our data for modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Extract the top 150 pc information\npc_columns = []\nfor i in range(150):\n    pc_columns.append('PC' + str(i + 1))\n\nPC_train = pd.DataFrame(data=pca.data[:,:150], columns = pc_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"PC_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"PC_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Setting the seed for calculation\nseed = 2019","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(PC_train, target,\n                                                   test_size = 0.25, random_state = seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)  # Don't cheat - fit only on training data\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # apply same transformation to test data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Random Forest__"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nrf_model = RandomForestClassifier(n_estimators=300, max_depth=10,\n                                  oob_score=True,\n                              random_state=seed)\nrf_model.fit(X_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import metrics\n\nrf_pred = rf_model.predict(X_test)\nprint(metrics.accuracy_score(y_test, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__LightGBM__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Get a validation dataset\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                   test_size = 0.25, random_state = seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import lightgbm as lgb\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Tunning in hyperparameters\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'max_depth': 6,\n    'learning_rate': 0.06,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 5,\n    'max_bin': 255,\n    'verbose': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_round = 2000\nlgbm_model = lgb.train(params, lgb_train, num_round, valid_sets = lgb_eval, early_stopping_rounds = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lgbm_pred = lgbm_model.predict(X_test, num_iteration = lgbm_model.best_iteration)\nprint(metrics.accuracy_score(y_test, lgbm_pred.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}