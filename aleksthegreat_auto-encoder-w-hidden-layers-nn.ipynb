{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my take on a DAE with a NN classifier.  I didn't recreate my noise like Jahrer did just some standard random sampling; although I did try to recreate his code myself.  Never could get it to work as well as his did.  \n\nAnyways I hope you guys find it as interesting as I do!  I was able to achieve a 0.89 score with something similar a while back but I broke that code and I can't get it to run anymore.  \n\nThe only boost I have gotten from from the features was with a guassian transform, but nothing else seemed to help. Oh and I tried to upsampl using Oliver's approach, but I'm undecided if it helps here or not.\n\nHonestly my gut is telling me that I just need to build a NN with like 30 layers and move on to the next competition.  \n\nHere we go!"},{"metadata":{},"cell_type":"markdown","source":"Just like always, import that packages and load the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\nimport gc\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom scipy.stats import norm, rankdata\n\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport tensorflow as tf","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = reduce_mem_usage(pd.read_csv('../input/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('../input/test.csv'))","execution_count":3,"outputs":[{"output_type":"stream","text":"Memory usage after optimization is: 78.01 MB\nDecreased by 74.7%\nMemory usage after optimization is: 77.82 MB\nDecreased by 74.6%\n","name":"stdout"}]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"Grab the features, merge the data, and transform.  I know your suppose to keep train/test seperate but I have no shot at winning and this is faster anyways."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = [f for f in train if f not in ['ID_code','target']]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_original = pd.concat([train, test],axis=0,sort=False)\ndf = df_original[features]\ntarget = df_original['target'].values\nid = df_original['ID_code']","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy.special import erfinv\ntrafo_columns = [c for c in df.columns if len(df[c].unique()) != 2]\nfor col in trafo_columns:\n    values = sorted(set(df[col]))\n    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n    f = np.sqrt(2) * erfinv(f)\n    f -= f.mean()\n    df[col] = df[col].map(f)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if __name__ == '__main__':\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Create AUC_ROC callback "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\ndef auc_roc(y_true, y_pred):\n    # any tensorflow metric\n    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n\n    # find all variables created for this metric\n    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n\n    # Add metric variables to GLOBAL_VARIABLES collection.\n    # They will be initialized for new session.\n    for v in metric_vars:\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n\n    # force to update metric values\n    with tf.control_dependencies([update_op]):\n        value = tf.identity(value)\n        return value","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Learning Rate Scheduler"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler\nimport math\nfrom math import exp\nfrom math import ceil\n\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 5.0\n    lrate = initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)/epochs_drop))\n    return lrate\n        \ndef exp_decay(epoch):\n    initial_lrate = 0.1\n    k = 0.1\n    t = epoch\n    lrate = initial_lrate * exp(-k*t)\n    return lrate\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n#        self.lr.append(exp_decay(len(self.losses)))\n        self.lr.append(step_decay(len(self.losses)))","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define all callbacks"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"lrate = LearningRateScheduler(step_decay)\n#lrate = LearningRateScheduler(exp_decay)\nao = ModelCheckpoint(filepath=\"auto_0.h5\",save_best_only=True,verbose=0)\nnn = ModelCheckpoint(filepath=\"nn_0.h5\",save_best_only=True,verbose=0)\ntb = TensorBoard(log_dir='./logs',histogram_freq=0,write_graph=True,write_images=True)\nrl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1)\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\nloss_history = LossHistory()","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build AE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.activations import elu\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras.objectives import binary_crossentropy\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import backend as K\nfrom imblearn.keras import balanced_batch_generator\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, AllKNN, InstanceHardnessThreshold\nfrom sklearn.model_selection import KFold\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom keras.utils import multi_gpu_model\nimport math\n\nverbose = 10\nlearning_rate = 0.0003\nnb_epoch = int(3)\ndcy = learning_rate / nb_epoch\nbatch_size = 256\nencoding_dim =400\nhidden_dim = int(encoding_dim*2) #i.e. 7\npredictions = np.zeros(len(df))\nlabel_cols = [\"target\"]\nopt = keras.optimizers.SGD(lr=learning_rate, decay=dcy, nesterov=False)\n\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(series.shape[1]))\n\ntrn_data, val_data = train_test_split(df[trafo_columns], test_size=0.3)\nnoisy_trn_data = add_noise(trn_data, 0.07)\ninput_dim = noisy_trn_data.shape[1] #num of columns\n\nwith tf.device('/cpu:0'):\n    input_layer = Input(shape=(input_dim, ))\n\n    x = Dense(hidden_dim, activation=\"relu\", name=\"first\", init='identity')(input_layer)\n    x = Dense(hidden_dim, activation=\"relu\", name='second')(x)\n    x = BatchNormalization()(x)\n    x = Dense(hidden_dim, activation=\"relu\", name='third')(x)\n\n    output_layer = Dense(input_dim, activation=\"linear\")(x)\n    autoencoder = Model(inputs=input_layer, outputs=output_layer)    \n    autoencoder.summary()\n    \nautoencoder.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer=opt)","execution_count":12,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 200)               0         \n_________________________________________________________________\nfirst (Dense)                (None, 800)               160800    \n_________________________________________________________________\nsecond (Dense)               (None, 800)               640800    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 800)               3200      \n_________________________________________________________________\nthird (Dense)                (None, 800)               640800    \n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               160200    \n=================================================================\nTotal params: 1,605,800\nTrainable params: 1,604,200\nNon-trainable params: 1,600\n_________________________________________________________________\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(800, activation=\"relu\", name=\"first\", kernel_initializer=\"identity\")`\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Fit AE"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    history = autoencoder.fit(noisy_trn_data, trn_data,\n                        epochs=nb_epoch,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        validation_data=(val_data, val_data),\n                        verbose=1,\n                        callbacks=[ao,tb,es,loss_history,lrate])\n","execution_count":13,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 280000 samples, validate on 120000 samples\nEpoch 1/3\n280000/280000 [==============================] - 65s 233us/step - loss: 0.2286 - acc: 0.1000 - val_loss: 0.1218 - val_acc: 0.1467\nEpoch 2/3\n 42752/280000 [===>..........................] - ETA: 50s - loss: 0.1142 - acc: 0.1579","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-59c971ea9a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         callbacks=[ao,tb,es,loss_history,lrate])\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"Create Hidden Layer Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we build a new model with the activations of the old model\n# this model is truncated after the first layer\n\n#second_hidden_layer = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('second').output)\nthird_hidden_layer = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('third').output)\n\n#print(second_hidden_layer.summary())\nprint(third_hidden_layer.summary())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract Hidden Layer Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n#    get_2nd_hidden_layer = second_hidden_layer.predict(df)\n    get_3rd_hidden_layer = third_hidden_layer.predict(df)\n    \n#print(get_2nd_hidden_layer.shape)\nprint(get_3rd_hidden_layer.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean up"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del df_original, df, noisy_trn_data, test, train, trn_data, val_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Append layers into new DF"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#layer_output_2 = reduce_mem_usage(pd.DataFrame(get_2nd_hidden_layer))\nlayer_output_3 = reduce_mem_usage(pd.DataFrame(get_3rd_hidden_layer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#hidden = np.concatenate([layer_output_2, layer_output_3], axis=1)\n#hidden = pd.DataFrame(hidden)\nhidden = pd.DataFrame(layer_output_3)\nprint(hidden.shape)\n\n#del layer_output_2\ndel layer_output_3\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Attach the original label and target"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#hidden\nhidden['target'] = target\nhidden['ID_code'] = id.values\nprint(hidden.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reshape Data and set shape values.  FYI I grabbed Janek's 3D take on the NN; from his post: \n> NN - why input shape matters?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#find kernal here https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\nfeatures = [f for f in hidden if f not in ['ID_code','target']]\ntrain = hidden[hidden['target'].notnull()]\ntest = hidden[hidden['target'].isnull()]\ntest = np.reshape(test[features].values, (-1,test[features].shape[1], 1))\n\nlen_input_columns, len_data = train.shape[1], train.shape[0]\n\nprint(train.shape)\nprint(test.shape)\nprint(train.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split into train/valid"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"predictions = np.zeros(shape=(len(test), 1))\nlabel_cols = [\"target\"]\ntrain_x, valid_x, train_y, valid_y = train_test_split(train[features], train['target'], test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/cpu:0'):\n\n    input_dim = train_x.shape[1] #num of columns, 4500\n    input_layer = Input(shape=(input_dim, 1))\n\n    x = Dense(hidden_dim, activation='relu')(input_layer)\n    x = Flatten()(x)\n\n\n    output_layer = Dense(1, activation='sigmoid')(x)\n    model= Model(inputs=input_layer, outputs=output_layer)\n    model.summary()\n\nopt = keras.optimizers.SGD(lr=learning_rate, decay=dcy, nesterov=False)\nmodel.compile(metrics=['accuracy', auc_roc],\n                    loss='binary_crossentropy',\n                    optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upsample Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = (pd.Series(train_y == 1))\n\n# Add positive examples\ntrain_x_x = pd.concat([train_x, train_x.loc[pos]], axis=0)\ntrain_y_y = pd.concat([train_y, train_y.loc[pos]], axis=0)\n\n# Shuffle data\nidx = np.arange(len(train_x_x))\nnp.random.shuffle(idx)\ntrain_x_x = train_x_x.iloc[idx]\ntrain_y_y = train_y_y.iloc[idx]\n\ntrain_x_x = np.reshape(train_x.values, (-1, train_x.shape[1], 1))\nvalid_x_x = np.reshape(valid_x.values, (-1, valid_x.shape[1], 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train NN and predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_x_x, train_y,\n                    epochs=1,\n                    batch_size=int(128),\n                    shuffle=True,\n                    validation_data=(valid_x_x, valid_y),\n                    verbose=1,\n                    callbacks=[nn,tb,es,loss_history,lrate])\n\npredictions = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}